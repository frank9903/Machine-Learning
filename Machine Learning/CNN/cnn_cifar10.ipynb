{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs480_fall20_asst4_cnn_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9DXXrkDkgtf"
      },
      "source": [
        "# # libraries (do not import additional libraries)\n",
        "# import keras\n",
        "# from keras.datasets import cifar10\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.models import Sequential, load_model\n",
        "# from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "# from keras.layers import Conv2D, MaxPooling2D\n",
        "# from keras.callbacks import ModelCheckpoint\n",
        "# import math\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # parameters for this script\n",
        "# batch_size = 32\n",
        "# num_classes = 10\n",
        "# epochs = 20\n",
        "# data_augmentation = False\n",
        "\n",
        "# # Load the data, split between train and test sets:\n",
        "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# print('x_train shape:', x_train.shape)\n",
        "# print(x_train.shape[0], 'train samples')\n",
        "# print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# # Convert class vectors to binary class matrices.\n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# # Define a convolutional neural network\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model.add(Dropout(0.25))\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(512))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(num_classes))\n",
        "# model.add(Activation('softmax'))\n",
        "\n",
        "# # initiate RMSprop optimizer\n",
        "# opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# # Compile the model before using it\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=opt,\n",
        "#               metrics=['accuracy'])\n",
        "# print(model.summary())\n",
        "\n",
        "# # normalize the data\n",
        "# x_train = x_train.astype('float32')\n",
        "# x_test = x_test.astype('float32')\n",
        "# x_train /= 255\n",
        "# x_test /= 255\n",
        "\n",
        "# # partition training set into training and validation set\n",
        "# x_validate = x_train[40000:,:]\n",
        "# x_train = x_train[:40000,:]\n",
        "# y_validate = y_train[40000:,:]\n",
        "# y_train = y_train[:40000,:]\n",
        "\n",
        "# # create a callback that will save the best model while training\n",
        "# save_best_model = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "\n",
        "# # train without data augmentation\n",
        "# if not data_augmentation:\n",
        "#     print('Not using data augmentation.')\n",
        "#     history = model.fit(x_train, y_train,\n",
        "#                         batch_size=batch_size,\n",
        "#                         epochs=epochs,\n",
        "#                         validation_data=(x_validate, y_validate),\n",
        "#                         shuffle=True,\n",
        "#                         callbacks=[save_best_model])\n",
        "\n",
        "# # train with data augmentation\n",
        "# else:\n",
        "#     print('Using real-time data augmentation.')\n",
        "#     # This will do preprocessing and realtime data augmentation:\n",
        "#     datagen = ImageDataGenerator(\n",
        "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "#         samplewise_center=False,  # set each sample mean to 0\n",
        "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "#         samplewise_std_normalization=False,  # divide each input by its std\n",
        "#         zca_whitening=False,  # apply ZCA whitening\n",
        "#         zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "#         rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "#         # randomly shift images horizontally (fraction of total width)\n",
        "#         width_shift_range=0.1,\n",
        "#         # randomly shift images vertically (fraction of total height)\n",
        "#         height_shift_range=0.1,\n",
        "#         shear_range=0.,  # set range for random shear\n",
        "#         zoom_range=0.,  # set range for random zoom\n",
        "#         channel_shift_range=0.,  # set range for random channel shifts\n",
        "#         # set mode for filling points outside the input boundaries\n",
        "#         fill_mode='nearest',\n",
        "#         cval=0.,  # value used for fill_mode = \"constant\"\n",
        "#         horizontal_flip=True,  # randomly flip images\n",
        "#         vertical_flip=False,  # randomly flip images\n",
        "#         # set rescaling factor (applied before any other transformation)\n",
        "#         rescale=None,\n",
        "#         # set function that will be applied on each input\n",
        "#         preprocessing_function=None,\n",
        "#         # image data format, either \"channels_first\" or \"channels_last\"\n",
        "#         data_format=None,\n",
        "#         # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "#         validation_split=0.0)\n",
        "\n",
        "#     # Compute quantities required for feature-wise normalization\n",
        "#     # (std, mean, and principal components if ZCA whitening is applied).\n",
        "#     datagen.fit(x_train)\n",
        "\n",
        "#     # Fit the model on the batches generated by datagen.flow().\n",
        "#     history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "#                         steps_per_epoch=math.ceil(x_train.shape[0]/batch_size),\n",
        "#                         epochs=epochs,\n",
        "#                         validation_data=(x_validate, y_validate),\n",
        "#                         callbacks=[save_best_model])\n",
        "    \n",
        "# # Plot training accuracy\n",
        "# plt.plot(history.history['accuracy'])\n",
        "# plt.title('training accuracy')\n",
        "# plt.ylabel('training accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['Conv'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot validation accuracy\n",
        "# plt.plot(history.history['val_accuracy'])\n",
        "# plt.title('validation accuracy')\n",
        "# plt.ylabel('validation accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['Conv'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Evaluate the best model saved (i.e., model with best validation accuracy) on the test set\n",
        "# saved_model = load_model('best_model.h5')\n",
        "# scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "# print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqVel97FIhNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a33690-b777-4989-a455-4c2d8e521795"
      },
      "source": [
        "# libraries (do not import additional libraries)\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# parameters for this script\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "data_augmentation = False\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Load the data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# normalize the data\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# partition training set into training and validation set\n",
        "x_validate = x_train[40000:,:]\n",
        "x_train = x_train[:40000,:]\n",
        "y_validate = y_train[40000:,:]\n",
        "y_train = y_train[:40000,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlU-1cCBDcUk"
      },
      "source": [
        "def a():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "  \n",
        "  print(model.summary())\n",
        "\n",
        "  models = []\n",
        "  for i in range(5):\n",
        "    m = Sequential()\n",
        "    m.add(Flatten())\n",
        "    for j in range(i):\n",
        "      m.add(Dense(512))\n",
        "      m.add(Activation('relu'))\n",
        "      m.add(Dropout(0.5))\n",
        "    m.add(Dense(num_classes))\n",
        "    m.add(Activation('softmax'))\n",
        "    m.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "    models.append(m)\n",
        "  \n",
        "  best_conv_model = ModelCheckpoint('best_conv_model1.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "  best_sdm_models = [ModelCheckpoint('best_sdm_'+str(i)+'_model1.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1) for i in range(5)]\n",
        "  print('Not using data augmentation.')\n",
        "  history = model.fit(x_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      validation_data=(x_validate, y_validate),\n",
        "                      shuffle=True,\n",
        "                      callbacks=[best_conv_model])\n",
        "  histories = []\n",
        "  for i in range(5):\n",
        "    h = models[i].fit(x_train, y_train,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=epochs,\n",
        "                  validation_data=(x_validate, y_validate),\n",
        "                  shuffle=True,\n",
        "                  callbacks=[best_sdm_models[i]])\n",
        "    histories.append(h)\n",
        "    \n",
        "  # Plot training accuracy\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  for h in histories:\n",
        "    plt.plot(h.history['accuracy'])\n",
        "  plt.title('training accuracy')\n",
        "  plt.ylabel('training accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Conv','0 hidden layer', '1 hidden layer', '2 hidden layers', '3 hidden layers', '4 hidden layers'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  # Plot validation accuracy\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  for h in histories:\n",
        "    plt.plot(h.history['val_accuracy'])\n",
        "  plt.title('validation accuracy')\n",
        "  plt.ylabel('validation accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Conv','0 hidden layer', '1 hidden layer', '2 hidden layers', '3 hidden layers', '4 hidden layers'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Evaluate the best model saved (i.e., model with best validation accuracy) on the test set\n",
        "  saved_model = load_model('best_conv_model1.h5')\n",
        "  scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test accuracy:', scores[1])\n",
        "  for i in range(5):\n",
        "    del saved_model\n",
        "    saved_model = load_model('best_sdm_'+str(i)+'_model1.h5')\n",
        "    scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "    print('Test accuracy:', scores[1])\n",
        "\n",
        "def b():\n",
        "  history = []\n",
        "  relu_neural_model = Sequential()\n",
        "  relu_neural_model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "  relu_neural_model.add(Activation('relu'))\n",
        "  relu_neural_model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "  relu_neural_model.add(Activation('relu'))\n",
        "  relu_neural_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  relu_neural_model.add(Dropout(0.25))\n",
        "  relu_neural_model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  relu_neural_model.add(Activation('relu'))\n",
        "  relu_neural_model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  relu_neural_model.add(Activation('relu'))\n",
        "  relu_neural_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  relu_neural_model.add(Dropout(0.25))\n",
        "  relu_neural_model.add(Flatten())\n",
        "  relu_neural_model.add(Dense(512))\n",
        "  relu_neural_model.add(Activation('relu'))\n",
        "  relu_neural_model.add(Dropout(0.5))\n",
        "  relu_neural_model.add(Dense(num_classes))\n",
        "  relu_neural_model.add(Activation('softmax'))\n",
        "  best_relu_model = ModelCheckpoint('best_relu_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "  relu_neural_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  print('Not using data augmentation.')\n",
        "  h = relu_neural_model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(x_validate, y_validate),\n",
        "                shuffle=True,\n",
        "                callbacks=[best_relu_model])\n",
        "  history.append(h)\n",
        "\n",
        "  sigmoid_neural_model = Sequential()\n",
        "  sigmoid_neural_model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "  sigmoid_neural_model.add(Activation('sigmoid'))\n",
        "  sigmoid_neural_model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "  sigmoid_neural_model.add(Activation('sigmoid'))\n",
        "  sigmoid_neural_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  sigmoid_neural_model.add(Dropout(0.25))\n",
        "  sigmoid_neural_model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  sigmoid_neural_model.add(Activation('sigmoid'))\n",
        "  sigmoid_neural_model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  sigmoid_neural_model.add(Activation('sigmoid'))\n",
        "  sigmoid_neural_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  sigmoid_neural_model.add(Dropout(0.25))\n",
        "  sigmoid_neural_model.add(Flatten())\n",
        "  sigmoid_neural_model.add(Dense(512))\n",
        "  sigmoid_neural_model.add(Activation('sigmoid'))\n",
        "  sigmoid_neural_model.add(Dropout(0.5))\n",
        "  sigmoid_neural_model.add(Dense(num_classes))\n",
        "  sigmoid_neural_model.add(Activation('softmax'))\n",
        "  best_sigmoid_model = ModelCheckpoint('best_sigmoid_model.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "  sigmoid_neural_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  print('Not using data augmentation.')\n",
        "  h = sigmoid_neural_model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(x_validate, y_validate),\n",
        "                shuffle=True,\n",
        "                callbacks=[best_sigmoid_model])\n",
        "  history.append(h)\n",
        "\n",
        "  plt.plot(history[0].history['accuracy'])\n",
        "  plt.plot(history[1].history['accuracy'])\n",
        "  plt.title('training accuracy')\n",
        "  plt.ylabel('training accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['relu', 'sigmoid'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  plt.plot(history[0].history['val_accuracy'])\n",
        "  plt.plot(history[1].history['val_accuracy'])\n",
        "  plt.title('validation accuracy')\n",
        "  plt.ylabel('validation accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['relu', 'sigmoid'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  saved_model = load_model('best_relu_model.h5')\n",
        "  scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test accuracy:', scores[1])\n",
        "  del saved_model\n",
        "  saved_model = load_model('best_sigmoid_model.h5')\n",
        "  scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test accuracy:', scores[1])\n",
        "\n",
        "def c():\n",
        "  epochs = 100\n",
        "  histories = []\n",
        "  best_models = []\n",
        "  i = -1\n",
        "  for dropout in [True, False]:\n",
        "    for augmentation in [True, False]:\n",
        "      i += 1\n",
        "      model = Sequential()\n",
        "      model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "      if dropout:\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "      model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "      if dropout:\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "      model.add(Flatten())\n",
        "      model.add(Dense(512))\n",
        "      model.add(Activation('relu'))\n",
        "      if dropout:\n",
        "        model.add(Dropout(0.5))\n",
        "      model.add(Dense(num_classes))\n",
        "      model.add(Activation('softmax'))\n",
        "\n",
        "      # initiate RMSprop optimizer\n",
        "      opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "      # Compile the model before using it\n",
        "      model.compile(loss='categorical_crossentropy',\n",
        "                    optimizer=opt,\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "      # create a callback that will save the best model while training\n",
        "      save_best_model = ModelCheckpoint('best_model'+str(i)+'.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "      best_models.append(save_best_model)\n",
        "\n",
        "      # train without data augmentation\n",
        "      if not augmentation:\n",
        "        print('Not using data augmentation.')\n",
        "        history = model.fit(x_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=100,\n",
        "                            validation_data=(x_validate, y_validate),\n",
        "                            shuffle=True,\n",
        "                            callbacks=[save_best_model])\n",
        "\n",
        "      # train with data augmentation\n",
        "      else:\n",
        "        print('Using real-time data augmentation.')\n",
        "        # This will do preprocessing and realtime data augmentation:\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            # randomly shift images horizontally (fraction of total width)\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically (fraction of total height)\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.,  # set range for random shear\n",
        "            zoom_range=0.,  # set range for random zoom\n",
        "            channel_shift_range=0.,  # set range for random channel shifts\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            cval=0.,  # value used for fill_mode = \"constant\"\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False,  # randomly flip images\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "\n",
        "        # Compute quantities required for feature-wise normalization\n",
        "        # (std, mean, and principal components if ZCA whitening is applied).\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        # Fit the model on the batches generated by datagen.flow().\n",
        "        history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                            steps_per_epoch=math.ceil(x_train.shape[0]/batch_size),\n",
        "                            epochs=100,\n",
        "                            validation_data=(x_validate, y_validate),\n",
        "                            callbacks=[save_best_model])\n",
        "      histories.append(history)\n",
        "\n",
        "  plt.plot(histories[0].history['accuracy'])\n",
        "  plt.plot(histories[1].history['accuracy'])\n",
        "  plt.plot(histories[2].history['accuracy'])\n",
        "  plt.plot(histories[3].history['accuracy'])\n",
        "  plt.title('training accuracy')\n",
        "  plt.ylabel('training accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['relu_with_dropout_with_augmentation', 'relu_with_dropout_without_augmentation', 'relu_without_dropout_with_augmentation', 'relu_without_dropout_without_augmentation'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  plt.plot(histories[0].history['val_accuracy'])\n",
        "  plt.plot(histories[1].history['val_accuracy'])\n",
        "  plt.plot(histories[2].history['val_accuracy'])\n",
        "  plt.plot(histories[3].history['val_accuracy'])\n",
        "  plt.title('validation accuracy')\n",
        "  plt.ylabel('validation accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['relu_with_dropout_with_augmentation', 'relu_with_dropout_without_augmentation', 'relu_without_dropout_with_augmentation', 'relu_without_dropout_without_augmentation'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  for i in range(len(best_models)):\n",
        "    saved_model = load_model('best_model'+str(i)+'.h5')\n",
        "    scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "    print('Test accuracy:', scores[1])\n",
        "    del saved_model\n",
        "\n",
        "def d():\n",
        "  epochs = 100\n",
        "  models = []\n",
        "  for i in range(3):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    models.append(model)\n",
        "  best_models = []\n",
        "  histories = []\n",
        "  names = ['best_RMSprop_model.h5', 'best_Adagrad_model.h5', 'best_Adam_model.h5']\n",
        "  opts = [keras.optimizers.RMSprop(lr=0.0001, decay=1e-6), keras.optimizers.Adagrad(), keras.optimizers.Adam()]\n",
        "  for i, opt in enumerate(opts):\n",
        "    best_model = ModelCheckpoint(names[i], monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "    models[i].compile(loss='categorical_crossentropy',\n",
        "                      optimizer=opt,\n",
        "                      metrics=['accuracy'])\n",
        "    history = models[i].fit(x_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=100,\n",
        "                            validation_data=(x_validate, y_validate),\n",
        "                            shuffle=True,\n",
        "                            callbacks=[best_model])\n",
        "    best_models.append(best_model)\n",
        "    histories.append(history)\n",
        "  \n",
        "  plt.plot(histories[0].history['accuracy'])\n",
        "  plt.plot(histories[1].history['accuracy'])\n",
        "  plt.plot(histories[2].history['accuracy'])\n",
        "  plt.title('training accuracy')\n",
        "  plt.ylabel('training accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['RMSprop_model', 'Adagrad_model', 'Adam_model'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  plt.plot(histories[0].history['val_accuracy'])\n",
        "  plt.plot(histories[1].history['val_accuracy'])\n",
        "  plt.plot(histories[2].history['val_accuracy'])\n",
        "  plt.title('validation accuracy')\n",
        "  plt.ylabel('validation accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['RMSprop_model', 'Adagrad_model', 'Adam_model'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  for i in range(len(best_models)):\n",
        "    saved_model = load_model(names[i])\n",
        "    scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "    print('Test accuracy:', scores[1])\n",
        "    del saved_model\n",
        "\n",
        "def e():\n",
        "  epochs = 100\n",
        "  histories = []\n",
        "  best_models = []\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  save_best_model = ModelCheckpoint('best_model_3x3.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=100,\n",
        "                      validation_data=(x_validate, y_validate),\n",
        "                      shuffle=True,\n",
        "                      callbacks=[save_best_model])\n",
        "  histories.append(history)\n",
        "  \n",
        "  model5x5 = Sequential()\n",
        "  model5x5.add(Conv2D(32, (5, 5), padding='same',input_shape=x_train.shape[1:]))\n",
        "  model5x5.add(Activation('relu'))\n",
        "  model5x5.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model5x5.add(Dropout(0.25))\n",
        "\n",
        "  model5x5.add(Conv2D(64, (5, 5), padding='same'))\n",
        "  model5x5.add(Activation('relu'))\n",
        "  model5x5.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model5x5.add(Dropout(0.25))\n",
        "\n",
        "  model5x5.add(Flatten())\n",
        "  model5x5.add(Dense(512))\n",
        "  model5x5.add(Activation('relu'))\n",
        "  model5x5.add(Dropout(0.5))\n",
        "  model5x5.add(Dense(num_classes))\n",
        "  model5x5.add(Activation('softmax'))\n",
        "  model5x5.compile(loss='categorical_crossentropy',\n",
        "                   optimizer=opt,\n",
        "                   metrics=['accuracy'])\n",
        "  save_best_model_5x5 = ModelCheckpoint('best_model_5x5.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "  history = model5x5.fit(x_train, y_train,\n",
        "                         batch_size=batch_size,\n",
        "                         epochs=100,\n",
        "                         validation_data=(x_validate, y_validate),\n",
        "                         shuffle=True,\n",
        "                         callbacks=[save_best_model_5x5])\n",
        "  histories.append(history)\n",
        "\n",
        "  plt.plot(histories[0].history['accuracy'])\n",
        "  plt.plot(histories[1].history['accuracy'])\n",
        "  plt.title('training accuracy')\n",
        "  plt.ylabel('training accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['3x3 model', '5x5 model'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  plt.plot(histories[0].history['val_accuracy'])\n",
        "  plt.plot(histories[1].history['val_accuracy'])\n",
        "  plt.title('validation accuracy')\n",
        "  plt.ylabel('validation accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['3x3 model', '5x5 model'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "  saved_model = load_model('best_model_3x3.h5')\n",
        "  scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test accuracy:', scores[1])\n",
        "  del saved_model\n",
        "  saved_model = load_model('best_model_5x5.h5')\n",
        "  scores = saved_model.evaluate(x_test, y_test, verbose=1)\n",
        "  print('Test accuracy:', scores[1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGGPndQok1yh"
      },
      "source": [
        "### **Part 1:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LhHGVzdwsdA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be330e90-8956-4b9a-ae67-491aed948d3f"
      },
      "source": [
        "a()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 2,168,362\n",
            "Trainable params: 2,168,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Not using data augmentation.\n",
            "Epoch 1/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.8213 - accuracy: 0.3365\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.43850, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.8203 - accuracy: 0.3369 - val_loss: 1.5559 - val_accuracy: 0.4385\n",
            "Epoch 2/20\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.5049 - accuracy: 0.4552\n",
            "Epoch 00002: val_accuracy improved from 0.43850 to 0.50710, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5044 - accuracy: 0.4555 - val_loss: 1.3826 - val_accuracy: 0.5071\n",
            "Epoch 3/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.3500 - accuracy: 0.5144\n",
            "Epoch 00003: val_accuracy improved from 0.50710 to 0.54830, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3500 - accuracy: 0.5145 - val_loss: 1.2717 - val_accuracy: 0.5483\n",
            "Epoch 4/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.2472 - accuracy: 0.5554\n",
            "Epoch 00004: val_accuracy improved from 0.54830 to 0.60130, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2471 - accuracy: 0.5554 - val_loss: 1.1496 - val_accuracy: 0.6013\n",
            "Epoch 5/20\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.1668 - accuracy: 0.5872\n",
            "Epoch 00005: val_accuracy improved from 0.60130 to 0.61130, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1665 - accuracy: 0.5871 - val_loss: 1.1237 - val_accuracy: 0.6113\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.1016 - accuracy: 0.6131\n",
            "Epoch 00006: val_accuracy improved from 0.61130 to 0.65240, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1016 - accuracy: 0.6131 - val_loss: 1.0176 - val_accuracy: 0.6524\n",
            "Epoch 7/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.0451 - accuracy: 0.6320\n",
            "Epoch 00007: val_accuracy improved from 0.65240 to 0.65780, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0451 - accuracy: 0.6321 - val_loss: 0.9958 - val_accuracy: 0.6578\n",
            "Epoch 8/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.0009 - accuracy: 0.6489\n",
            "Epoch 00008: val_accuracy improved from 0.65780 to 0.66340, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0006 - accuracy: 0.6489 - val_loss: 0.9692 - val_accuracy: 0.6634\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.9589 - accuracy: 0.6653\n",
            "Epoch 00009: val_accuracy improved from 0.66340 to 0.67040, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9589 - accuracy: 0.6653 - val_loss: 0.9474 - val_accuracy: 0.6704\n",
            "Epoch 10/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.9262 - accuracy: 0.6775\n",
            "Epoch 00010: val_accuracy improved from 0.67040 to 0.69170, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.9262 - accuracy: 0.6776 - val_loss: 0.8885 - val_accuracy: 0.6917\n",
            "Epoch 11/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.8979 - accuracy: 0.6872\n",
            "Epoch 00011: val_accuracy improved from 0.69170 to 0.69750, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.8973 - accuracy: 0.6874 - val_loss: 0.8729 - val_accuracy: 0.6975\n",
            "Epoch 12/20\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.8699 - accuracy: 0.6982\n",
            "Epoch 00012: val_accuracy improved from 0.69750 to 0.71320, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8695 - accuracy: 0.6987 - val_loss: 0.8331 - val_accuracy: 0.7132\n",
            "Epoch 13/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8428 - accuracy: 0.7104\n",
            "Epoch 00013: val_accuracy did not improve from 0.71320\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8427 - accuracy: 0.7104 - val_loss: 0.8245 - val_accuracy: 0.7129\n",
            "Epoch 14/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8191 - accuracy: 0.7153\n",
            "Epoch 00014: val_accuracy improved from 0.71320 to 0.71660, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8197 - accuracy: 0.7151 - val_loss: 0.8182 - val_accuracy: 0.7166\n",
            "Epoch 15/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8066 - accuracy: 0.7228\n",
            "Epoch 00015: val_accuracy improved from 0.71660 to 0.72500, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8065 - accuracy: 0.7229 - val_loss: 0.7925 - val_accuracy: 0.7250\n",
            "Epoch 16/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7862 - accuracy: 0.7258\n",
            "Epoch 00016: val_accuracy improved from 0.72500 to 0.72980, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7860 - accuracy: 0.7259 - val_loss: 0.7863 - val_accuracy: 0.7298\n",
            "Epoch 17/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.7710 - accuracy: 0.7337\n",
            "Epoch 00017: val_accuracy improved from 0.72980 to 0.73430, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7710 - accuracy: 0.7338 - val_loss: 0.7768 - val_accuracy: 0.7343\n",
            "Epoch 18/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.7586 - accuracy: 0.7393\n",
            "Epoch 00018: val_accuracy improved from 0.73430 to 0.74350, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7585 - accuracy: 0.7394 - val_loss: 0.7535 - val_accuracy: 0.7435\n",
            "Epoch 19/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.7488 - accuracy: 0.7438\n",
            "Epoch 00019: val_accuracy did not improve from 0.74350\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7486 - accuracy: 0.7439 - val_loss: 0.7760 - val_accuracy: 0.7360\n",
            "Epoch 20/20\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7286 - accuracy: 0.7491\n",
            "Epoch 00020: val_accuracy improved from 0.74350 to 0.74540, saving model to best_conv_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7285 - accuracy: 0.7491 - val_loss: 0.7480 - val_accuracy: 0.7454\n",
            "Epoch 1/20\n",
            "1236/1250 [============================>.] - ETA: 0s - loss: 2.0063 - accuracy: 0.2825\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.33020, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 2.0054 - accuracy: 0.2830 - val_loss: 1.9116 - val_accuracy: 0.3302\n",
            "Epoch 2/20\n",
            "1231/1250 [============================>.] - ETA: 0s - loss: 1.8619 - accuracy: 0.3481\n",
            "Epoch 00002: val_accuracy improved from 0.33020 to 0.35020, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.8614 - accuracy: 0.3483 - val_loss: 1.8596 - val_accuracy: 0.3502\n",
            "Epoch 3/20\n",
            "1235/1250 [============================>.] - ETA: 0s - loss: 1.8223 - accuracy: 0.3668\n",
            "Epoch 00003: val_accuracy improved from 0.35020 to 0.36410, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.8227 - accuracy: 0.3666 - val_loss: 1.8372 - val_accuracy: 0.3641\n",
            "Epoch 4/20\n",
            "1238/1250 [============================>.] - ETA: 0s - loss: 1.7982 - accuracy: 0.3747\n",
            "Epoch 00004: val_accuracy improved from 0.36410 to 0.36530, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7985 - accuracy: 0.3744 - val_loss: 1.8306 - val_accuracy: 0.3653\n",
            "Epoch 5/20\n",
            "1234/1250 [============================>.] - ETA: 0s - loss: 1.7851 - accuracy: 0.3820\n",
            "Epoch 00005: val_accuracy improved from 0.36530 to 0.37790, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7850 - accuracy: 0.3821 - val_loss: 1.8020 - val_accuracy: 0.3779\n",
            "Epoch 6/20\n",
            "1229/1250 [============================>.] - ETA: 0s - loss: 1.7728 - accuracy: 0.3878\n",
            "Epoch 00006: val_accuracy improved from 0.37790 to 0.37900, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7729 - accuracy: 0.3873 - val_loss: 1.7981 - val_accuracy: 0.3790\n",
            "Epoch 7/20\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.7654 - accuracy: 0.3920\n",
            "Epoch 00007: val_accuracy did not improve from 0.37900\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7645 - accuracy: 0.3924 - val_loss: 1.8069 - val_accuracy: 0.3679\n",
            "Epoch 8/20\n",
            "1228/1250 [============================>.] - ETA: 0s - loss: 1.7573 - accuracy: 0.3923\n",
            "Epoch 00008: val_accuracy improved from 0.37900 to 0.38090, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7576 - accuracy: 0.3918 - val_loss: 1.7908 - val_accuracy: 0.3809\n",
            "Epoch 9/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.7496 - accuracy: 0.3961\n",
            "Epoch 00009: val_accuracy improved from 0.38090 to 0.38920, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7501 - accuracy: 0.3958 - val_loss: 1.7826 - val_accuracy: 0.3892\n",
            "Epoch 10/20\n",
            "1236/1250 [============================>.] - ETA: 0s - loss: 1.7433 - accuracy: 0.3977\n",
            "Epoch 00010: val_accuracy did not improve from 0.38920\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7443 - accuracy: 0.3973 - val_loss: 1.7901 - val_accuracy: 0.3765\n",
            "Epoch 11/20\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.7367 - accuracy: 0.4022\n",
            "Epoch 00011: val_accuracy did not improve from 0.38920\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7376 - accuracy: 0.4018 - val_loss: 1.7804 - val_accuracy: 0.3818\n",
            "Epoch 12/20\n",
            "1234/1250 [============================>.] - ETA: 0s - loss: 1.7341 - accuracy: 0.4041\n",
            "Epoch 00012: val_accuracy did not improve from 0.38920\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 1.7344 - accuracy: 0.4041 - val_loss: 1.7795 - val_accuracy: 0.3879\n",
            "Epoch 13/20\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.7300 - accuracy: 0.4043\n",
            "Epoch 00013: val_accuracy did not improve from 0.38920\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 1.7294 - accuracy: 0.4046 - val_loss: 1.7795 - val_accuracy: 0.3881\n",
            "Epoch 14/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.7266 - accuracy: 0.4068\n",
            "Epoch 00014: val_accuracy did not improve from 0.38920\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7267 - accuracy: 0.4068 - val_loss: 1.7756 - val_accuracy: 0.3842\n",
            "Epoch 15/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.7222 - accuracy: 0.4088\n",
            "Epoch 00015: val_accuracy did not improve from 0.38920\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 1.7225 - accuracy: 0.4086 - val_loss: 1.7754 - val_accuracy: 0.3874\n",
            "Epoch 16/20\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.7187 - accuracy: 0.4096\n",
            "Epoch 00016: val_accuracy improved from 0.38920 to 0.39110, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 1.7183 - accuracy: 0.4098 - val_loss: 1.7649 - val_accuracy: 0.3911\n",
            "Epoch 17/20\n",
            "1230/1250 [============================>.] - ETA: 0s - loss: 1.7168 - accuracy: 0.4101\n",
            "Epoch 00017: val_accuracy did not improve from 0.39110\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 1.7156 - accuracy: 0.4105 - val_loss: 1.7750 - val_accuracy: 0.3891\n",
            "Epoch 18/20\n",
            "1232/1250 [============================>.] - ETA: 0s - loss: 1.7126 - accuracy: 0.4144\n",
            "Epoch 00018: val_accuracy improved from 0.39110 to 0.39580, saving model to best_sdm_0_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7123 - accuracy: 0.4146 - val_loss: 1.7726 - val_accuracy: 0.3958\n",
            "Epoch 19/20\n",
            "1228/1250 [============================>.] - ETA: 0s - loss: 1.7102 - accuracy: 0.4142\n",
            "Epoch 00019: val_accuracy did not improve from 0.39580\n",
            "1250/1250 [==============================] - 3s 3ms/step - loss: 1.7104 - accuracy: 0.4140 - val_loss: 1.7679 - val_accuracy: 0.3923\n",
            "Epoch 20/20\n",
            "1231/1250 [============================>.] - ETA: 0s - loss: 1.7067 - accuracy: 0.4168\n",
            "Epoch 00020: val_accuracy did not improve from 0.39580\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.7073 - accuracy: 0.4164 - val_loss: 1.7725 - val_accuracy: 0.3899\n",
            "Epoch 1/20\n",
            "1236/1250 [============================>.] - ETA: 0s - loss: 1.9915 - accuracy: 0.2795\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.32020, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9899 - accuracy: 0.2801 - val_loss: 1.8510 - val_accuracy: 0.3202\n",
            "Epoch 2/20\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.8323 - accuracy: 0.3437\n",
            "Epoch 00002: val_accuracy improved from 0.32020 to 0.38020, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.8317 - accuracy: 0.3439 - val_loss: 1.7542 - val_accuracy: 0.3802\n",
            "Epoch 3/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.7711 - accuracy: 0.3702\n",
            "Epoch 00003: val_accuracy improved from 0.38020 to 0.39710, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.7711 - accuracy: 0.3702 - val_loss: 1.7085 - val_accuracy: 0.3971\n",
            "Epoch 4/20\n",
            "1235/1250 [============================>.] - ETA: 0s - loss: 1.7303 - accuracy: 0.3851\n",
            "Epoch 00004: val_accuracy improved from 0.39710 to 0.41110, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.7298 - accuracy: 0.3852 - val_loss: 1.6710 - val_accuracy: 0.4111\n",
            "Epoch 5/20\n",
            "1237/1250 [============================>.] - ETA: 0s - loss: 1.6993 - accuracy: 0.3984\n",
            "Epoch 00005: val_accuracy improved from 0.41110 to 0.41430, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.6988 - accuracy: 0.3984 - val_loss: 1.6478 - val_accuracy: 0.4143\n",
            "Epoch 6/20\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.6725 - accuracy: 0.4085\n",
            "Epoch 00006: val_accuracy improved from 0.41430 to 0.43490, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.6726 - accuracy: 0.4083 - val_loss: 1.6118 - val_accuracy: 0.4349\n",
            "Epoch 7/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.6497 - accuracy: 0.4178\n",
            "Epoch 00007: val_accuracy did not improve from 0.43490\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.6495 - accuracy: 0.4179 - val_loss: 1.6194 - val_accuracy: 0.4296\n",
            "Epoch 8/20\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.6335 - accuracy: 0.4249\n",
            "Epoch 00008: val_accuracy improved from 0.43490 to 0.44080, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.6333 - accuracy: 0.4247 - val_loss: 1.5922 - val_accuracy: 0.4408\n",
            "Epoch 9/20\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.6188 - accuracy: 0.4272\n",
            "Epoch 00009: val_accuracy improved from 0.44080 to 0.44390, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6184 - accuracy: 0.4275 - val_loss: 1.5770 - val_accuracy: 0.4439\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.6013 - accuracy: 0.4329\n",
            "Epoch 00010: val_accuracy improved from 0.44390 to 0.44840, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.6013 - accuracy: 0.4329 - val_loss: 1.5649 - val_accuracy: 0.4484\n",
            "Epoch 11/20\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.5896 - accuracy: 0.4399\n",
            "Epoch 00011: val_accuracy improved from 0.44840 to 0.45350, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5900 - accuracy: 0.4400 - val_loss: 1.5730 - val_accuracy: 0.4535\n",
            "Epoch 12/20\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.5738 - accuracy: 0.4481\n",
            "Epoch 00012: val_accuracy improved from 0.45350 to 0.46200, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.5744 - accuracy: 0.4479 - val_loss: 1.5462 - val_accuracy: 0.4620\n",
            "Epoch 13/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.5636 - accuracy: 0.4503\n",
            "Epoch 00013: val_accuracy improved from 0.46200 to 0.46410, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5638 - accuracy: 0.4503 - val_loss: 1.5370 - val_accuracy: 0.4641\n",
            "Epoch 14/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.5538 - accuracy: 0.4540\n",
            "Epoch 00014: val_accuracy improved from 0.46410 to 0.46710, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.5537 - accuracy: 0.4541 - val_loss: 1.5397 - val_accuracy: 0.4671\n",
            "Epoch 15/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.5415 - accuracy: 0.4582\n",
            "Epoch 00015: val_accuracy improved from 0.46710 to 0.47290, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5423 - accuracy: 0.4579 - val_loss: 1.5199 - val_accuracy: 0.4729\n",
            "Epoch 16/20\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.4610\n",
            "Epoch 00016: val_accuracy improved from 0.47290 to 0.48060, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5315 - accuracy: 0.4612 - val_loss: 1.5014 - val_accuracy: 0.4806\n",
            "Epoch 17/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.5213 - accuracy: 0.4651\n",
            "Epoch 00017: val_accuracy improved from 0.48060 to 0.49060, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.5214 - accuracy: 0.4651 - val_loss: 1.4883 - val_accuracy: 0.4906\n",
            "Epoch 18/20\n",
            "1237/1250 [============================>.] - ETA: 0s - loss: 1.5164 - accuracy: 0.4665\n",
            "Epoch 00018: val_accuracy did not improve from 0.49060\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.5161 - accuracy: 0.4668 - val_loss: 1.5006 - val_accuracy: 0.4783\n",
            "Epoch 19/20\n",
            "1235/1250 [============================>.] - ETA: 0s - loss: 1.5078 - accuracy: 0.4730\n",
            "Epoch 00019: val_accuracy did not improve from 0.49060\n",
            "1250/1250 [==============================] - 4s 3ms/step - loss: 1.5083 - accuracy: 0.4727 - val_loss: 1.4976 - val_accuracy: 0.4876\n",
            "Epoch 20/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.5080 - accuracy: 0.4722\n",
            "Epoch 00020: val_accuracy improved from 0.49060 to 0.49440, saving model to best_sdm_1_model1.h5\n",
            "1250/1250 [==============================] - 4s 4ms/step - loss: 1.5080 - accuracy: 0.4722 - val_loss: 1.4710 - val_accuracy: 0.4944\n",
            "Epoch 1/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 2.0846 - accuracy: 0.2310\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.33680, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 2.0844 - accuracy: 0.2311 - val_loss: 1.8762 - val_accuracy: 0.3368\n",
            "Epoch 2/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.9124 - accuracy: 0.3097\n",
            "Epoch 00002: val_accuracy improved from 0.33680 to 0.37130, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9122 - accuracy: 0.3098 - val_loss: 1.7846 - val_accuracy: 0.3713\n",
            "Epoch 3/20\n",
            "1238/1250 [============================>.] - ETA: 0s - loss: 1.8520 - accuracy: 0.3339\n",
            "Epoch 00003: val_accuracy did not improve from 0.37130\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.8516 - accuracy: 0.3341 - val_loss: 1.7727 - val_accuracy: 0.3694\n",
            "Epoch 4/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.8118 - accuracy: 0.3530\n",
            "Epoch 00004: val_accuracy improved from 0.37130 to 0.39290, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.8113 - accuracy: 0.3530 - val_loss: 1.7133 - val_accuracy: 0.3929\n",
            "Epoch 5/20\n",
            "1237/1250 [============================>.] - ETA: 0s - loss: 1.7822 - accuracy: 0.3670\n",
            "Epoch 00005: val_accuracy improved from 0.39290 to 0.40770, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7824 - accuracy: 0.3668 - val_loss: 1.6946 - val_accuracy: 0.4077\n",
            "Epoch 6/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.7586 - accuracy: 0.3741\n",
            "Epoch 00006: val_accuracy improved from 0.40770 to 0.42190, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7589 - accuracy: 0.3740 - val_loss: 1.6622 - val_accuracy: 0.4219\n",
            "Epoch 7/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.7429 - accuracy: 0.3798\n",
            "Epoch 00007: val_accuracy improved from 0.42190 to 0.42470, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7430 - accuracy: 0.3798 - val_loss: 1.6460 - val_accuracy: 0.4247\n",
            "Epoch 8/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.7335 - accuracy: 0.3880\n",
            "Epoch 00008: val_accuracy did not improve from 0.42470\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7335 - accuracy: 0.3880 - val_loss: 1.6469 - val_accuracy: 0.4190\n",
            "Epoch 9/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.7138 - accuracy: 0.3899\n",
            "Epoch 00009: val_accuracy improved from 0.42470 to 0.43130, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7134 - accuracy: 0.3898 - val_loss: 1.6126 - val_accuracy: 0.4313\n",
            "Epoch 10/20\n",
            "1236/1250 [============================>.] - ETA: 0s - loss: 1.7029 - accuracy: 0.3957\n",
            "Epoch 00010: val_accuracy improved from 0.43130 to 0.44540, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7016 - accuracy: 0.3958 - val_loss: 1.5958 - val_accuracy: 0.4454\n",
            "Epoch 11/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.6899 - accuracy: 0.4032\n",
            "Epoch 00011: val_accuracy improved from 0.44540 to 0.44700, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6902 - accuracy: 0.4031 - val_loss: 1.5934 - val_accuracy: 0.4470\n",
            "Epoch 12/20\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.6788 - accuracy: 0.4075\n",
            "Epoch 00012: val_accuracy did not improve from 0.44700\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6788 - accuracy: 0.4076 - val_loss: 1.5824 - val_accuracy: 0.4459\n",
            "Epoch 13/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.6768 - accuracy: 0.4062\n",
            "Epoch 00013: val_accuracy improved from 0.44700 to 0.44820, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6767 - accuracy: 0.4063 - val_loss: 1.5762 - val_accuracy: 0.4482\n",
            "Epoch 14/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.6591 - accuracy: 0.4105\n",
            "Epoch 00014: val_accuracy improved from 0.44820 to 0.45750, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6589 - accuracy: 0.4105 - val_loss: 1.5586 - val_accuracy: 0.4575\n",
            "Epoch 15/20\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.6544 - accuracy: 0.4100\n",
            "Epoch 00015: val_accuracy did not improve from 0.45750\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6544 - accuracy: 0.4100 - val_loss: 1.5719 - val_accuracy: 0.4515\n",
            "Epoch 16/20\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.6541 - accuracy: 0.4140\n",
            "Epoch 00016: val_accuracy did not improve from 0.45750\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6540 - accuracy: 0.4139 - val_loss: 1.5633 - val_accuracy: 0.4544\n",
            "Epoch 17/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.6414 - accuracy: 0.4170\n",
            "Epoch 00017: val_accuracy improved from 0.45750 to 0.45980, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6418 - accuracy: 0.4169 - val_loss: 1.5539 - val_accuracy: 0.4598\n",
            "Epoch 18/20\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.6397 - accuracy: 0.4211\n",
            "Epoch 00018: val_accuracy improved from 0.45980 to 0.46260, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6395 - accuracy: 0.4214 - val_loss: 1.5438 - val_accuracy: 0.4626\n",
            "Epoch 19/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.6321 - accuracy: 0.4239\n",
            "Epoch 00019: val_accuracy improved from 0.46260 to 0.46690, saving model to best_sdm_2_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6316 - accuracy: 0.4241 - val_loss: 1.5428 - val_accuracy: 0.4669\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.6213 - accuracy: 0.4268\n",
            "Epoch 00020: val_accuracy did not improve from 0.46690\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6213 - accuracy: 0.4268 - val_loss: 1.5568 - val_accuracy: 0.4597\n",
            "Epoch 1/20\n",
            "1236/1250 [============================>.] - ETA: 0s - loss: 2.1695 - accuracy: 0.1860\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.31130, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 2.1686 - accuracy: 0.1864 - val_loss: 1.9499 - val_accuracy: 0.3113\n",
            "Epoch 2/20\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.9795 - accuracy: 0.2678\n",
            "Epoch 00002: val_accuracy improved from 0.31130 to 0.32570, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9795 - accuracy: 0.2682 - val_loss: 1.8611 - val_accuracy: 0.3257\n",
            "Epoch 3/20\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.9142 - accuracy: 0.3025\n",
            "Epoch 00003: val_accuracy improved from 0.32570 to 0.33950, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.9141 - accuracy: 0.3025 - val_loss: 1.8228 - val_accuracy: 0.3395\n",
            "Epoch 4/20\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.8817 - accuracy: 0.3198\n",
            "Epoch 00004: val_accuracy improved from 0.33950 to 0.36130, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.8817 - accuracy: 0.3196 - val_loss: 1.8061 - val_accuracy: 0.3613\n",
            "Epoch 5/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.8448 - accuracy: 0.3350\n",
            "Epoch 00005: val_accuracy improved from 0.36130 to 0.38880, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.8448 - accuracy: 0.3351 - val_loss: 1.7569 - val_accuracy: 0.3888\n",
            "Epoch 6/20\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.8228 - accuracy: 0.3451\n",
            "Epoch 00006: val_accuracy improved from 0.38880 to 0.38930, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.8226 - accuracy: 0.3455 - val_loss: 1.7320 - val_accuracy: 0.3893\n",
            "Epoch 7/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.8011 - accuracy: 0.3527\n",
            "Epoch 00007: val_accuracy improved from 0.38930 to 0.40470, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.8009 - accuracy: 0.3528 - val_loss: 1.7024 - val_accuracy: 0.4047\n",
            "Epoch 8/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.7838 - accuracy: 0.3600\n",
            "Epoch 00008: val_accuracy improved from 0.40470 to 0.40940, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7838 - accuracy: 0.3601 - val_loss: 1.7041 - val_accuracy: 0.4094\n",
            "Epoch 9/20\n",
            "1237/1250 [============================>.] - ETA: 0s - loss: 1.7689 - accuracy: 0.3667\n",
            "Epoch 00009: val_accuracy improved from 0.40940 to 0.41160, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7697 - accuracy: 0.3660 - val_loss: 1.6863 - val_accuracy: 0.4116\n",
            "Epoch 10/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.7593 - accuracy: 0.3686\n",
            "Epoch 00010: val_accuracy did not improve from 0.41160\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7593 - accuracy: 0.3686 - val_loss: 1.6912 - val_accuracy: 0.4060\n",
            "Epoch 11/20\n",
            "1238/1250 [============================>.] - ETA: 0s - loss: 1.7423 - accuracy: 0.3780\n",
            "Epoch 00011: val_accuracy improved from 0.41160 to 0.42690, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7423 - accuracy: 0.3778 - val_loss: 1.6548 - val_accuracy: 0.4269\n",
            "Epoch 12/20\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.7376 - accuracy: 0.3801\n",
            "Epoch 00012: val_accuracy did not improve from 0.42690\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7370 - accuracy: 0.3802 - val_loss: 1.6841 - val_accuracy: 0.4068\n",
            "Epoch 13/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.7298 - accuracy: 0.3802\n",
            "Epoch 00013: val_accuracy improved from 0.42690 to 0.44260, saving model to best_sdm_3_model1.h5\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7295 - accuracy: 0.3801 - val_loss: 1.6633 - val_accuracy: 0.4426\n",
            "Epoch 14/20\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.7196 - accuracy: 0.3895\n",
            "Epoch 00014: val_accuracy did not improve from 0.44260\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7190 - accuracy: 0.3897 - val_loss: 1.6946 - val_accuracy: 0.4184\n",
            "Epoch 15/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.7106 - accuracy: 0.3899\n",
            "Epoch 00015: val_accuracy did not improve from 0.44260\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7110 - accuracy: 0.3898 - val_loss: 1.6802 - val_accuracy: 0.4267\n",
            "Epoch 16/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.7017 - accuracy: 0.3893\n",
            "Epoch 00016: val_accuracy did not improve from 0.44260\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.7020 - accuracy: 0.3892 - val_loss: 1.6561 - val_accuracy: 0.4323\n",
            "Epoch 17/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.6920 - accuracy: 0.3920\n",
            "Epoch 00017: val_accuracy did not improve from 0.44260\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6922 - accuracy: 0.3919 - val_loss: 1.7036 - val_accuracy: 0.3882\n",
            "Epoch 18/20\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.6898 - accuracy: 0.3987\n",
            "Epoch 00018: val_accuracy did not improve from 0.44260\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6898 - accuracy: 0.3987 - val_loss: 1.6572 - val_accuracy: 0.4204\n",
            "Epoch 19/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.6846 - accuracy: 0.4024\n",
            "Epoch 00019: val_accuracy did not improve from 0.44260\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6846 - accuracy: 0.4024 - val_loss: 1.6693 - val_accuracy: 0.4179\n",
            "Epoch 20/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.6809 - accuracy: 0.4050\n",
            "Epoch 00020: val_accuracy did not improve from 0.44260\n",
            "1250/1250 [==============================] - 5s 4ms/step - loss: 1.6809 - accuracy: 0.4049 - val_loss: 1.6420 - val_accuracy: 0.4327\n",
            "Epoch 1/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 2.2330 - accuracy: 0.1477\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.22870, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 2.2329 - accuracy: 0.1477 - val_loss: 2.0524 - val_accuracy: 0.2287\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 2.0517 - accuracy: 0.2203\n",
            "Epoch 00002: val_accuracy improved from 0.22870 to 0.28980, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 2.0517 - accuracy: 0.2203 - val_loss: 1.9308 - val_accuracy: 0.2898\n",
            "Epoch 3/20\n",
            "1238/1250 [============================>.] - ETA: 0s - loss: 1.9789 - accuracy: 0.2596\n",
            "Epoch 00003: val_accuracy improved from 0.28980 to 0.31840, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.9786 - accuracy: 0.2597 - val_loss: 1.8908 - val_accuracy: 0.3184\n",
            "Epoch 4/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.9414 - accuracy: 0.2798\n",
            "Epoch 00004: val_accuracy improved from 0.31840 to 0.33600, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.9412 - accuracy: 0.2797 - val_loss: 1.8333 - val_accuracy: 0.3360\n",
            "Epoch 5/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.9078 - accuracy: 0.2992\n",
            "Epoch 00005: val_accuracy improved from 0.33600 to 0.34300, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.9078 - accuracy: 0.2990 - val_loss: 1.8288 - val_accuracy: 0.3430\n",
            "Epoch 6/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.8884 - accuracy: 0.3091\n",
            "Epoch 00006: val_accuracy improved from 0.34300 to 0.35120, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8884 - accuracy: 0.3090 - val_loss: 1.8061 - val_accuracy: 0.3512\n",
            "Epoch 7/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.8708 - accuracy: 0.3210\n",
            "Epoch 00007: val_accuracy improved from 0.35120 to 0.35430, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8711 - accuracy: 0.3207 - val_loss: 1.8322 - val_accuracy: 0.3543\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.8540 - accuracy: 0.3229\n",
            "Epoch 00008: val_accuracy improved from 0.35430 to 0.37200, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8540 - accuracy: 0.3229 - val_loss: 1.8003 - val_accuracy: 0.3720\n",
            "Epoch 9/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.8376 - accuracy: 0.3308\n",
            "Epoch 00009: val_accuracy improved from 0.37200 to 0.37860, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8372 - accuracy: 0.3310 - val_loss: 1.7864 - val_accuracy: 0.3786\n",
            "Epoch 10/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.8244 - accuracy: 0.3382\n",
            "Epoch 00010: val_accuracy improved from 0.37860 to 0.38510, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8243 - accuracy: 0.3381 - val_loss: 1.7896 - val_accuracy: 0.3851\n",
            "Epoch 11/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.8182 - accuracy: 0.3412\n",
            "Epoch 00011: val_accuracy did not improve from 0.38510\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8183 - accuracy: 0.3411 - val_loss: 1.8025 - val_accuracy: 0.3759\n",
            "Epoch 12/20\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.8096 - accuracy: 0.3413\n",
            "Epoch 00012: val_accuracy improved from 0.38510 to 0.39180, saving model to best_sdm_4_model1.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8097 - accuracy: 0.3412 - val_loss: 1.7836 - val_accuracy: 0.3918\n",
            "Epoch 13/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.8020 - accuracy: 0.3526\n",
            "Epoch 00013: val_accuracy did not improve from 0.39180\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8021 - accuracy: 0.3528 - val_loss: 1.8265 - val_accuracy: 0.3721\n",
            "Epoch 14/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.7986 - accuracy: 0.3523\n",
            "Epoch 00014: val_accuracy did not improve from 0.39180\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 1.7981 - accuracy: 0.3525 - val_loss: 1.8521 - val_accuracy: 0.3652\n",
            "Epoch 15/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.7847 - accuracy: 0.3589\n",
            "Epoch 00015: val_accuracy did not improve from 0.39180\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7844 - accuracy: 0.3591 - val_loss: 1.8151 - val_accuracy: 0.3784\n",
            "Epoch 16/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.7844 - accuracy: 0.3578\n",
            "Epoch 00016: val_accuracy did not improve from 0.39180\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7847 - accuracy: 0.3577 - val_loss: 1.8299 - val_accuracy: 0.3659\n",
            "Epoch 17/20\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.7790 - accuracy: 0.3603\n",
            "Epoch 00017: val_accuracy did not improve from 0.39180\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7786 - accuracy: 0.3606 - val_loss: 1.8417 - val_accuracy: 0.3628\n",
            "Epoch 18/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.7734 - accuracy: 0.3621\n",
            "Epoch 00018: val_accuracy did not improve from 0.39180\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7733 - accuracy: 0.3622 - val_loss: 1.8227 - val_accuracy: 0.3912\n",
            "Epoch 19/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.7722 - accuracy: 0.3641\n",
            "Epoch 00019: val_accuracy did not improve from 0.39180\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7720 - accuracy: 0.3643 - val_loss: 1.8439 - val_accuracy: 0.3654\n",
            "Epoch 20/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.7724 - accuracy: 0.3638\n",
            "Epoch 00020: val_accuracy did not improve from 0.39180\n",
            "1250/1250 [==============================] - 6s 4ms/step - loss: 1.7721 - accuracy: 0.3639 - val_loss: 1.8449 - val_accuracy: 0.3614\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7647 - accuracy: 0.7409\n",
            "Test accuracy: 0.7408999800682068\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.7482 - accuracy: 0.3886\n",
            "Test accuracy: 0.3885999917984009\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4565 - accuracy: 0.4898\n",
            "Test accuracy: 0.48980000615119934\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.5193 - accuracy: 0.4708\n",
            "Test accuracy: 0.4708000123500824\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.6464 - accuracy: 0.4431\n",
            "Test accuracy: 0.4431000053882599\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.7653 - accuracy: 0.3951\n",
            "Test accuracy: 0.3950999975204468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhXHPta2lF92"
      },
      "source": [
        "CNN model perform better than SDNN models with 0,1,2,3,4 layers. Although 20 epochs is not sufficient to reach convergence, it is sufficient to see the trend. Suppose we have more epochs, adding more layers of SDNN models should increase the training accuracy. CNN has pooling layers, alternation of convolution, parameter sharing and locally equivalent representation, which could all result in better accuracy. Also, CNN need less space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj_HN7vulCIY"
      },
      "source": [
        "### **Part 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEwF3x9iYjoy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26b60564-ef54-4968-fca4-f0c5b1385688"
      },
      "source": [
        "b()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not using data augmentation.\n",
            "Epoch 1/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.8532 - accuracy: 0.3271\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.43870, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.8521 - accuracy: 0.3274 - val_loss: 1.5515 - val_accuracy: 0.4387\n",
            "Epoch 2/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.5136 - accuracy: 0.4544\n",
            "Epoch 00002: val_accuracy improved from 0.43870 to 0.51910, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5135 - accuracy: 0.4545 - val_loss: 1.3638 - val_accuracy: 0.5191\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.3581 - accuracy: 0.5104\n",
            "Epoch 00003: val_accuracy improved from 0.51910 to 0.55200, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.3581 - accuracy: 0.5104 - val_loss: 1.2583 - val_accuracy: 0.5520\n",
            "Epoch 4/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.2495 - accuracy: 0.5521\n",
            "Epoch 00004: val_accuracy improved from 0.55200 to 0.59410, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2491 - accuracy: 0.5522 - val_loss: 1.1550 - val_accuracy: 0.5941\n",
            "Epoch 5/20\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.1718 - accuracy: 0.5857\n",
            "Epoch 00005: val_accuracy improved from 0.59410 to 0.62470, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1714 - accuracy: 0.5858 - val_loss: 1.0748 - val_accuracy: 0.6247\n",
            "Epoch 6/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.1064 - accuracy: 0.6080\n",
            "Epoch 00006: val_accuracy improved from 0.62470 to 0.62840, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1062 - accuracy: 0.6080 - val_loss: 1.0608 - val_accuracy: 0.6284\n",
            "Epoch 7/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.0511 - accuracy: 0.6300\n",
            "Epoch 00007: val_accuracy improved from 0.62840 to 0.65920, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.0508 - accuracy: 0.6302 - val_loss: 0.9819 - val_accuracy: 0.6592\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.0000 - accuracy: 0.6482\n",
            "Epoch 00008: val_accuracy improved from 0.65920 to 0.66390, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.0000 - accuracy: 0.6482 - val_loss: 0.9588 - val_accuracy: 0.6639\n",
            "Epoch 9/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.9605 - accuracy: 0.6628\n",
            "Epoch 00009: val_accuracy improved from 0.66390 to 0.67800, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9604 - accuracy: 0.6628 - val_loss: 0.9201 - val_accuracy: 0.6780\n",
            "Epoch 10/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.9266 - accuracy: 0.6751\n",
            "Epoch 00010: val_accuracy improved from 0.67800 to 0.69040, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.9266 - accuracy: 0.6752 - val_loss: 0.8948 - val_accuracy: 0.6904\n",
            "Epoch 11/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.8943 - accuracy: 0.6893\n",
            "Epoch 00011: val_accuracy improved from 0.69040 to 0.70280, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8942 - accuracy: 0.6891 - val_loss: 0.8587 - val_accuracy: 0.7028\n",
            "Epoch 12/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8705 - accuracy: 0.6948\n",
            "Epoch 00012: val_accuracy improved from 0.70280 to 0.70410, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8704 - accuracy: 0.6948 - val_loss: 0.8450 - val_accuracy: 0.7041\n",
            "Epoch 13/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.8490 - accuracy: 0.7045\n",
            "Epoch 00013: val_accuracy improved from 0.70410 to 0.71030, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8488 - accuracy: 0.7046 - val_loss: 0.8221 - val_accuracy: 0.7103\n",
            "Epoch 14/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.8231 - accuracy: 0.7136\n",
            "Epoch 00014: val_accuracy improved from 0.71030 to 0.72220, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.8233 - accuracy: 0.7135 - val_loss: 0.8030 - val_accuracy: 0.7222\n",
            "Epoch 15/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.8025 - accuracy: 0.7203\n",
            "Epoch 00015: val_accuracy improved from 0.72220 to 0.72340, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8021 - accuracy: 0.7204 - val_loss: 0.8016 - val_accuracy: 0.7234\n",
            "Epoch 16/20\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7829 - accuracy: 0.7269\n",
            "Epoch 00016: val_accuracy improved from 0.72340 to 0.72780, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7830 - accuracy: 0.7268 - val_loss: 0.7943 - val_accuracy: 0.7278\n",
            "Epoch 17/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.7698 - accuracy: 0.7336\n",
            "Epoch 00017: val_accuracy improved from 0.72780 to 0.73350, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7700 - accuracy: 0.7335 - val_loss: 0.7871 - val_accuracy: 0.7335\n",
            "Epoch 18/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7559 - accuracy: 0.7404\n",
            "Epoch 00018: val_accuracy improved from 0.73350 to 0.74340, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.7564 - accuracy: 0.7403 - val_loss: 0.7544 - val_accuracy: 0.7434\n",
            "Epoch 19/20\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7409 - accuracy: 0.7447\n",
            "Epoch 00019: val_accuracy did not improve from 0.74340\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7413 - accuracy: 0.7446 - val_loss: 0.7622 - val_accuracy: 0.7421\n",
            "Epoch 20/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.7289 - accuracy: 0.7503\n",
            "Epoch 00020: val_accuracy improved from 0.74340 to 0.74630, saving model to best_relu_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7289 - accuracy: 0.7504 - val_loss: 0.7462 - val_accuracy: 0.7463\n",
            "Not using data augmentation.\n",
            "Epoch 1/20\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 2.3334 - accuracy: 0.1005\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.09520, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3332 - accuracy: 0.1006 - val_loss: 2.3101 - val_accuracy: 0.0952\n",
            "Epoch 2/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 2.3089 - accuracy: 0.1004\n",
            "Epoch 00002: val_accuracy improved from 0.09520 to 0.09970, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3088 - accuracy: 0.1003 - val_loss: 2.3036 - val_accuracy: 0.0997\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 2.3064 - accuracy: 0.1015\n",
            "Epoch 00003: val_accuracy improved from 0.09970 to 0.10140, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3064 - accuracy: 0.1015 - val_loss: 2.3039 - val_accuracy: 0.1014\n",
            "Epoch 4/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 2.3056 - accuracy: 0.1011\n",
            "Epoch 00004: val_accuracy did not improve from 0.10140\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3057 - accuracy: 0.1010 - val_loss: 2.3025 - val_accuracy: 0.0980\n",
            "Epoch 5/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 2.3052 - accuracy: 0.0992\n",
            "Epoch 00005: val_accuracy did not improve from 0.10140\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3052 - accuracy: 0.0993 - val_loss: 2.3044 - val_accuracy: 0.0977\n",
            "Epoch 6/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1015\n",
            "Epoch 00006: val_accuracy improved from 0.10140 to 0.10160, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3050 - accuracy: 0.1015 - val_loss: 2.3032 - val_accuracy: 0.1016\n",
            "Epoch 7/20\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 2.3045 - accuracy: 0.1016\n",
            "Epoch 00007: val_accuracy did not improve from 0.10160\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3045 - accuracy: 0.1015 - val_loss: 2.3028 - val_accuracy: 0.0980\n",
            "Epoch 8/20\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 2.3035 - accuracy: 0.1034\n",
            "Epoch 00008: val_accuracy improved from 0.10160 to 0.14920, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3034 - accuracy: 0.1033 - val_loss: 2.2890 - val_accuracy: 0.1492\n",
            "Epoch 9/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 2.1700 - accuracy: 0.1979\n",
            "Epoch 00009: val_accuracy improved from 0.14920 to 0.25640, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.1696 - accuracy: 0.1981 - val_loss: 2.0553 - val_accuracy: 0.2564\n",
            "Epoch 10/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 2.0288 - accuracy: 0.2683\n",
            "Epoch 00010: val_accuracy improved from 0.25640 to 0.30230, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0287 - accuracy: 0.2680 - val_loss: 1.9522 - val_accuracy: 0.3023\n",
            "Epoch 11/20\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.9424 - accuracy: 0.3023\n",
            "Epoch 00011: val_accuracy improved from 0.30230 to 0.32620, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.9422 - accuracy: 0.3025 - val_loss: 1.8684 - val_accuracy: 0.3262\n",
            "Epoch 12/20\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.8811 - accuracy: 0.3258\n",
            "Epoch 00012: val_accuracy improved from 0.32620 to 0.34910, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.8811 - accuracy: 0.3259 - val_loss: 1.8197 - val_accuracy: 0.3491\n",
            "Epoch 13/20\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.8441 - accuracy: 0.3360\n",
            "Epoch 00013: val_accuracy improved from 0.34910 to 0.36520, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.8444 - accuracy: 0.3360 - val_loss: 1.7862 - val_accuracy: 0.3652\n",
            "Epoch 14/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.8131 - accuracy: 0.3479\n",
            "Epoch 00014: val_accuracy did not improve from 0.36520\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.8132 - accuracy: 0.3480 - val_loss: 1.7639 - val_accuracy: 0.3621\n",
            "Epoch 15/20\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.7896 - accuracy: 0.3578\n",
            "Epoch 00015: val_accuracy improved from 0.36520 to 0.36890, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.7892 - accuracy: 0.3581 - val_loss: 1.7459 - val_accuracy: 0.3689\n",
            "Epoch 16/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.7686 - accuracy: 0.3668\n",
            "Epoch 00016: val_accuracy improved from 0.36890 to 0.37790, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.7689 - accuracy: 0.3668 - val_loss: 1.7281 - val_accuracy: 0.3779\n",
            "Epoch 17/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.7508 - accuracy: 0.3715\n",
            "Epoch 00017: val_accuracy improved from 0.37790 to 0.39150, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.7498 - accuracy: 0.3718 - val_loss: 1.7034 - val_accuracy: 0.3915\n",
            "Epoch 18/20\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.7319 - accuracy: 0.3799\n",
            "Epoch 00018: val_accuracy improved from 0.39150 to 0.39390, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.7318 - accuracy: 0.3800 - val_loss: 1.6888 - val_accuracy: 0.3939\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.7107 - accuracy: 0.3893\n",
            "Epoch 00019: val_accuracy improved from 0.39390 to 0.40340, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.7107 - accuracy: 0.3893 - val_loss: 1.6627 - val_accuracy: 0.4034\n",
            "Epoch 20/20\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.6931 - accuracy: 0.3952\n",
            "Epoch 00020: val_accuracy improved from 0.40340 to 0.41670, saving model to best_sigmoid_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.6924 - accuracy: 0.3954 - val_loss: 1.6366 - val_accuracy: 0.4167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7576 - accuracy: 0.7375\n",
            "Test accuracy: 0.737500011920929\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.6106 - accuracy: 0.4314\n",
            "Test accuracy: 0.43140000104904175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNrObmv0nrHw"
      },
      "source": [
        "ReLU model performs better than Sigmoid model. ReLU model could reduce the likelihood of vanishing gradient which will result in better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ompaXRgJnrWu"
      },
      "source": [
        "### **Part 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl2uEqQ8KsBV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3d6787a-957b-4808-f591-43344bb494f4"
      },
      "source": [
        "c()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.8885 - accuracy: 0.3089\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.41960, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.8881 - accuracy: 0.3090 - val_loss: 1.6326 - val_accuracy: 0.4196\n",
            "Epoch 2/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.6036 - accuracy: 0.4142\n",
            "Epoch 00002: val_accuracy improved from 0.41960 to 0.50310, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.6034 - accuracy: 0.4142 - val_loss: 1.3867 - val_accuracy: 0.5031\n",
            "Epoch 3/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.4603 - accuracy: 0.4718\n",
            "Epoch 00003: val_accuracy improved from 0.50310 to 0.54730, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.4605 - accuracy: 0.4718 - val_loss: 1.2778 - val_accuracy: 0.5473\n",
            "Epoch 4/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.3659 - accuracy: 0.5108\n",
            "Epoch 00004: val_accuracy did not improve from 0.54730\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.3660 - accuracy: 0.5107 - val_loss: 1.2666 - val_accuracy: 0.5452\n",
            "Epoch 5/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.3039 - accuracy: 0.5306\n",
            "Epoch 00005: val_accuracy improved from 0.54730 to 0.59310, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.3038 - accuracy: 0.5307 - val_loss: 1.1510 - val_accuracy: 0.5931\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.2423 - accuracy: 0.5563\n",
            "Epoch 00006: val_accuracy improved from 0.59310 to 0.59910, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 1.2423 - accuracy: 0.5563 - val_loss: 1.1257 - val_accuracy: 0.5991\n",
            "Epoch 7/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.2008 - accuracy: 0.5739\n",
            "Epoch 00007: val_accuracy improved from 0.59910 to 0.60270, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.2010 - accuracy: 0.5739 - val_loss: 1.1139 - val_accuracy: 0.6027\n",
            "Epoch 8/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.1576 - accuracy: 0.5875\n",
            "Epoch 00008: val_accuracy improved from 0.60270 to 0.63700, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.1574 - accuracy: 0.5877 - val_loss: 1.0275 - val_accuracy: 0.6370\n",
            "Epoch 9/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.1248 - accuracy: 0.6012\n",
            "Epoch 00009: val_accuracy improved from 0.63700 to 0.64300, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.1248 - accuracy: 0.6012 - val_loss: 1.0102 - val_accuracy: 0.6430\n",
            "Epoch 10/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.1006 - accuracy: 0.6114\n",
            "Epoch 00010: val_accuracy improved from 0.64300 to 0.65430, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.1009 - accuracy: 0.6113 - val_loss: 0.9869 - val_accuracy: 0.6543\n",
            "Epoch 11/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.0697 - accuracy: 0.6209\n",
            "Epoch 00011: val_accuracy improved from 0.65430 to 0.67590, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.0696 - accuracy: 0.6209 - val_loss: 0.9282 - val_accuracy: 0.6759\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.0502 - accuracy: 0.6313\n",
            "Epoch 00012: val_accuracy did not improve from 0.67590\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.0502 - accuracy: 0.6313 - val_loss: 0.9850 - val_accuracy: 0.6587\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.0252 - accuracy: 0.6403\n",
            "Epoch 00013: val_accuracy did not improve from 0.67590\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.0252 - accuracy: 0.6403 - val_loss: 0.9619 - val_accuracy: 0.6631\n",
            "Epoch 14/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.0149 - accuracy: 0.6432\n",
            "Epoch 00014: val_accuracy improved from 0.67590 to 0.69370, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 1.0149 - accuracy: 0.6433 - val_loss: 0.8857 - val_accuracy: 0.6937\n",
            "Epoch 15/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.9968 - accuracy: 0.6512\n",
            "Epoch 00015: val_accuracy improved from 0.69370 to 0.69440, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9966 - accuracy: 0.6513 - val_loss: 0.8863 - val_accuracy: 0.6944\n",
            "Epoch 16/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.9794 - accuracy: 0.6579\n",
            "Epoch 00016: val_accuracy improved from 0.69440 to 0.71030, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9793 - accuracy: 0.6579 - val_loss: 0.8390 - val_accuracy: 0.7103\n",
            "Epoch 17/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.9637 - accuracy: 0.6628\n",
            "Epoch 00017: val_accuracy did not improve from 0.71030\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9636 - accuracy: 0.6629 - val_loss: 0.8753 - val_accuracy: 0.7023\n",
            "Epoch 18/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.9501 - accuracy: 0.6693\n",
            "Epoch 00018: val_accuracy improved from 0.71030 to 0.72400, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9505 - accuracy: 0.6693 - val_loss: 0.8112 - val_accuracy: 0.7240\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.9360 - accuracy: 0.6734\n",
            "Epoch 00019: val_accuracy did not improve from 0.72400\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9360 - accuracy: 0.6734 - val_loss: 0.8609 - val_accuracy: 0.7077\n",
            "Epoch 20/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.9266 - accuracy: 0.6812\n",
            "Epoch 00020: val_accuracy did not improve from 0.72400\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9272 - accuracy: 0.6809 - val_loss: 0.8296 - val_accuracy: 0.7168\n",
            "Epoch 21/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.9182 - accuracy: 0.6826\n",
            "Epoch 00021: val_accuracy did not improve from 0.72400\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9182 - accuracy: 0.6825 - val_loss: 0.8297 - val_accuracy: 0.7153\n",
            "Epoch 22/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.9071 - accuracy: 0.6860\n",
            "Epoch 00022: val_accuracy did not improve from 0.72400\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9071 - accuracy: 0.6859 - val_loss: 0.8331 - val_accuracy: 0.7163\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8992 - accuracy: 0.6893\n",
            "Epoch 00023: val_accuracy improved from 0.72400 to 0.73100, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8992 - accuracy: 0.6893 - val_loss: 0.7925 - val_accuracy: 0.7310\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.9001 - accuracy: 0.6892\n",
            "Epoch 00024: val_accuracy did not improve from 0.73100\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.9001 - accuracy: 0.6892 - val_loss: 0.8574 - val_accuracy: 0.7090\n",
            "Epoch 25/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8878 - accuracy: 0.6937\n",
            "Epoch 00025: val_accuracy did not improve from 0.73100\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8878 - accuracy: 0.6938 - val_loss: 0.8075 - val_accuracy: 0.7267\n",
            "Epoch 26/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8810 - accuracy: 0.6973\n",
            "Epoch 00026: val_accuracy improved from 0.73100 to 0.74040, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8810 - accuracy: 0.6974 - val_loss: 0.7700 - val_accuracy: 0.7404\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8809 - accuracy: 0.6970\n",
            "Epoch 00027: val_accuracy did not improve from 0.74040\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8809 - accuracy: 0.6970 - val_loss: 0.8415 - val_accuracy: 0.7148\n",
            "Epoch 28/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8751 - accuracy: 0.6987\n",
            "Epoch 00028: val_accuracy did not improve from 0.74040\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8750 - accuracy: 0.6987 - val_loss: 0.7753 - val_accuracy: 0.7384\n",
            "Epoch 29/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8645 - accuracy: 0.7031\n",
            "Epoch 00029: val_accuracy did not improve from 0.74040\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8644 - accuracy: 0.7031 - val_loss: 0.8070 - val_accuracy: 0.7300\n",
            "Epoch 30/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8670 - accuracy: 0.7029\n",
            "Epoch 00030: val_accuracy improved from 0.74040 to 0.74590, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8667 - accuracy: 0.7030 - val_loss: 0.7612 - val_accuracy: 0.7459\n",
            "Epoch 31/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8603 - accuracy: 0.7050\n",
            "Epoch 00031: val_accuracy did not improve from 0.74590\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8601 - accuracy: 0.7051 - val_loss: 0.7531 - val_accuracy: 0.7438\n",
            "Epoch 32/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8568 - accuracy: 0.7056\n",
            "Epoch 00032: val_accuracy did not improve from 0.74590\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8566 - accuracy: 0.7056 - val_loss: 0.7589 - val_accuracy: 0.7434\n",
            "Epoch 33/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8552 - accuracy: 0.7085\n",
            "Epoch 00033: val_accuracy improved from 0.74590 to 0.74740, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8553 - accuracy: 0.7085 - val_loss: 0.7541 - val_accuracy: 0.7474\n",
            "Epoch 34/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8419 - accuracy: 0.7112\n",
            "Epoch 00034: val_accuracy improved from 0.74740 to 0.75830, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8419 - accuracy: 0.7112 - val_loss: 0.7305 - val_accuracy: 0.7583\n",
            "Epoch 35/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8455 - accuracy: 0.7100\n",
            "Epoch 00035: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8457 - accuracy: 0.7100 - val_loss: 0.7560 - val_accuracy: 0.7455\n",
            "Epoch 36/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8432 - accuracy: 0.7117\n",
            "Epoch 00036: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8433 - accuracy: 0.7116 - val_loss: 0.7799 - val_accuracy: 0.7373\n",
            "Epoch 37/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8415 - accuracy: 0.7127\n",
            "Epoch 00037: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8414 - accuracy: 0.7128 - val_loss: 0.7585 - val_accuracy: 0.7444\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8324 - accuracy: 0.7197\n",
            "Epoch 00038: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8324 - accuracy: 0.7197 - val_loss: 0.7885 - val_accuracy: 0.7369\n",
            "Epoch 39/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8320 - accuracy: 0.7159\n",
            "Epoch 00039: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8319 - accuracy: 0.7159 - val_loss: 0.7894 - val_accuracy: 0.7346\n",
            "Epoch 40/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8318 - accuracy: 0.7149\n",
            "Epoch 00040: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8320 - accuracy: 0.7149 - val_loss: 0.7484 - val_accuracy: 0.7471\n",
            "Epoch 41/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8292 - accuracy: 0.7174\n",
            "Epoch 00041: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8292 - accuracy: 0.7174 - val_loss: 0.7625 - val_accuracy: 0.7452\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8235 - accuracy: 0.7196\n",
            "Epoch 00042: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8235 - accuracy: 0.7196 - val_loss: 0.7402 - val_accuracy: 0.7520\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8203 - accuracy: 0.7239\n",
            "Epoch 00043: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8203 - accuracy: 0.7239 - val_loss: 0.7606 - val_accuracy: 0.7434\n",
            "Epoch 44/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8238 - accuracy: 0.7195\n",
            "Epoch 00044: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8237 - accuracy: 0.7195 - val_loss: 0.7511 - val_accuracy: 0.7520\n",
            "Epoch 45/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8196 - accuracy: 0.7222\n",
            "Epoch 00045: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8196 - accuracy: 0.7223 - val_loss: 0.7524 - val_accuracy: 0.7510\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8215 - accuracy: 0.7215\n",
            "Epoch 00046: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8215 - accuracy: 0.7215 - val_loss: 0.7849 - val_accuracy: 0.7563\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8130 - accuracy: 0.7253\n",
            "Epoch 00047: val_accuracy did not improve from 0.75830\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8130 - accuracy: 0.7253 - val_loss: 0.7587 - val_accuracy: 0.7471\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8119 - accuracy: 0.7259\n",
            "Epoch 00048: val_accuracy improved from 0.75830 to 0.75890, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8119 - accuracy: 0.7259 - val_loss: 0.7301 - val_accuracy: 0.7589\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8140 - accuracy: 0.7238\n",
            "Epoch 00049: val_accuracy did not improve from 0.75890\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8140 - accuracy: 0.7238 - val_loss: 0.7895 - val_accuracy: 0.7429\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8101 - accuracy: 0.7266\n",
            "Epoch 00050: val_accuracy did not improve from 0.75890\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8101 - accuracy: 0.7266 - val_loss: 0.7516 - val_accuracy: 0.7537\n",
            "Epoch 51/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8104 - accuracy: 0.7269\n",
            "Epoch 00051: val_accuracy did not improve from 0.75890\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8106 - accuracy: 0.7269 - val_loss: 0.8009 - val_accuracy: 0.7336\n",
            "Epoch 52/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8141 - accuracy: 0.7274\n",
            "Epoch 00052: val_accuracy did not improve from 0.75890\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8142 - accuracy: 0.7273 - val_loss: 0.7485 - val_accuracy: 0.7533\n",
            "Epoch 53/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8030 - accuracy: 0.7307\n",
            "Epoch 00053: val_accuracy improved from 0.75890 to 0.75970, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8031 - accuracy: 0.7306 - val_loss: 0.7270 - val_accuracy: 0.7597\n",
            "Epoch 54/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8032 - accuracy: 0.7297\n",
            "Epoch 00054: val_accuracy did not improve from 0.75970\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8031 - accuracy: 0.7297 - val_loss: 0.7655 - val_accuracy: 0.7414\n",
            "Epoch 55/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7992 - accuracy: 0.7330\n",
            "Epoch 00055: val_accuracy improved from 0.75970 to 0.76630, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7993 - accuracy: 0.7330 - val_loss: 0.7198 - val_accuracy: 0.7663\n",
            "Epoch 56/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8086 - accuracy: 0.7270\n",
            "Epoch 00056: val_accuracy did not improve from 0.76630\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8084 - accuracy: 0.7271 - val_loss: 0.7100 - val_accuracy: 0.7616\n",
            "Epoch 57/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8030 - accuracy: 0.7283\n",
            "Epoch 00057: val_accuracy did not improve from 0.76630\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8031 - accuracy: 0.7282 - val_loss: 0.8060 - val_accuracy: 0.7471\n",
            "Epoch 58/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8004 - accuracy: 0.7314\n",
            "Epoch 00058: val_accuracy did not improve from 0.76630\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8004 - accuracy: 0.7314 - val_loss: 0.7294 - val_accuracy: 0.7548\n",
            "Epoch 59/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8015 - accuracy: 0.7321\n",
            "Epoch 00059: val_accuracy did not improve from 0.76630\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8020 - accuracy: 0.7320 - val_loss: 0.7689 - val_accuracy: 0.7434\n",
            "Epoch 60/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8086 - accuracy: 0.7298\n",
            "Epoch 00060: val_accuracy did not improve from 0.76630\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8088 - accuracy: 0.7297 - val_loss: 0.7438 - val_accuracy: 0.7531\n",
            "Epoch 61/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7981 - accuracy: 0.7310\n",
            "Epoch 00061: val_accuracy improved from 0.76630 to 0.76800, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7982 - accuracy: 0.7310 - val_loss: 0.7233 - val_accuracy: 0.7680\n",
            "Epoch 62/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7966 - accuracy: 0.7322\n",
            "Epoch 00062: val_accuracy did not improve from 0.76800\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7965 - accuracy: 0.7322 - val_loss: 0.7483 - val_accuracy: 0.7590\n",
            "Epoch 63/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7978 - accuracy: 0.7316\n",
            "Epoch 00063: val_accuracy did not improve from 0.76800\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7978 - accuracy: 0.7316 - val_loss: 0.7293 - val_accuracy: 0.7550\n",
            "Epoch 64/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8027 - accuracy: 0.7306\n",
            "Epoch 00064: val_accuracy did not improve from 0.76800\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8026 - accuracy: 0.7307 - val_loss: 0.7337 - val_accuracy: 0.7569\n",
            "Epoch 65/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8007 - accuracy: 0.7325\n",
            "Epoch 00065: val_accuracy did not improve from 0.76800\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8009 - accuracy: 0.7325 - val_loss: 0.8000 - val_accuracy: 0.7469\n",
            "Epoch 66/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7981 - accuracy: 0.7302\n",
            "Epoch 00066: val_accuracy did not improve from 0.76800\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7980 - accuracy: 0.7303 - val_loss: 0.7270 - val_accuracy: 0.7600\n",
            "Epoch 67/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7982 - accuracy: 0.7345\n",
            "Epoch 00067: val_accuracy improved from 0.76800 to 0.76950, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.7980 - accuracy: 0.7345 - val_loss: 0.6994 - val_accuracy: 0.7695\n",
            "Epoch 68/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7939 - accuracy: 0.7342\n",
            "Epoch 00068: val_accuracy did not improve from 0.76950\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.7941 - accuracy: 0.7341 - val_loss: 0.7615 - val_accuracy: 0.7538\n",
            "Epoch 69/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7983 - accuracy: 0.7314\n",
            "Epoch 00069: val_accuracy improved from 0.76950 to 0.77250, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7986 - accuracy: 0.7313 - val_loss: 0.6967 - val_accuracy: 0.7725\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8040 - accuracy: 0.7310\n",
            "Epoch 00070: val_accuracy did not improve from 0.77250\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.8040 - accuracy: 0.7310 - val_loss: 0.7468 - val_accuracy: 0.7538\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7962 - accuracy: 0.7333\n",
            "Epoch 00071: val_accuracy did not improve from 0.77250\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.7962 - accuracy: 0.7333 - val_loss: 0.7883 - val_accuracy: 0.7484\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7956 - accuracy: 0.7357\n",
            "Epoch 00072: val_accuracy did not improve from 0.77250\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7956 - accuracy: 0.7357 - val_loss: 0.7567 - val_accuracy: 0.7492\n",
            "Epoch 73/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7973 - accuracy: 0.7353\n",
            "Epoch 00073: val_accuracy improved from 0.77250 to 0.77360, saving model to best_model0.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7973 - accuracy: 0.7354 - val_loss: 0.6893 - val_accuracy: 0.7736\n",
            "Epoch 74/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8023 - accuracy: 0.7321\n",
            "Epoch 00074: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 20s 16ms/step - loss: 0.8024 - accuracy: 0.7321 - val_loss: 0.7730 - val_accuracy: 0.7500\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7981 - accuracy: 0.7337\n",
            "Epoch 00075: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.7981 - accuracy: 0.7337 - val_loss: 0.7235 - val_accuracy: 0.7652\n",
            "Epoch 76/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7948 - accuracy: 0.7329\n",
            "Epoch 00076: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.7948 - accuracy: 0.7329 - val_loss: 0.8116 - val_accuracy: 0.7364\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7994 - accuracy: 0.7349\n",
            "Epoch 00077: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7994 - accuracy: 0.7349 - val_loss: 0.7169 - val_accuracy: 0.7623\n",
            "Epoch 78/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7971 - accuracy: 0.7337\n",
            "Epoch 00078: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7971 - accuracy: 0.7337 - val_loss: 0.7019 - val_accuracy: 0.7687\n",
            "Epoch 79/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8011 - accuracy: 0.7339\n",
            "Epoch 00079: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.8006 - accuracy: 0.7341 - val_loss: 0.7567 - val_accuracy: 0.7451\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8036 - accuracy: 0.7347\n",
            "Epoch 00080: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8036 - accuracy: 0.7347 - val_loss: 0.7174 - val_accuracy: 0.7651\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7967 - accuracy: 0.7345\n",
            "Epoch 00081: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7967 - accuracy: 0.7345 - val_loss: 0.7875 - val_accuracy: 0.7476\n",
            "Epoch 82/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7985 - accuracy: 0.7350\n",
            "Epoch 00082: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 16ms/step - loss: 0.7994 - accuracy: 0.7349 - val_loss: 0.7901 - val_accuracy: 0.7592\n",
            "Epoch 83/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7977 - accuracy: 0.7334\n",
            "Epoch 00083: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7975 - accuracy: 0.7335 - val_loss: 0.7324 - val_accuracy: 0.7539\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8019 - accuracy: 0.7331\n",
            "Epoch 00084: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8019 - accuracy: 0.7331 - val_loss: 0.6928 - val_accuracy: 0.7721\n",
            "Epoch 85/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8010 - accuracy: 0.7325\n",
            "Epoch 00085: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8012 - accuracy: 0.7325 - val_loss: 0.7140 - val_accuracy: 0.7580\n",
            "Epoch 86/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8043 - accuracy: 0.7323\n",
            "Epoch 00086: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8046 - accuracy: 0.7322 - val_loss: 0.7708 - val_accuracy: 0.7435\n",
            "Epoch 87/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8037 - accuracy: 0.7323\n",
            "Epoch 00087: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8038 - accuracy: 0.7322 - val_loss: 0.7183 - val_accuracy: 0.7689\n",
            "Epoch 88/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8073 - accuracy: 0.7342\n",
            "Epoch 00088: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8069 - accuracy: 0.7343 - val_loss: 0.6948 - val_accuracy: 0.7689\n",
            "Epoch 89/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8008 - accuracy: 0.7347\n",
            "Epoch 00089: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8008 - accuracy: 0.7347 - val_loss: 0.7091 - val_accuracy: 0.7667\n",
            "Epoch 90/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8057 - accuracy: 0.7357\n",
            "Epoch 00090: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8057 - accuracy: 0.7357 - val_loss: 0.7212 - val_accuracy: 0.7661\n",
            "Epoch 91/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7983 - accuracy: 0.7353\n",
            "Epoch 00091: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.7987 - accuracy: 0.7351 - val_loss: 0.8258 - val_accuracy: 0.7554\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8045 - accuracy: 0.7308\n",
            "Epoch 00092: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8045 - accuracy: 0.7308 - val_loss: 0.7279 - val_accuracy: 0.7558\n",
            "Epoch 93/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8114 - accuracy: 0.7318\n",
            "Epoch 00093: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8115 - accuracy: 0.7318 - val_loss: 0.7653 - val_accuracy: 0.7385\n",
            "Epoch 94/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8087 - accuracy: 0.7340\n",
            "Epoch 00094: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8087 - accuracy: 0.7339 - val_loss: 0.7140 - val_accuracy: 0.7618\n",
            "Epoch 95/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8056 - accuracy: 0.7316\n",
            "Epoch 00095: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8056 - accuracy: 0.7317 - val_loss: 0.7725 - val_accuracy: 0.7590\n",
            "Epoch 96/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8088 - accuracy: 0.7316\n",
            "Epoch 00096: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8083 - accuracy: 0.7317 - val_loss: 0.6892 - val_accuracy: 0.7663\n",
            "Epoch 97/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8124 - accuracy: 0.7307\n",
            "Epoch 00097: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8126 - accuracy: 0.7307 - val_loss: 0.8090 - val_accuracy: 0.7330\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8114 - accuracy: 0.7326\n",
            "Epoch 00098: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8114 - accuracy: 0.7326 - val_loss: 0.8301 - val_accuracy: 0.7272\n",
            "Epoch 99/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8231 - accuracy: 0.7283\n",
            "Epoch 00099: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.8226 - accuracy: 0.7285 - val_loss: 0.7160 - val_accuracy: 0.7619\n",
            "Epoch 100/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8114 - accuracy: 0.7308\n",
            "Epoch 00100: val_accuracy did not improve from 0.77360\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.8117 - accuracy: 0.7308 - val_loss: 0.7870 - val_accuracy: 0.7462\n",
            "Not using data augmentation.\n",
            "Epoch 1/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.8235 - accuracy: 0.3380\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.42680, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.8226 - accuracy: 0.3383 - val_loss: 1.6216 - val_accuracy: 0.4268\n",
            "Epoch 2/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.5151 - accuracy: 0.4539\n",
            "Epoch 00002: val_accuracy improved from 0.42680 to 0.51330, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5150 - accuracy: 0.4540 - val_loss: 1.3562 - val_accuracy: 0.5133\n",
            "Epoch 3/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.3565 - accuracy: 0.5121\n",
            "Epoch 00003: val_accuracy improved from 0.51330 to 0.54890, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3563 - accuracy: 0.5120 - val_loss: 1.2800 - val_accuracy: 0.5489\n",
            "Epoch 4/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.2494 - accuracy: 0.5511\n",
            "Epoch 00004: val_accuracy improved from 0.54890 to 0.58990, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2494 - accuracy: 0.5511 - val_loss: 1.1746 - val_accuracy: 0.5899\n",
            "Epoch 5/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.1728 - accuracy: 0.5820\n",
            "Epoch 00005: val_accuracy improved from 0.58990 to 0.61710, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1725 - accuracy: 0.5821 - val_loss: 1.0884 - val_accuracy: 0.6171\n",
            "Epoch 6/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.1063 - accuracy: 0.6091\n",
            "Epoch 00006: val_accuracy improved from 0.61710 to 0.63860, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1061 - accuracy: 0.6092 - val_loss: 1.0313 - val_accuracy: 0.6386\n",
            "Epoch 7/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.0488 - accuracy: 0.6287\n",
            "Epoch 00007: val_accuracy improved from 0.63860 to 0.65410, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0489 - accuracy: 0.6288 - val_loss: 0.9931 - val_accuracy: 0.6541\n",
            "Epoch 8/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.9994 - accuracy: 0.6470\n",
            "Epoch 00008: val_accuracy improved from 0.65410 to 0.66440, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9995 - accuracy: 0.6471 - val_loss: 0.9572 - val_accuracy: 0.6644\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.9583 - accuracy: 0.6628\n",
            "Epoch 00009: val_accuracy improved from 0.66440 to 0.67170, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9583 - accuracy: 0.6628 - val_loss: 0.9367 - val_accuracy: 0.6717\n",
            "Epoch 10/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.9273 - accuracy: 0.6746\n",
            "Epoch 00010: val_accuracy improved from 0.67170 to 0.68730, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9267 - accuracy: 0.6747 - val_loss: 0.8817 - val_accuracy: 0.6873\n",
            "Epoch 11/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8970 - accuracy: 0.6891\n",
            "Epoch 00011: val_accuracy improved from 0.68730 to 0.69950, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8970 - accuracy: 0.6891 - val_loss: 0.8715 - val_accuracy: 0.6995\n",
            "Epoch 12/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.8697 - accuracy: 0.6980\n",
            "Epoch 00012: val_accuracy improved from 0.69950 to 0.70340, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8693 - accuracy: 0.6982 - val_loss: 0.8516 - val_accuracy: 0.7034\n",
            "Epoch 13/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.8402 - accuracy: 0.7079\n",
            "Epoch 00013: val_accuracy improved from 0.70340 to 0.70990, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8398 - accuracy: 0.7079 - val_loss: 0.8313 - val_accuracy: 0.7099\n",
            "Epoch 14/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8207 - accuracy: 0.7143\n",
            "Epoch 00014: val_accuracy improved from 0.70990 to 0.71020, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8207 - accuracy: 0.7143 - val_loss: 0.8379 - val_accuracy: 0.7102\n",
            "Epoch 15/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.8028 - accuracy: 0.7219\n",
            "Epoch 00015: val_accuracy improved from 0.71020 to 0.72700, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8027 - accuracy: 0.7221 - val_loss: 0.7956 - val_accuracy: 0.7270\n",
            "Epoch 16/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7833 - accuracy: 0.7299\n",
            "Epoch 00016: val_accuracy did not improve from 0.72700\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7833 - accuracy: 0.7298 - val_loss: 0.8080 - val_accuracy: 0.7189\n",
            "Epoch 17/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7662 - accuracy: 0.7367\n",
            "Epoch 00017: val_accuracy improved from 0.72700 to 0.73020, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7663 - accuracy: 0.7367 - val_loss: 0.7772 - val_accuracy: 0.7302\n",
            "Epoch 18/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7537 - accuracy: 0.7426\n",
            "Epoch 00018: val_accuracy improved from 0.73020 to 0.73630, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7538 - accuracy: 0.7426 - val_loss: 0.7699 - val_accuracy: 0.7363\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7423 - accuracy: 0.7430\n",
            "Epoch 00019: val_accuracy improved from 0.73630 to 0.74080, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7423 - accuracy: 0.7430 - val_loss: 0.7507 - val_accuracy: 0.7408\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7360 - accuracy: 0.7460\n",
            "Epoch 00020: val_accuracy did not improve from 0.74080\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7360 - accuracy: 0.7460 - val_loss: 0.7624 - val_accuracy: 0.7408\n",
            "Epoch 21/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7225 - accuracy: 0.7534\n",
            "Epoch 00021: val_accuracy did not improve from 0.74080\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7225 - accuracy: 0.7534 - val_loss: 0.7615 - val_accuracy: 0.7389\n",
            "Epoch 22/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7071 - accuracy: 0.7560\n",
            "Epoch 00022: val_accuracy improved from 0.74080 to 0.74820, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7070 - accuracy: 0.7560 - val_loss: 0.7338 - val_accuracy: 0.7482\n",
            "Epoch 23/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7009 - accuracy: 0.7624\n",
            "Epoch 00023: val_accuracy improved from 0.74820 to 0.75180, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7016 - accuracy: 0.7621 - val_loss: 0.7353 - val_accuracy: 0.7518\n",
            "Epoch 24/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6962 - accuracy: 0.7645\n",
            "Epoch 00024: val_accuracy improved from 0.75180 to 0.75400, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6958 - accuracy: 0.7649 - val_loss: 0.7238 - val_accuracy: 0.7540\n",
            "Epoch 25/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6882 - accuracy: 0.7686\n",
            "Epoch 00025: val_accuracy did not improve from 0.75400\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6883 - accuracy: 0.7686 - val_loss: 0.7327 - val_accuracy: 0.7523\n",
            "Epoch 26/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6852 - accuracy: 0.7676\n",
            "Epoch 00026: val_accuracy did not improve from 0.75400\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6851 - accuracy: 0.7675 - val_loss: 0.7288 - val_accuracy: 0.7536\n",
            "Epoch 27/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6702 - accuracy: 0.7729\n",
            "Epoch 00027: val_accuracy improved from 0.75400 to 0.75450, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6703 - accuracy: 0.7728 - val_loss: 0.7313 - val_accuracy: 0.7545\n",
            "Epoch 28/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6703 - accuracy: 0.7713\n",
            "Epoch 00028: val_accuracy improved from 0.75450 to 0.76130, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6703 - accuracy: 0.7713 - val_loss: 0.7130 - val_accuracy: 0.7613\n",
            "Epoch 29/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6664 - accuracy: 0.7741\n",
            "Epoch 00029: val_accuracy improved from 0.76130 to 0.76540, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6661 - accuracy: 0.7742 - val_loss: 0.7036 - val_accuracy: 0.7654\n",
            "Epoch 30/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6659 - accuracy: 0.7731\n",
            "Epoch 00030: val_accuracy did not improve from 0.76540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6664 - accuracy: 0.7729 - val_loss: 0.7394 - val_accuracy: 0.7607\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6603 - accuracy: 0.7766\n",
            "Epoch 00031: val_accuracy did not improve from 0.76540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6603 - accuracy: 0.7766 - val_loss: 0.7059 - val_accuracy: 0.7626\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6499 - accuracy: 0.7812\n",
            "Epoch 00032: val_accuracy did not improve from 0.76540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6499 - accuracy: 0.7812 - val_loss: 0.7130 - val_accuracy: 0.7617\n",
            "Epoch 33/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6532 - accuracy: 0.7829\n",
            "Epoch 00033: val_accuracy did not improve from 0.76540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6533 - accuracy: 0.7828 - val_loss: 0.7308 - val_accuracy: 0.7628\n",
            "Epoch 34/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6437 - accuracy: 0.7818\n",
            "Epoch 00034: val_accuracy improved from 0.76540 to 0.76560, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6439 - accuracy: 0.7819 - val_loss: 0.7071 - val_accuracy: 0.7656\n",
            "Epoch 35/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6467 - accuracy: 0.7839\n",
            "Epoch 00035: val_accuracy did not improve from 0.76560\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6468 - accuracy: 0.7840 - val_loss: 0.7322 - val_accuracy: 0.7609\n",
            "Epoch 36/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6429 - accuracy: 0.7859\n",
            "Epoch 00036: val_accuracy did not improve from 0.76560\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6435 - accuracy: 0.7858 - val_loss: 0.7641 - val_accuracy: 0.7563\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.7859\n",
            "Epoch 00037: val_accuracy did not improve from 0.76560\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6392 - accuracy: 0.7859 - val_loss: 0.7035 - val_accuracy: 0.7641\n",
            "Epoch 38/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6352 - accuracy: 0.7894\n",
            "Epoch 00038: val_accuracy improved from 0.76560 to 0.77270, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6355 - accuracy: 0.7895 - val_loss: 0.6909 - val_accuracy: 0.7727\n",
            "Epoch 39/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.7883\n",
            "Epoch 00039: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6339 - accuracy: 0.7883 - val_loss: 0.6993 - val_accuracy: 0.7675\n",
            "Epoch 40/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6336 - accuracy: 0.7876\n",
            "Epoch 00040: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6338 - accuracy: 0.7877 - val_loss: 0.7466 - val_accuracy: 0.7643\n",
            "Epoch 41/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6300 - accuracy: 0.7920\n",
            "Epoch 00041: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6304 - accuracy: 0.7919 - val_loss: 0.7100 - val_accuracy: 0.7678\n",
            "Epoch 42/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6304 - accuracy: 0.7890\n",
            "Epoch 00042: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6303 - accuracy: 0.7891 - val_loss: 0.7110 - val_accuracy: 0.7699\n",
            "Epoch 43/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6228 - accuracy: 0.7945\n",
            "Epoch 00043: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6230 - accuracy: 0.7946 - val_loss: 0.7164 - val_accuracy: 0.7707\n",
            "Epoch 44/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6270 - accuracy: 0.7922\n",
            "Epoch 00044: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6268 - accuracy: 0.7923 - val_loss: 0.6961 - val_accuracy: 0.7705\n",
            "Epoch 45/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6262 - accuracy: 0.7927\n",
            "Epoch 00045: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6257 - accuracy: 0.7927 - val_loss: 0.7133 - val_accuracy: 0.7724\n",
            "Epoch 46/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6172 - accuracy: 0.7948\n",
            "Epoch 00046: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6165 - accuracy: 0.7950 - val_loss: 0.7355 - val_accuracy: 0.7645\n",
            "Epoch 47/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6159 - accuracy: 0.7968\n",
            "Epoch 00047: val_accuracy improved from 0.77270 to 0.77610, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6154 - accuracy: 0.7970 - val_loss: 0.6958 - val_accuracy: 0.7761\n",
            "Epoch 48/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6170 - accuracy: 0.7950\n",
            "Epoch 00048: val_accuracy did not improve from 0.77610\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6172 - accuracy: 0.7950 - val_loss: 0.7139 - val_accuracy: 0.7745\n",
            "Epoch 49/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6144 - accuracy: 0.7977\n",
            "Epoch 00049: val_accuracy did not improve from 0.77610\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6145 - accuracy: 0.7976 - val_loss: 0.7473 - val_accuracy: 0.7681\n",
            "Epoch 50/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6164 - accuracy: 0.7963\n",
            "Epoch 00050: val_accuracy did not improve from 0.77610\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6173 - accuracy: 0.7959 - val_loss: 0.7026 - val_accuracy: 0.7723\n",
            "Epoch 51/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6087 - accuracy: 0.8000\n",
            "Epoch 00051: val_accuracy did not improve from 0.77610\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6085 - accuracy: 0.8001 - val_loss: 0.7156 - val_accuracy: 0.7670\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.7982\n",
            "Epoch 00052: val_accuracy improved from 0.77610 to 0.77700, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6142 - accuracy: 0.7982 - val_loss: 0.7024 - val_accuracy: 0.7770\n",
            "Epoch 53/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6123 - accuracy: 0.8002\n",
            "Epoch 00053: val_accuracy did not improve from 0.77700\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6123 - accuracy: 0.8002 - val_loss: 0.8028 - val_accuracy: 0.7708\n",
            "Epoch 54/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6112 - accuracy: 0.7983\n",
            "Epoch 00054: val_accuracy did not improve from 0.77700\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6116 - accuracy: 0.7981 - val_loss: 0.7535 - val_accuracy: 0.7729\n",
            "Epoch 55/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6077 - accuracy: 0.8016\n",
            "Epoch 00055: val_accuracy did not improve from 0.77700\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6082 - accuracy: 0.8015 - val_loss: 0.7336 - val_accuracy: 0.7744\n",
            "Epoch 56/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6079 - accuracy: 0.7977\n",
            "Epoch 00056: val_accuracy improved from 0.77700 to 0.77740, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6074 - accuracy: 0.7978 - val_loss: 0.6929 - val_accuracy: 0.7774\n",
            "Epoch 57/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6104 - accuracy: 0.8001\n",
            "Epoch 00057: val_accuracy did not improve from 0.77740\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6100 - accuracy: 0.8001 - val_loss: 0.6957 - val_accuracy: 0.7765\n",
            "Epoch 58/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6090 - accuracy: 0.8008\n",
            "Epoch 00058: val_accuracy improved from 0.77740 to 0.78530, saving model to best_model1.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6085 - accuracy: 0.8008 - val_loss: 0.6715 - val_accuracy: 0.7853\n",
            "Epoch 59/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6023 - accuracy: 0.8024\n",
            "Epoch 00059: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6021 - accuracy: 0.8025 - val_loss: 0.7517 - val_accuracy: 0.7668\n",
            "Epoch 60/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6007 - accuracy: 0.8033\n",
            "Epoch 00060: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6007 - accuracy: 0.8032 - val_loss: 0.7332 - val_accuracy: 0.7705\n",
            "Epoch 61/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6019 - accuracy: 0.8031\n",
            "Epoch 00061: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6019 - accuracy: 0.8031 - val_loss: 0.7174 - val_accuracy: 0.7749\n",
            "Epoch 62/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6033 - accuracy: 0.8031\n",
            "Epoch 00062: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6031 - accuracy: 0.8031 - val_loss: 0.6836 - val_accuracy: 0.7813\n",
            "Epoch 63/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6045 - accuracy: 0.8024\n",
            "Epoch 00063: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6047 - accuracy: 0.8023 - val_loss: 0.7813 - val_accuracy: 0.7736\n",
            "Epoch 64/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6077 - accuracy: 0.7993\n",
            "Epoch 00064: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6078 - accuracy: 0.7993 - val_loss: 0.7158 - val_accuracy: 0.7769\n",
            "Epoch 65/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6080 - accuracy: 0.8022\n",
            "Epoch 00065: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6080 - accuracy: 0.8023 - val_loss: 0.7933 - val_accuracy: 0.7698\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.8001\n",
            "Epoch 00066: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6025 - accuracy: 0.8001 - val_loss: 0.7227 - val_accuracy: 0.7757\n",
            "Epoch 67/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6013 - accuracy: 0.8029\n",
            "Epoch 00067: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6013 - accuracy: 0.8029 - val_loss: 0.7313 - val_accuracy: 0.7713\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.8032\n",
            "Epoch 00068: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6069 - accuracy: 0.8032 - val_loss: 0.7350 - val_accuracy: 0.7772\n",
            "Epoch 69/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6004 - accuracy: 0.8058\n",
            "Epoch 00069: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6005 - accuracy: 0.8058 - val_loss: 0.7462 - val_accuracy: 0.7781\n",
            "Epoch 70/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6038 - accuracy: 0.8035\n",
            "Epoch 00070: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6040 - accuracy: 0.8033 - val_loss: 0.7705 - val_accuracy: 0.7737\n",
            "Epoch 71/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.5971 - accuracy: 0.8058\n",
            "Epoch 00071: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5970 - accuracy: 0.8059 - val_loss: 0.7602 - val_accuracy: 0.7818\n",
            "Epoch 72/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6066 - accuracy: 0.8030\n",
            "Epoch 00072: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6063 - accuracy: 0.8031 - val_loss: 0.6907 - val_accuracy: 0.7795\n",
            "Epoch 73/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.6072 - accuracy: 0.8041\n",
            "Epoch 00073: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6077 - accuracy: 0.8040 - val_loss: 0.8108 - val_accuracy: 0.7636\n",
            "Epoch 74/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6052 - accuracy: 0.8016\n",
            "Epoch 00074: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6051 - accuracy: 0.8016 - val_loss: 0.7456 - val_accuracy: 0.7802\n",
            "Epoch 75/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6028 - accuracy: 0.8037\n",
            "Epoch 00075: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6035 - accuracy: 0.8034 - val_loss: 0.8936 - val_accuracy: 0.7675\n",
            "Epoch 76/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6051 - accuracy: 0.8047\n",
            "Epoch 00076: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6049 - accuracy: 0.8048 - val_loss: 0.7273 - val_accuracy: 0.7739\n",
            "Epoch 77/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6050 - accuracy: 0.8027\n",
            "Epoch 00077: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6056 - accuracy: 0.8025 - val_loss: 0.7625 - val_accuracy: 0.7678\n",
            "Epoch 78/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6118 - accuracy: 0.8016\n",
            "Epoch 00078: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6120 - accuracy: 0.8016 - val_loss: 0.8595 - val_accuracy: 0.7654\n",
            "Epoch 79/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6021 - accuracy: 0.8051\n",
            "Epoch 00079: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6020 - accuracy: 0.8051 - val_loss: 0.8010 - val_accuracy: 0.7770\n",
            "Epoch 80/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6097 - accuracy: 0.8028\n",
            "Epoch 00080: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6098 - accuracy: 0.8026 - val_loss: 0.8626 - val_accuracy: 0.7465\n",
            "Epoch 81/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6039 - accuracy: 0.8031\n",
            "Epoch 00081: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6036 - accuracy: 0.8033 - val_loss: 0.8406 - val_accuracy: 0.7533\n",
            "Epoch 82/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.6093 - accuracy: 0.8044\n",
            "Epoch 00082: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6097 - accuracy: 0.8041 - val_loss: 0.7052 - val_accuracy: 0.7842\n",
            "Epoch 83/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6073 - accuracy: 0.8030\n",
            "Epoch 00083: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6074 - accuracy: 0.8030 - val_loss: 0.7809 - val_accuracy: 0.7790\n",
            "Epoch 84/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6065 - accuracy: 0.8034\n",
            "Epoch 00084: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6060 - accuracy: 0.8036 - val_loss: 0.7489 - val_accuracy: 0.7806\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6088 - accuracy: 0.8022\n",
            "Epoch 00085: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6088 - accuracy: 0.8022 - val_loss: 0.8801 - val_accuracy: 0.7700\n",
            "Epoch 86/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6092 - accuracy: 0.8017\n",
            "Epoch 00086: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6091 - accuracy: 0.8017 - val_loss: 0.7062 - val_accuracy: 0.7817\n",
            "Epoch 87/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6026 - accuracy: 0.8047\n",
            "Epoch 00087: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6023 - accuracy: 0.8047 - val_loss: 0.7199 - val_accuracy: 0.7824\n",
            "Epoch 88/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6041 - accuracy: 0.8037\n",
            "Epoch 00088: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6043 - accuracy: 0.8036 - val_loss: 0.7259 - val_accuracy: 0.7776\n",
            "Epoch 89/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6079 - accuracy: 0.8012\n",
            "Epoch 00089: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6079 - accuracy: 0.8012 - val_loss: 0.7092 - val_accuracy: 0.7789\n",
            "Epoch 90/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6120 - accuracy: 0.8022\n",
            "Epoch 00090: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6120 - accuracy: 0.8022 - val_loss: 0.7957 - val_accuracy: 0.7678\n",
            "Epoch 91/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6102 - accuracy: 0.8025\n",
            "Epoch 00091: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6099 - accuracy: 0.8026 - val_loss: 0.7292 - val_accuracy: 0.7765\n",
            "Epoch 92/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6061 - accuracy: 0.8019\n",
            "Epoch 00092: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6066 - accuracy: 0.8018 - val_loss: 0.8138 - val_accuracy: 0.7718\n",
            "Epoch 93/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6043 - accuracy: 0.8027\n",
            "Epoch 00093: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6042 - accuracy: 0.8030 - val_loss: 0.9011 - val_accuracy: 0.7553\n",
            "Epoch 94/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.8039\n",
            "Epoch 00094: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6078 - accuracy: 0.8040 - val_loss: 0.7272 - val_accuracy: 0.7688\n",
            "Epoch 95/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6053 - accuracy: 0.8051\n",
            "Epoch 00095: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6052 - accuracy: 0.8051 - val_loss: 0.7937 - val_accuracy: 0.7752\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6042 - accuracy: 0.8065\n",
            "Epoch 00096: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6042 - accuracy: 0.8065 - val_loss: 0.7604 - val_accuracy: 0.7745\n",
            "Epoch 97/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6048 - accuracy: 0.8027\n",
            "Epoch 00097: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6044 - accuracy: 0.8028 - val_loss: 0.7280 - val_accuracy: 0.7717\n",
            "Epoch 98/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6078 - accuracy: 0.8033\n",
            "Epoch 00098: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6074 - accuracy: 0.8033 - val_loss: 0.7193 - val_accuracy: 0.7771\n",
            "Epoch 99/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6101 - accuracy: 0.8024\n",
            "Epoch 00099: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6104 - accuracy: 0.8023 - val_loss: 0.8248 - val_accuracy: 0.7620\n",
            "Epoch 100/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6053 - accuracy: 0.8042\n",
            "Epoch 00100: val_accuracy did not improve from 0.78530\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6053 - accuracy: 0.8042 - val_loss: 0.7963 - val_accuracy: 0.7547\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.7367 - accuracy: 0.3736\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.46190, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 1.7363 - accuracy: 0.3737 - val_loss: 1.4924 - val_accuracy: 0.4619\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.4643 - accuracy: 0.4708\n",
            "Epoch 00002: val_accuracy improved from 0.46190 to 0.53260, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 1.4643 - accuracy: 0.4708 - val_loss: 1.3262 - val_accuracy: 0.5326\n",
            "Epoch 3/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.3279 - accuracy: 0.5265\n",
            "Epoch 00003: val_accuracy improved from 0.53260 to 0.53690, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 1.3275 - accuracy: 0.5266 - val_loss: 1.3004 - val_accuracy: 0.5369\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.2251 - accuracy: 0.5656\n",
            "Epoch 00004: val_accuracy improved from 0.53690 to 0.60070, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 19ms/step - loss: 1.2251 - accuracy: 0.5656 - val_loss: 1.1389 - val_accuracy: 0.6007\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.1510 - accuracy: 0.5935\n",
            "Epoch 00005: val_accuracy improved from 0.60070 to 0.64100, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 1.1510 - accuracy: 0.5935 - val_loss: 1.0301 - val_accuracy: 0.6410\n",
            "Epoch 6/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.0815 - accuracy: 0.6180\n",
            "Epoch 00006: val_accuracy improved from 0.64100 to 0.65490, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 1.0814 - accuracy: 0.6180 - val_loss: 0.9932 - val_accuracy: 0.6549\n",
            "Epoch 7/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.0306 - accuracy: 0.6407\n",
            "Epoch 00007: val_accuracy improved from 0.65490 to 0.67330, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 1.0306 - accuracy: 0.6406 - val_loss: 0.9358 - val_accuracy: 0.6733\n",
            "Epoch 8/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.9908 - accuracy: 0.6510\n",
            "Epoch 00008: val_accuracy improved from 0.67330 to 0.67600, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.9908 - accuracy: 0.6510 - val_loss: 0.9347 - val_accuracy: 0.6760\n",
            "Epoch 9/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.9510 - accuracy: 0.6679\n",
            "Epoch 00009: val_accuracy improved from 0.67600 to 0.69580, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.9512 - accuracy: 0.6679 - val_loss: 0.8850 - val_accuracy: 0.6958\n",
            "Epoch 10/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.9207 - accuracy: 0.6783\n",
            "Epoch 00010: val_accuracy improved from 0.69580 to 0.70140, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.9205 - accuracy: 0.6784 - val_loss: 0.8704 - val_accuracy: 0.7014\n",
            "Epoch 11/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8934 - accuracy: 0.6886\n",
            "Epoch 00011: val_accuracy improved from 0.70140 to 0.70260, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8932 - accuracy: 0.6887 - val_loss: 0.8512 - val_accuracy: 0.7026\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8614 - accuracy: 0.6987\n",
            "Epoch 00012: val_accuracy improved from 0.70260 to 0.70830, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.8614 - accuracy: 0.6987 - val_loss: 0.8501 - val_accuracy: 0.7083\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8343 - accuracy: 0.7096\n",
            "Epoch 00013: val_accuracy improved from 0.70830 to 0.71660, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.8343 - accuracy: 0.7096 - val_loss: 0.8174 - val_accuracy: 0.7166\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8129 - accuracy: 0.7161\n",
            "Epoch 00014: val_accuracy did not improve from 0.71660\n",
            "1250/1250 [==============================] - 23s 19ms/step - loss: 0.8129 - accuracy: 0.7161 - val_loss: 0.8487 - val_accuracy: 0.7127\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7858 - accuracy: 0.7260\n",
            "Epoch 00015: val_accuracy improved from 0.71660 to 0.74080, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7858 - accuracy: 0.7260 - val_loss: 0.7632 - val_accuracy: 0.7408\n",
            "Epoch 16/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7650 - accuracy: 0.7326\n",
            "Epoch 00016: val_accuracy did not improve from 0.74080\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7650 - accuracy: 0.7327 - val_loss: 0.8038 - val_accuracy: 0.7295\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7426 - accuracy: 0.7394\n",
            "Epoch 00017: val_accuracy did not improve from 0.74080\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7426 - accuracy: 0.7394 - val_loss: 0.7795 - val_accuracy: 0.7348\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7214 - accuracy: 0.7491\n",
            "Epoch 00018: val_accuracy improved from 0.74080 to 0.74940, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 24s 19ms/step - loss: 0.7214 - accuracy: 0.7491 - val_loss: 0.7500 - val_accuracy: 0.7494\n",
            "Epoch 19/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7033 - accuracy: 0.7557\n",
            "Epoch 00019: val_accuracy did not improve from 0.74940\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.7034 - accuracy: 0.7558 - val_loss: 0.7332 - val_accuracy: 0.7429\n",
            "Epoch 20/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.7619\n",
            "Epoch 00020: val_accuracy improved from 0.74940 to 0.75460, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6859 - accuracy: 0.7620 - val_loss: 0.7185 - val_accuracy: 0.7546\n",
            "Epoch 21/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6738 - accuracy: 0.7666\n",
            "Epoch 00021: val_accuracy improved from 0.75460 to 0.75770, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6739 - accuracy: 0.7666 - val_loss: 0.7240 - val_accuracy: 0.7577\n",
            "Epoch 22/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6554 - accuracy: 0.7728\n",
            "Epoch 00022: val_accuracy improved from 0.75770 to 0.75840, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 19ms/step - loss: 0.6551 - accuracy: 0.7729 - val_loss: 0.7065 - val_accuracy: 0.7584\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6422 - accuracy: 0.7752\n",
            "Epoch 00023: val_accuracy improved from 0.75840 to 0.76310, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6422 - accuracy: 0.7752 - val_loss: 0.7105 - val_accuracy: 0.7631\n",
            "Epoch 24/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6293 - accuracy: 0.7828\n",
            "Epoch 00024: val_accuracy improved from 0.76310 to 0.77670, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6294 - accuracy: 0.7827 - val_loss: 0.6479 - val_accuracy: 0.7767\n",
            "Epoch 25/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6149 - accuracy: 0.7864\n",
            "Epoch 00025: val_accuracy did not improve from 0.77670\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6148 - accuracy: 0.7863 - val_loss: 0.6939 - val_accuracy: 0.7721\n",
            "Epoch 26/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6042 - accuracy: 0.7905\n",
            "Epoch 00026: val_accuracy improved from 0.77670 to 0.78250, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.6041 - accuracy: 0.7906 - val_loss: 0.6503 - val_accuracy: 0.7825\n",
            "Epoch 27/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5933 - accuracy: 0.7953\n",
            "Epoch 00027: val_accuracy did not improve from 0.78250\n",
            "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5933 - accuracy: 0.7954 - val_loss: 0.6686 - val_accuracy: 0.7759\n",
            "Epoch 28/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5831 - accuracy: 0.7980\n",
            "Epoch 00028: val_accuracy did not improve from 0.78250\n",
            "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5833 - accuracy: 0.7979 - val_loss: 0.6774 - val_accuracy: 0.7740\n",
            "Epoch 29/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.5744 - accuracy: 0.8012\n",
            "Epoch 00029: val_accuracy did not improve from 0.78250\n",
            "1250/1250 [==============================] - 24s 19ms/step - loss: 0.5746 - accuracy: 0.8011 - val_loss: 0.6530 - val_accuracy: 0.7800\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.5622 - accuracy: 0.8062\n",
            "Epoch 00030: val_accuracy did not improve from 0.78250\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.5622 - accuracy: 0.8062 - val_loss: 0.6768 - val_accuracy: 0.7712\n",
            "Epoch 31/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5541 - accuracy: 0.8096\n",
            "Epoch 00031: val_accuracy improved from 0.78250 to 0.78550, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.5543 - accuracy: 0.8096 - val_loss: 0.6412 - val_accuracy: 0.7855\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.5406 - accuracy: 0.8142\n",
            "Epoch 00032: val_accuracy improved from 0.78550 to 0.78840, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5406 - accuracy: 0.8142 - val_loss: 0.6484 - val_accuracy: 0.7884\n",
            "Epoch 33/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5383 - accuracy: 0.8137\n",
            "Epoch 00033: val_accuracy did not improve from 0.78840\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5382 - accuracy: 0.8137 - val_loss: 0.6800 - val_accuracy: 0.7776\n",
            "Epoch 34/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5277 - accuracy: 0.8182\n",
            "Epoch 00034: val_accuracy improved from 0.78840 to 0.79790, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.5274 - accuracy: 0.8182 - val_loss: 0.6239 - val_accuracy: 0.7979\n",
            "Epoch 35/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.5189 - accuracy: 0.8210\n",
            "Epoch 00035: val_accuracy improved from 0.79790 to 0.79990, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5187 - accuracy: 0.8210 - val_loss: 0.6330 - val_accuracy: 0.7999\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.5161 - accuracy: 0.8223\n",
            "Epoch 00036: val_accuracy did not improve from 0.79990\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5161 - accuracy: 0.8223 - val_loss: 0.6787 - val_accuracy: 0.7854\n",
            "Epoch 37/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5132 - accuracy: 0.8232\n",
            "Epoch 00037: val_accuracy did not improve from 0.79990\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5132 - accuracy: 0.8232 - val_loss: 0.6469 - val_accuracy: 0.7983\n",
            "Epoch 38/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5027 - accuracy: 0.8278\n",
            "Epoch 00038: val_accuracy did not improve from 0.79990\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.5026 - accuracy: 0.8279 - val_loss: 0.6222 - val_accuracy: 0.7969\n",
            "Epoch 39/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4920 - accuracy: 0.8318\n",
            "Epoch 00039: val_accuracy did not improve from 0.79990\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4921 - accuracy: 0.8317 - val_loss: 0.6673 - val_accuracy: 0.7783\n",
            "Epoch 40/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4906 - accuracy: 0.8319\n",
            "Epoch 00040: val_accuracy improved from 0.79990 to 0.80200, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4907 - accuracy: 0.8319 - val_loss: 0.5920 - val_accuracy: 0.8020\n",
            "Epoch 41/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.4864 - accuracy: 0.8343\n",
            "Epoch 00041: val_accuracy did not improve from 0.80200\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4866 - accuracy: 0.8342 - val_loss: 0.6488 - val_accuracy: 0.7911\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.8360\n",
            "Epoch 00042: val_accuracy did not improve from 0.80200\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4762 - accuracy: 0.8360 - val_loss: 0.6195 - val_accuracy: 0.7940\n",
            "Epoch 43/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4705 - accuracy: 0.8371\n",
            "Epoch 00043: val_accuracy did not improve from 0.80200\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4708 - accuracy: 0.8370 - val_loss: 0.6248 - val_accuracy: 0.7914\n",
            "Epoch 44/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.4694 - accuracy: 0.8367\n",
            "Epoch 00044: val_accuracy improved from 0.80200 to 0.80920, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4695 - accuracy: 0.8368 - val_loss: 0.6034 - val_accuracy: 0.8092\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4680 - accuracy: 0.8397\n",
            "Epoch 00045: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4680 - accuracy: 0.8397 - val_loss: 0.6150 - val_accuracy: 0.8037\n",
            "Epoch 46/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4606 - accuracy: 0.8429\n",
            "Epoch 00046: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4607 - accuracy: 0.8429 - val_loss: 0.6830 - val_accuracy: 0.7919\n",
            "Epoch 47/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4578 - accuracy: 0.8457\n",
            "Epoch 00047: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4578 - accuracy: 0.8457 - val_loss: 0.6237 - val_accuracy: 0.8029\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.8433\n",
            "Epoch 00048: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4527 - accuracy: 0.8433 - val_loss: 0.6376 - val_accuracy: 0.8073\n",
            "Epoch 49/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4507 - accuracy: 0.8480\n",
            "Epoch 00049: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4507 - accuracy: 0.8480 - val_loss: 0.6382 - val_accuracy: 0.8013\n",
            "Epoch 50/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4471 - accuracy: 0.8489\n",
            "Epoch 00050: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4470 - accuracy: 0.8489 - val_loss: 0.6335 - val_accuracy: 0.8043\n",
            "Epoch 51/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4461 - accuracy: 0.8468\n",
            "Epoch 00051: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4461 - accuracy: 0.8468 - val_loss: 0.6465 - val_accuracy: 0.7967\n",
            "Epoch 52/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4427 - accuracy: 0.8494\n",
            "Epoch 00052: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4426 - accuracy: 0.8494 - val_loss: 0.6402 - val_accuracy: 0.8045\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.8518\n",
            "Epoch 00053: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4356 - accuracy: 0.8518 - val_loss: 0.6369 - val_accuracy: 0.8081\n",
            "Epoch 54/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8500\n",
            "Epoch 00054: val_accuracy did not improve from 0.80920\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4358 - accuracy: 0.8500 - val_loss: 0.6268 - val_accuracy: 0.7997\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.8502\n",
            "Epoch 00055: val_accuracy improved from 0.80920 to 0.81370, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4367 - accuracy: 0.8502 - val_loss: 0.6267 - val_accuracy: 0.8137\n",
            "Epoch 56/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8534\n",
            "Epoch 00056: val_accuracy did not improve from 0.81370\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4266 - accuracy: 0.8534 - val_loss: 0.6345 - val_accuracy: 0.7994\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.8541\n",
            "Epoch 00057: val_accuracy did not improve from 0.81370\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4265 - accuracy: 0.8541 - val_loss: 0.6587 - val_accuracy: 0.8025\n",
            "Epoch 58/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4284 - accuracy: 0.8548\n",
            "Epoch 00058: val_accuracy did not improve from 0.81370\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4286 - accuracy: 0.8548 - val_loss: 0.6693 - val_accuracy: 0.7915\n",
            "Epoch 59/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4276 - accuracy: 0.8543\n",
            "Epoch 00059: val_accuracy did not improve from 0.81370\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4275 - accuracy: 0.8543 - val_loss: 0.6749 - val_accuracy: 0.8060\n",
            "Epoch 60/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8544\n",
            "Epoch 00060: val_accuracy improved from 0.81370 to 0.81660, saving model to best_model2.h5\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4234 - accuracy: 0.8543 - val_loss: 0.5916 - val_accuracy: 0.8166\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4195 - accuracy: 0.8561\n",
            "Epoch 00061: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4195 - accuracy: 0.8561 - val_loss: 0.6735 - val_accuracy: 0.8022\n",
            "Epoch 62/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4218 - accuracy: 0.8551\n",
            "Epoch 00062: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4216 - accuracy: 0.8551 - val_loss: 0.7296 - val_accuracy: 0.7884\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.8573\n",
            "Epoch 00063: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 21s 17ms/step - loss: 0.4196 - accuracy: 0.8573 - val_loss: 0.5809 - val_accuracy: 0.8132\n",
            "Epoch 64/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.8551\n",
            "Epoch 00064: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4269 - accuracy: 0.8551 - val_loss: 0.6319 - val_accuracy: 0.8029\n",
            "Epoch 65/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.4182 - accuracy: 0.8568\n",
            "Epoch 00065: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4180 - accuracy: 0.8568 - val_loss: 0.6254 - val_accuracy: 0.8017\n",
            "Epoch 66/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4146 - accuracy: 0.8583\n",
            "Epoch 00066: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4147 - accuracy: 0.8582 - val_loss: 0.6405 - val_accuracy: 0.8024\n",
            "Epoch 67/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4231 - accuracy: 0.8561\n",
            "Epoch 00067: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4234 - accuracy: 0.8561 - val_loss: 0.6342 - val_accuracy: 0.8051\n",
            "Epoch 68/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.8570\n",
            "Epoch 00068: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4187 - accuracy: 0.8571 - val_loss: 0.7132 - val_accuracy: 0.7869\n",
            "Epoch 69/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.4151 - accuracy: 0.8603\n",
            "Epoch 00069: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4154 - accuracy: 0.8603 - val_loss: 0.7473 - val_accuracy: 0.7820\n",
            "Epoch 70/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.4175 - accuracy: 0.8558\n",
            "Epoch 00070: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4178 - accuracy: 0.8558 - val_loss: 0.7019 - val_accuracy: 0.7838\n",
            "Epoch 71/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4174 - accuracy: 0.8586\n",
            "Epoch 00071: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4173 - accuracy: 0.8586 - val_loss: 0.7821 - val_accuracy: 0.8035\n",
            "Epoch 72/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4231 - accuracy: 0.8542\n",
            "Epoch 00072: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4229 - accuracy: 0.8543 - val_loss: 0.6653 - val_accuracy: 0.8111\n",
            "Epoch 73/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4187 - accuracy: 0.8548\n",
            "Epoch 00073: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4186 - accuracy: 0.8548 - val_loss: 0.6318 - val_accuracy: 0.8107\n",
            "Epoch 74/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.4242 - accuracy: 0.8540\n",
            "Epoch 00074: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4240 - accuracy: 0.8540 - val_loss: 0.6835 - val_accuracy: 0.8042\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4232 - accuracy: 0.8550\n",
            "Epoch 00075: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4232 - accuracy: 0.8550 - val_loss: 0.6503 - val_accuracy: 0.8025\n",
            "Epoch 76/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8569\n",
            "Epoch 00076: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4218 - accuracy: 0.8569 - val_loss: 0.8294 - val_accuracy: 0.7669\n",
            "Epoch 77/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4154 - accuracy: 0.8573\n",
            "Epoch 00077: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4157 - accuracy: 0.8572 - val_loss: 0.6502 - val_accuracy: 0.7935\n",
            "Epoch 78/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8589\n",
            "Epoch 00078: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4205 - accuracy: 0.8588 - val_loss: 0.6570 - val_accuracy: 0.8002\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.8574\n",
            "Epoch 00079: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4183 - accuracy: 0.8574 - val_loss: 0.7110 - val_accuracy: 0.8023\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.8588\n",
            "Epoch 00080: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 17ms/step - loss: 0.4202 - accuracy: 0.8588 - val_loss: 0.7084 - val_accuracy: 0.8019\n",
            "Epoch 81/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4249 - accuracy: 0.8561\n",
            "Epoch 00081: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4249 - accuracy: 0.8561 - val_loss: 0.6758 - val_accuracy: 0.8042\n",
            "Epoch 82/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8538\n",
            "Epoch 00082: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4218 - accuracy: 0.8536 - val_loss: 0.6894 - val_accuracy: 0.7984\n",
            "Epoch 83/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8560\n",
            "Epoch 00083: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4206 - accuracy: 0.8561 - val_loss: 0.7492 - val_accuracy: 0.8048\n",
            "Epoch 84/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.8553\n",
            "Epoch 00084: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4263 - accuracy: 0.8554 - val_loss: 0.6797 - val_accuracy: 0.8004\n",
            "Epoch 85/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.4247 - accuracy: 0.8526\n",
            "Epoch 00085: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4250 - accuracy: 0.8524 - val_loss: 0.6945 - val_accuracy: 0.7906\n",
            "Epoch 86/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4253 - accuracy: 0.8538\n",
            "Epoch 00086: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4253 - accuracy: 0.8538 - val_loss: 0.7433 - val_accuracy: 0.8055\n",
            "Epoch 87/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4252 - accuracy: 0.8541\n",
            "Epoch 00087: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4249 - accuracy: 0.8542 - val_loss: 0.7304 - val_accuracy: 0.8100\n",
            "Epoch 88/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4257 - accuracy: 0.8543\n",
            "Epoch 00088: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4256 - accuracy: 0.8544 - val_loss: 0.6402 - val_accuracy: 0.8143\n",
            "Epoch 89/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4233 - accuracy: 0.8561\n",
            "Epoch 00089: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4233 - accuracy: 0.8561 - val_loss: 0.7455 - val_accuracy: 0.7900\n",
            "Epoch 90/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4245 - accuracy: 0.8552\n",
            "Epoch 00090: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4245 - accuracy: 0.8551 - val_loss: 0.8407 - val_accuracy: 0.7647\n",
            "Epoch 91/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4262 - accuracy: 0.8547\n",
            "Epoch 00091: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4261 - accuracy: 0.8547 - val_loss: 0.6493 - val_accuracy: 0.8025\n",
            "Epoch 92/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4232 - accuracy: 0.8571\n",
            "Epoch 00092: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4231 - accuracy: 0.8571 - val_loss: 0.6841 - val_accuracy: 0.8044\n",
            "Epoch 93/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4240 - accuracy: 0.8565\n",
            "Epoch 00093: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4242 - accuracy: 0.8564 - val_loss: 0.7999 - val_accuracy: 0.7779\n",
            "Epoch 94/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8572\n",
            "Epoch 00094: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4210 - accuracy: 0.8570 - val_loss: 0.6495 - val_accuracy: 0.8019\n",
            "Epoch 95/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4176 - accuracy: 0.8569\n",
            "Epoch 00095: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4177 - accuracy: 0.8569 - val_loss: 0.7001 - val_accuracy: 0.8093\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.8540\n",
            "Epoch 00096: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4259 - accuracy: 0.8540 - val_loss: 0.6774 - val_accuracy: 0.8036\n",
            "Epoch 97/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8549\n",
            "Epoch 00097: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 23s 18ms/step - loss: 0.4268 - accuracy: 0.8549 - val_loss: 0.7751 - val_accuracy: 0.8026\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.8540\n",
            "Epoch 00098: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4282 - accuracy: 0.8540 - val_loss: 0.6645 - val_accuracy: 0.7947\n",
            "Epoch 99/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8566\n",
            "Epoch 00099: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4203 - accuracy: 0.8566 - val_loss: 0.7590 - val_accuracy: 0.7837\n",
            "Epoch 100/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4242 - accuracy: 0.8557\n",
            "Epoch 00100: val_accuracy did not improve from 0.81660\n",
            "1250/1250 [==============================] - 22s 18ms/step - loss: 0.4243 - accuracy: 0.8557 - val_loss: 0.6786 - val_accuracy: 0.8119\n",
            "Not using data augmentation.\n",
            "Epoch 1/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.6265 - accuracy: 0.4161\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.50110, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6257 - accuracy: 0.4163 - val_loss: 1.3972 - val_accuracy: 0.5011\n",
            "Epoch 2/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.3106 - accuracy: 0.5328\n",
            "Epoch 00002: val_accuracy improved from 0.50110 to 0.56480, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3106 - accuracy: 0.5328 - val_loss: 1.2220 - val_accuracy: 0.5648\n",
            "Epoch 3/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.1521 - accuracy: 0.5929\n",
            "Epoch 00003: val_accuracy improved from 0.56480 to 0.59980, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1514 - accuracy: 0.5931 - val_loss: 1.1367 - val_accuracy: 0.5998\n",
            "Epoch 4/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.0372 - accuracy: 0.6364\n",
            "Epoch 00004: val_accuracy improved from 0.59980 to 0.62160, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0364 - accuracy: 0.6368 - val_loss: 1.1024 - val_accuracy: 0.6216\n",
            "Epoch 5/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.9470 - accuracy: 0.6699\n",
            "Epoch 00005: val_accuracy improved from 0.62160 to 0.66220, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9468 - accuracy: 0.6700 - val_loss: 0.9845 - val_accuracy: 0.6622\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8725 - accuracy: 0.6964\n",
            "Epoch 00006: val_accuracy improved from 0.66220 to 0.67790, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8725 - accuracy: 0.6964 - val_loss: 0.9316 - val_accuracy: 0.6779\n",
            "Epoch 7/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8051 - accuracy: 0.7218\n",
            "Epoch 00007: val_accuracy improved from 0.67790 to 0.67820, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8049 - accuracy: 0.7218 - val_loss: 0.9526 - val_accuracy: 0.6782\n",
            "Epoch 8/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.7432 - accuracy: 0.7440\n",
            "Epoch 00008: val_accuracy improved from 0.67820 to 0.69540, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7429 - accuracy: 0.7439 - val_loss: 0.9019 - val_accuracy: 0.6954\n",
            "Epoch 9/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6859 - accuracy: 0.7645\n",
            "Epoch 00009: val_accuracy improved from 0.69540 to 0.70520, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6851 - accuracy: 0.7646 - val_loss: 0.8735 - val_accuracy: 0.7052\n",
            "Epoch 10/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6252 - accuracy: 0.7850\n",
            "Epoch 00010: val_accuracy improved from 0.70520 to 0.70820, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6252 - accuracy: 0.7849 - val_loss: 0.8552 - val_accuracy: 0.7082\n",
            "Epoch 11/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5675 - accuracy: 0.8042\n",
            "Epoch 00011: val_accuracy improved from 0.70820 to 0.71560, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5673 - accuracy: 0.8042 - val_loss: 0.8599 - val_accuracy: 0.7156\n",
            "Epoch 12/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.5128 - accuracy: 0.8249\n",
            "Epoch 00012: val_accuracy improved from 0.71560 to 0.72900, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5126 - accuracy: 0.8249 - val_loss: 0.8345 - val_accuracy: 0.7290\n",
            "Epoch 13/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4539 - accuracy: 0.8472\n",
            "Epoch 00013: val_accuracy did not improve from 0.72900\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.4536 - accuracy: 0.8474 - val_loss: 0.8644 - val_accuracy: 0.7171\n",
            "Epoch 14/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.3992 - accuracy: 0.8651\n",
            "Epoch 00014: val_accuracy did not improve from 0.72900\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3991 - accuracy: 0.8652 - val_loss: 0.8814 - val_accuracy: 0.7262\n",
            "Epoch 15/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.3445 - accuracy: 0.8854\n",
            "Epoch 00015: val_accuracy improved from 0.72900 to 0.72980, saving model to best_model3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3446 - accuracy: 0.8854 - val_loss: 0.9152 - val_accuracy: 0.7298\n",
            "Epoch 16/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.9036\n",
            "Epoch 00016: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2940 - accuracy: 0.9035 - val_loss: 0.9899 - val_accuracy: 0.7132\n",
            "Epoch 17/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.2451 - accuracy: 0.9207\n",
            "Epoch 00017: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2450 - accuracy: 0.9206 - val_loss: 1.0388 - val_accuracy: 0.7193\n",
            "Epoch 18/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.2038 - accuracy: 0.9349\n",
            "Epoch 00018: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2039 - accuracy: 0.9349 - val_loss: 1.0838 - val_accuracy: 0.7187\n",
            "Epoch 19/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.1634 - accuracy: 0.9487\n",
            "Epoch 00019: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.1632 - accuracy: 0.9488 - val_loss: 1.1512 - val_accuracy: 0.7248\n",
            "Epoch 20/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.1321 - accuracy: 0.9588\n",
            "Epoch 00020: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.1322 - accuracy: 0.9588 - val_loss: 1.1899 - val_accuracy: 0.7180\n",
            "Epoch 21/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.1029 - accuracy: 0.9680\n",
            "Epoch 00021: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.1029 - accuracy: 0.9680 - val_loss: 1.3553 - val_accuracy: 0.7194\n",
            "Epoch 22/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0830 - accuracy: 0.9744\n",
            "Epoch 00022: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0830 - accuracy: 0.9744 - val_loss: 1.3713 - val_accuracy: 0.7172\n",
            "Epoch 23/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9803\n",
            "Epoch 00023: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0661 - accuracy: 0.9803 - val_loss: 1.4488 - val_accuracy: 0.7189\n",
            "Epoch 24/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 0.9840\n",
            "Epoch 00024: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0537 - accuracy: 0.9840 - val_loss: 1.6312 - val_accuracy: 0.7210\n",
            "Epoch 25/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0428 - accuracy: 0.9874\n",
            "Epoch 00025: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0428 - accuracy: 0.9874 - val_loss: 1.6971 - val_accuracy: 0.7195\n",
            "Epoch 26/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9888\n",
            "Epoch 00026: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0364 - accuracy: 0.9888 - val_loss: 1.7659 - val_accuracy: 0.7219\n",
            "Epoch 27/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.0301 - accuracy: 0.9911\n",
            "Epoch 00027: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0301 - accuracy: 0.9911 - val_loss: 1.7864 - val_accuracy: 0.7272\n",
            "Epoch 28/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9921\n",
            "Epoch 00028: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0267 - accuracy: 0.9921 - val_loss: 2.0263 - val_accuracy: 0.7175\n",
            "Epoch 29/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9926\n",
            "Epoch 00029: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0238 - accuracy: 0.9926 - val_loss: 2.0305 - val_accuracy: 0.7158\n",
            "Epoch 30/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.0217 - accuracy: 0.9933\n",
            "Epoch 00030: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0217 - accuracy: 0.9933 - val_loss: 1.9579 - val_accuracy: 0.7265\n",
            "Epoch 31/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9934\n",
            "Epoch 00031: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0198 - accuracy: 0.9934 - val_loss: 2.2208 - val_accuracy: 0.7102\n",
            "Epoch 32/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9949\n",
            "Epoch 00032: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0169 - accuracy: 0.9948 - val_loss: 2.3077 - val_accuracy: 0.7123\n",
            "Epoch 33/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9951\n",
            "Epoch 00033: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0160 - accuracy: 0.9951 - val_loss: 2.2657 - val_accuracy: 0.7108\n",
            "Epoch 34/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9953\n",
            "Epoch 00034: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0151 - accuracy: 0.9953 - val_loss: 2.3080 - val_accuracy: 0.7209\n",
            "Epoch 35/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.9953\n",
            "Epoch 00035: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0140 - accuracy: 0.9953 - val_loss: 2.4172 - val_accuracy: 0.7230\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9958\n",
            "Epoch 00036: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0140 - accuracy: 0.9958 - val_loss: 2.3014 - val_accuracy: 0.7229\n",
            "Epoch 37/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9957\n",
            "Epoch 00037: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 2.6072 - val_accuracy: 0.7155\n",
            "Epoch 38/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9959\n",
            "Epoch 00038: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 2.6235 - val_accuracy: 0.7111\n",
            "Epoch 39/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9960\n",
            "Epoch 00039: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 2.4797 - val_accuracy: 0.7260\n",
            "Epoch 40/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9964\n",
            "Epoch 00040: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0115 - accuracy: 0.9964 - val_loss: 2.4820 - val_accuracy: 0.7267\n",
            "Epoch 41/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9964\n",
            "Epoch 00041: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 2.5826 - val_accuracy: 0.7285\n",
            "Epoch 42/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9971\n",
            "Epoch 00042: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 2.6605 - val_accuracy: 0.7194\n",
            "Epoch 43/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9970\n",
            "Epoch 00043: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 2.9298 - val_accuracy: 0.7138\n",
            "Epoch 44/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9973\n",
            "Epoch 00044: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 2.8821 - val_accuracy: 0.7190\n",
            "Epoch 45/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0102 - accuracy: 0.9971\n",
            "Epoch 00045: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 2.8544 - val_accuracy: 0.7171\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9968\n",
            "Epoch 00046: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 2.9999 - val_accuracy: 0.7156\n",
            "Epoch 47/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9972\n",
            "Epoch 00047: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0086 - accuracy: 0.9972 - val_loss: 2.8762 - val_accuracy: 0.7178\n",
            "Epoch 48/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9971\n",
            "Epoch 00048: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0080 - accuracy: 0.9971 - val_loss: 2.9839 - val_accuracy: 0.7226\n",
            "Epoch 49/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9972\n",
            "Epoch 00049: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0083 - accuracy: 0.9972 - val_loss: 2.9580 - val_accuracy: 0.7223\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9979\n",
            "Epoch 00050: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 3.0316 - val_accuracy: 0.7205\n",
            "Epoch 51/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9971\n",
            "Epoch 00051: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0083 - accuracy: 0.9971 - val_loss: 3.1720 - val_accuracy: 0.7100\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 0.9979\n",
            "Epoch 00052: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0068 - accuracy: 0.9979 - val_loss: 3.2338 - val_accuracy: 0.7207\n",
            "Epoch 53/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9979\n",
            "Epoch 00053: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 3.1277 - val_accuracy: 0.7217\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9979\n",
            "Epoch 00054: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 3.1703 - val_accuracy: 0.7211\n",
            "Epoch 55/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9976\n",
            "Epoch 00055: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0067 - accuracy: 0.9976 - val_loss: 3.1279 - val_accuracy: 0.7271\n",
            "Epoch 56/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9978\n",
            "Epoch 00056: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 3.1827 - val_accuracy: 0.7164\n",
            "Epoch 57/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9976\n",
            "Epoch 00057: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 3.4287 - val_accuracy: 0.7161\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9978\n",
            "Epoch 00058: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 3.1053 - val_accuracy: 0.7221\n",
            "Epoch 59/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9978\n",
            "Epoch 00059: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 3.2381 - val_accuracy: 0.7260\n",
            "Epoch 60/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9987\n",
            "Epoch 00060: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 3.4689 - val_accuracy: 0.7142\n",
            "Epoch 61/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9981\n",
            "Epoch 00061: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 3.6263 - val_accuracy: 0.7177\n",
            "Epoch 62/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9981\n",
            "Epoch 00062: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 3.4640 - val_accuracy: 0.7231\n",
            "Epoch 63/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9982\n",
            "Epoch 00063: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 3.4068 - val_accuracy: 0.7234\n",
            "Epoch 64/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9979\n",
            "Epoch 00064: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 3.7107 - val_accuracy: 0.7123\n",
            "Epoch 65/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9987\n",
            "Epoch 00065: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 3.6297 - val_accuracy: 0.7200\n",
            "Epoch 66/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9985\n",
            "Epoch 00066: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 3.6798 - val_accuracy: 0.7233\n",
            "Epoch 67/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9982\n",
            "Epoch 00067: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 3.7968 - val_accuracy: 0.7203\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9987\n",
            "Epoch 00068: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 3.7776 - val_accuracy: 0.7174\n",
            "Epoch 69/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9984\n",
            "Epoch 00069: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 3.6331 - val_accuracy: 0.7257\n",
            "Epoch 70/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9987\n",
            "Epoch 00070: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 3.7174 - val_accuracy: 0.7251\n",
            "Epoch 71/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9984\n",
            "Epoch 00071: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 3.8457 - val_accuracy: 0.7247\n",
            "Epoch 72/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9983\n",
            "Epoch 00072: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 3.9575 - val_accuracy: 0.7190\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9981\n",
            "Epoch 00073: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 3.9187 - val_accuracy: 0.7146\n",
            "Epoch 74/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9987\n",
            "Epoch 00074: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 3.9800 - val_accuracy: 0.7208\n",
            "Epoch 75/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9985\n",
            "Epoch 00075: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 4.2529 - val_accuracy: 0.7180\n",
            "Epoch 76/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9983\n",
            "Epoch 00076: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 4.1044 - val_accuracy: 0.7101\n",
            "Epoch 77/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9984\n",
            "Epoch 00077: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 4.0514 - val_accuracy: 0.7209\n",
            "Epoch 78/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9982\n",
            "Epoch 00078: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 4.0798 - val_accuracy: 0.7181\n",
            "Epoch 79/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9989\n",
            "Epoch 00079: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 4.2448 - val_accuracy: 0.7145\n",
            "Epoch 80/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9982\n",
            "Epoch 00080: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 4.2237 - val_accuracy: 0.7159\n",
            "Epoch 81/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9989\n",
            "Epoch 00081: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 4.1058 - val_accuracy: 0.7178\n",
            "Epoch 82/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9985\n",
            "Epoch 00082: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 4.3028 - val_accuracy: 0.7172\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9987\n",
            "Epoch 00083: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 4.2079 - val_accuracy: 0.7147\n",
            "Epoch 84/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9985\n",
            "Epoch 00084: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 4.4566 - val_accuracy: 0.7198\n",
            "Epoch 85/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9989\n",
            "Epoch 00085: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0051 - accuracy: 0.9989 - val_loss: 4.2005 - val_accuracy: 0.7254\n",
            "Epoch 86/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9987\n",
            "Epoch 00086: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 4.3464 - val_accuracy: 0.7125\n",
            "Epoch 87/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9988\n",
            "Epoch 00087: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 4.2703 - val_accuracy: 0.7243\n",
            "Epoch 88/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9989\n",
            "Epoch 00088: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 4.4343 - val_accuracy: 0.7215\n",
            "Epoch 89/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9984\n",
            "Epoch 00089: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 4.4713 - val_accuracy: 0.7168\n",
            "Epoch 90/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9990\n",
            "Epoch 00090: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 4.5859 - val_accuracy: 0.7273\n",
            "Epoch 91/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9991\n",
            "Epoch 00091: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 4.4214 - val_accuracy: 0.7233\n",
            "Epoch 92/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9988\n",
            "Epoch 00092: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 4.7832 - val_accuracy: 0.7163\n",
            "Epoch 93/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9987\n",
            "Epoch 00093: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 4.3904 - val_accuracy: 0.7172\n",
            "Epoch 94/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9987\n",
            "Epoch 00094: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 4.6500 - val_accuracy: 0.7187\n",
            "Epoch 95/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9991\n",
            "Epoch 00095: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 4.5489 - val_accuracy: 0.7194\n",
            "Epoch 96/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9991\n",
            "Epoch 00096: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 4.4166 - val_accuracy: 0.7219\n",
            "Epoch 97/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9990\n",
            "Epoch 00097: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 4.5418 - val_accuracy: 0.7210\n",
            "Epoch 98/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9991\n",
            "Epoch 00098: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 4.6255 - val_accuracy: 0.7203\n",
            "Epoch 99/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9991\n",
            "Epoch 00099: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 4.9073 - val_accuracy: 0.7232\n",
            "Epoch 100/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9989\n",
            "Epoch 00100: val_accuracy did not improve from 0.72980\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 4.8214 - val_accuracy: 0.7227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hUVdrAf2f6TDLphVATOpJQA4iI4ooL7CI27CLRXQsqrLrr2laxrrr6WXZ1XTu6NlZARMVV7FIEAQHpJSSQhIRkUqaXO/d8f9zJJJMCAUFknd/zzJOZe88999wzk/c9bznnCCklceLEiRMnTkt0x7oBceLEiRPn50lcQcSJEydOnDaJK4g4ceLEidMmcQURJ06cOHHaJK4g4sSJEydOm8QVRJw4ceLEaZO4gohzXCKEGCeEKGv2eZMQYlxHyh7Gvf4lhLjrcK+PE+d4xXCsGxAnzpFASjnwSNQjhCgCfi+lPLlZ3dceibrjxDneiFsQceL8QhFCxAeIcQ5IXEHEOWYIIW4VQsxrcewpIcTfI++vEEJsEUK4hBDFQohrDlBXiRBifOS9VQgxRwhRJ4TYDIxoUfY2IcSuSL2bhRDnRI4PAP4FjBZCuIUQ9ZHjc4QQDzS7/iohxE4hRK0QYpEQonOzc1IIca0QYocQol4I8YwQQrTT5pFCiBWRcvuEEE8LIUzNzg8UQiyJ3KdKCHFH5LheCHFHs2dYI4ToJoTIjdzf0KyOL4UQv4+8LxJCLBNCPCGEcAD3CCF6CSE+F0I4hBA1Qog3hBApza7vJoRYIISojpR5WghhirSpoFm5LCGEVwiR2d53FOf4I64g4hxL3gZ+I4Swgyb4gAuANyPn9wOTgSTgCuAJIcSwDtQ7G+gVeU0Aprc4vwsYCyQD9wKvCyFypJRbgGuBFVLKRCllSovrEEL8Cngo0s4coDTyHM2ZjKaUBkXKTWinnWHgJiADGA2cDlwXuY8d+BT4L9AZ6A18FrnuZuBi4DdofXMl4D1QhzRjFFAMZAMPAiLyPJ2BAUA34J5IG/TAB5FnzAW6AG9LKYORZ76sWb0XA59JKas72I44xwNSyvgr/jpmL2ApcHnk/RnArgOUXQj8IfJ+HFDW7FwJMD7yvhiY2Ozc1c3LtlHvOuCsyPsiYGmL83OAByLvXwL+1uxcIhACciOfJXBys/P/AW7rYF/cCLwbeX8x8H075bY1trfF8dzI/Q3Njn2JFlNpfLY9B2nD2Y33RVNa1c3ra1ZuFLAHEJHPq4ELjvXvKf46sq+4BRHnWPMmmjAEuIQm6wEhxCQhxLcRd0Y92og5owN1dgb2Nvtc2vykEOJyIcS6iGunHsjvYL2NdUfrk1K6AQfa6LqRymbvvWhKpBVCiL5CiA+EEJVCCCfw12bt6IZm6bTFgc4djOb9ghAiWwjxthCiPNKG11u0oVRKqbSsREq5Eu3Zxgkh+qNZOIsOs01xfqbEFUScY807aEKmK3AOEQUhhDAD84HHgGypuXsWo7lEDsY+NOHWSPfGN0KIHsALwA1AeqTejc3qPdjyxhVAj2b1JQDpQHkH2tWSZ4GtQB8pZRJwR7N27AV6tnPdXjT3WUs8kb+2Zsc6tSjT8vn+GjlWEGnDZS3a0P0AwexXI+WnAfOklP52ysU5TokriDjHFKn5rL8EXgF2Sy0OAGACzGguDkUIMQn4dQer/Q9wuxAiNaJ4ZjY7l4AmEKtBC4SjWRCNVAFdmweLW/AWcIUQYkhEif0VWCmlLOlg25pjB5yAOzIKn9Hs3AdAjhDiRiGEWQhhF0KMipx7EbhfCNFHaAwSQqRH+rIcuCwSyL6SthVJyza4gQYhRBfglmbnVqEp24eFEAlCCIsQYkyz86+jKfXLgNcO4/nj/MyJK4g4PwfeBMbTzL0kpXQBs9CEfR2a+6mjLox70dxAu4FPgH83q3cz8H/ACjRlUAAsa3bt58AmoFIIUdOyYinlp8BdaNbNPjQBfFEH29WSP6E9lwvNqpnb7D4utJjMmWguqx3AaZHTj6P1yydoCuYlwBo5dxWakHcAA4HlB2nDvcAwoAH4EFjQrA3hyP17o8UbyoALm53fC6xFU7jfHMJzxzlOaAwwxYkTJ84hI4R4GaiQUv7lWLclzpEnPlEmTpw4h4UQIhc4Fxh6bFsS52gRdzHFiRPnkBFC3I8W3H9USrn7WLcnztEh7mKKEydOnDhtErcg4sSJEydOm/zPxCAyMjJkbm7usW5GnDhx4hxXrFmzpkZK2eYaWv8zCiI3N5fVq1cf62bEiRMnznGFEKK0vXNxF1OcOHHixGmTuIKIEydOnDhtElcQceLEiROnTf5nYhBtEQqFKCsrw++PryEWJ84vGYvFQteuXTEajce6KccV/9MKoqysDLvdTm5uLu1s6hUnTpz/caSUOBwOysrKyMvLO9bNOa74n3Yx+f1+0tPT48ohTpxfMEII0tPT456Ew+B/WkEAceUQJ06cuBw4TP7nFUScOL9EvCEv7qD7WDcjznFOXEHEifMz5XDXSQurYfa69rLXtZewGj7CrfplU+mpZFf94e72evwRVxA/ExIT29y2+Edz99138+mnnwLw5JNP4vV6j8g9j1Z7O8KcOXOoqKj4UXUcrX45Uiiqwra6bdT56w752mpfNYqqoEqVhkADCxcuZPPmza3KeUIe9rr2okoVoFW55n30S0dKyYIdCzhr4VkU/bfoZ6V4SxpKcPgcR6XuuIL4CZFSoqrqT3rP++67j/HjxwOtBeGRRlFa7W1/VDgSCuKn7JfDodZfS1gNU+OrOSRLIqAEqPXVkmJJwWKwUOuvbVdB1PnrcAac1Pi0jfNalmveRz8WKSX7vfspaShhr2svFe4KnAHnEan7aOPwOZj1xSxmL59NkjmJ+kA92+q2HetmRXl41cNcteSqo1L3L0ZB3Pv+Ji58bsURfd37/qaD3rekpIR+/fpx+eWXk5+fz/3338+IESMYNGgQs2fPblX+yy+/ZPLkydHPN9xwA3PmzGmz7u+++45zzz0XgPfeew+r1UowGMTv99Ozp7bffVFREfPmzePvf/87FRUVnHbaaZx22mnROu68804GDx7MiSeeSFVVVbvPsXv3bkaPHk1BQQF/+UvT5mFffvklY8eOZcqUKZxwwgn4/X6uuOIKCgoKGDp0KF988QWgCfWzzjqLcePG0adPH+69995oHY8//jj5+fnk5+fz5JNPRvstP79pq+jHHnuMe+65h3nz5rF69WouvfRShgwZgs/nO+L9IpHc9OebOtQv77//PqNGjWLo0KGMHz8+Wvaee+7hsccei5bLz8+npKQEgPvvv59+/fpx8sknc/HFF/PQ3x5CSsm4ceO46aabKCwsZPSQ0Wxet5kZ02bQu0/vmD5//fXXGTlyJEOGDOGaa64hHNZGs4mJifzxtj9y7rhzOXf8uYQbwny74lsWLVrELbfcwpAhQ9i1axcvvPACI0aMYPzo8dxYdCN7HXv56puvWpVr7COAzz77jKFDh1JQUMCVV15JIBAAtDXQZs+ezbBhwygoKGDr1q1t9pPD76DaW40iFQJKAGfQyV7XXur99e327c+B5RXLOW/ReSwvX84thbfw70na7rXfVX53xO5R5aniiv9ewfu73j/ka1WpsqFmA4MzBx+x9jTnF6MgjiU7duzguuuu44knnqC8vJxVq1axbt061qxZw9dff33Y9Q4dOpR169YB8M0335Cfn893333HypUrGTVqVEzZWbNm0blzZ7744ouo0PZ4PJx44omsX7+eU045hRdeeKHde/3hD39gxowZ/PDDD+Tk5MScW7t2LU899RTbt2/nmWeeQQjBDz/8wFtvvcX06dOj6YWrVq1i/vz5bNiwgXfeeYfVq1ezZs0aXnnlFVauXMm3337LCy+8wPfff99uO6ZOnUphYSFvvPEG69atw2q1tirzY/olGA7i9XjpPbg3i5cuZuzYsQfsl5PGnMS7n73Lwq8Wcv4F5/O3v/2t3bKgKa/58+ezfv165i+az4pVK3D4HNFRvMlkYsnSJZw//XxmXT6L2X+bzUcrPmLOnDk4HA62bNnC3LlzWbZsGevWrUOv1/PGG28A2vc5YOgAln63lFNPOZW5r82l8MRCTp90Oo8++ijr1q2jV69enHvuuSz7dhnzv5xP/gn5LHhjAXmD85gyZUpMuUb8fj9FRUXMnTuXH374AUVRePbZZ6PnMzIyWLt2LTNmzIhRio24g26qPFUkmZLoldyL3qm96ZvalwRjAuXuchoCDQfss2NBKBzi8TWPc82Sa0gxp/DW5Le4fODldEroRG5SLqsqVx2R+/gUHzM/n8nqqtXcsfQOXt306iFdX+IswRV0MShj0BFpT0uO6kQ5IcRE4ClAD7wopXy4xfnuwKtASqTMbVLKxZFztwO/A8LALCnlxz+mLbPPHPhjLv9R9OjRgxNPPJE//elPfPLJJwwdqu3Q6Ha72bFjB6eccsph1WswGOjVqxdbtmxh1apV3HzzzXz99deEw2HGjh170OtNJlPUWhk+fDhLlixpt+yyZcuYP38+ANOmTePWW2+Nnhs5cmR0AtLSpUuZOXMmAP3796dHjx5s374dgDPOOIP09HQAzj33XJYuXYoQgnPOOYeEhITo8W+++YYpU6YcandE+TH90hBowGgycs6Uc6gL1NHjhB58v7RtheVX/Hy75Vse/MuD1FTVoIQU+vTsEz0fVsO4g24STU0xjWXLlnHmlDNxhBzUq/WMmzAOk97Efu9+wjLMmWeeicPvYGDBQPIH5tMvtx/V3mpy83LZu3cvS5cuZc2aNYwYMQIAn89HVlYWUkqMJiNnTDqDNEta9PtMs6QRCodQ1Cb338aNG7ntjttw1DkI+oKMO30crqCLkBpq8zm3bdtGXl4emd0z2ePcwyWXXcLz/3qeG2+8ESBqrQ0fPpwFCxbEXBsIByhzlWE2mOmc2DmabqoTOrrZu7HHtYdydzkAdpMdnTgyY1ZVqnhDXmxGW0ydiqrw4g8vUpBRwJguY9q9/valt/NxycdM7TuVP4/4M1ZD00BkRKcRLN69GEVVMOjaF6FSSrbXbWe3czcTekxolWqrSpU7l97J1tqtPDHuCRbvXsxjqx+jPlDPrKGzOpSau6F6A8BRsyCOmoIQQuiBZ4AzgDLgOyHEIillc2foX4D/SCmfFUKcACwGciPvLwIGAp2BT4UQfaWUP5/I0CHQKPyklNx+++1cc8017ZY1GAwxcYqDTe455ZRT+OijjzAajYwfP56ioiLC4TCPPvroQdtlNBqjP0K9Xn/QGEJ7P9jG5zsYLa8/0D/AofZDS5r3yymnncLrv3u9Q/3SEGzAaDTS2d4Zi9FCSIZw+lv7yusD9VS4K7j31nv5401/5Ldn/pZ5H83j2UefxRvy4lW9uDwuSp2ldE7sHG2/lBJX0EV9oJ4MawYp5hRSzCmY9CaC4SCKTnPBpFhSMJvNpFpSqfHVoKKiKAqqqnLhpRfy8EMPYzaYo+3xBD0YDAYybZnohC76faaaUwFiUl6Liop47o3n6N6/OyveW8GXX32JSW/CF/LhDDhxBV2Y9WYUVcEb8uLwOfAqXircWtzHFXTF9IXZrLWjrd9QuascBHS3d0ev08ec0+v0dLd3p8RZQpmrDJ3QkWBMINmcTLI5+YDfUyOhcAiP4kGHLqoInEEnDYEGVKliM9qi95ZS8pdlf+HD4g8BuKT/JdxceDNmvTmmzr3OvXxS8glX5l/JTcNvanXPkZ1G8s72d9ji2EJBZkGr8/X+ep7b8Bxf7P0iqvzSJ6QzotOImHL/XPdPlpQu4U+Ff2J8j/Gc1u00Hlj5AC/+8CLzts+jZ3JPeqb05JL+l9AntU+r+4CmIOxGO7nJuR3qr0PlaLqYRgI7pZTFUsog8DZwVosyEkiKvE8GGiOPZwFvSykDkf1ud0bqO66ZMGECL7/8Mm639s9aXl7O/v37Y8r06NGDzZs3EwgEqK+v57PPPjtgnWPHjuXJJ59k9OjRZGZm4nA42LZtW4z/vhG73Y7L5WqjloMzZswY3n77bYCoS6O99jSe3759O3v27KFfv34ALFmyhNraWnw+HwsXLmTMmDGMHTuWhQsX4vV68Xg8vPvuu4wdO5bs7Gz279+Pw+HA6/Py/gdN/tmOPEdjv4wYNYJ6Qz1V1VUH7Re/4iegBBBoiivNkkayOZlQOIQ31BTEVlSFfe59WA1Wgp4gvXN7k2hK5NMFnyKlZHfDbtJz0tm5aScJxgQ+W/4Zu3dr2zYPLBzIJ4s/IVmfTIJMYPGHi9EJHV3tXZFSUuWtQq/Tk2DUlK5RZyTZlIyiKtT76+ld2JsF8xewbvc6pJTU1tZSWlqKw+9ACNFKsBr1RlKTU6mqq4pm3rhcLuzpdkyYePPNNxEIuiR2IcGeQIWjgj3OPeyo24E76Mbhc5DSNYV9e/eh7Fewm+y89cZbHbJ6A+EAPsVHpjUTk97UZhm9Tk9ech7d7N1IMafgU3yUu8sPGJhXpYrD52B3w262122n3FXOXtdeSp2llDpLaQg0YDfZybJl4Qv5KHGWoKgKDcEGPiz+kOsGX8dlAy7jza1vctEHF1HcUBxT/zvb30EndFw64NI271/YqRCgXTfT+8Xv8/qW1+mV0ovbR94OwPf7Y63Q4vpintvwHGf3PpvLT7g82hd3n3g3D578ION7aMkBi4sXc+2Sa6MuyJasr15PQWbBEbO8WnI0FUQXYG+zz2WRY825B7hMCFGGZj3MPIRrjzt+/etfc8kll0SDvVOnTm0l6Lp168YFF1xAfn4+F1xwQdQd1R6jRo2iqqoq+g87aNAgCgoK2hydX3311UycODEmSN1RnnrqKZ555hkKCgooLy9vt9x1112HqqoUFBRw4YUXMmfOnOgIc+TIkZx33nkMGjSI8847j8LCQoYNG0ZRUREjR45k1KhR/P73v2fo0KHoDXpuuf0WhhUO45RfnULnvM7RdMyioiKuvfZahgwZQmV9JQEl0EqgNPbLsBOHAdBzQE/y8/MP2C+n/+r0VucSjYkIIajyVkXvUe2tRpUqnRM6c+8993L++eczfPhwcrJysBgsZNoymTFtBj6nj9+e9Fvefultcnvl0hBooMuALkz87UR+NepXTJo0iYKCApKTk7EarBj12kJyaZa0mH/4NGsaEonD76D/wP7cMfsOpp8znfxB+ZxxxhnsKdsTHdW3JSimXTKNl59+maFDh7Jr1y5m3zub8399PlN/PZX+/fsDYDPauHb6tbz5rze5ZPwlBPYHSDAlkJ2QzeDOg3ltzmsUXVrExJMmIoVk+u+nt/sbaKSxTXaT/YDldEJHkjmJnMQcsmyauyyoBtssK6WkzFVGpaeSsAyTZcuiV0oveqX0Ijc5l+5J3emb2peu9q5k2jLpltSNQDjAjrodeENeriq4ihlDZnDryFt5dvyzOHwO/vjlHwmFNfeaX/GzYOcCftX9V2TZstpsQ4Y1g17JvdoNVO+q30WqOZVnTn+GSwZcQu+U3qzdvzamzPKK5QDMGDwj5jcphGBKrynMHj2bVye9ymuTXsMZdHLLV7fEuAlBS1PeWb+TQZlHJ/4AaB1+NF7AVLS4Q+PnacDTLcrcDPwx8n40sBlNaT0NXNas3EvA1DbucTWwGljdvXt32ZLNmze3Ohbn2PDKK6/I66+/vsPly1xlcmP1RrnVsVXude6VG6s3SofXEVOmwl0hN1ZvlBurN8otji1yr3OvDKvhmDK763fLrY6tcmP1Rlnhrmj3fqqqym212+Tu+t2tztV4a+TG6o3SFXBJf8gvN1ZvlOWu8g4/iy/kk5trNsuN1Rvljrodsr6hXkoppcfjkcOHD5dr1qyJtsEVcLV6BimlrPXVSlfAJVVVlaqqyl31u+RWx1aphBVZ6a6UG6s3yoASaLcNxfXFcptjm1RVVdb6auXG6o3SF/J1+BkaUcKK3FSz6YB92ciu+l1yZ93OQ6rfG/TKjdUbZYO/odU5VVVlmbOszd/CgXAH3XKLY4v8dt23UlXVmHNf7PlC5s/Jl8+vf15KKeXCHQtl/px8ubJi5QHrfGDFA3LE6yNkMBxsdW7a4mly+kfTo5/vXX6vPPGNE6USVqLHbvjsBjlp/qQOtX/RzkUyf06+fOy7x2KOr6xYKfPn5Muv937doXraA1gt25HjR9OCKAe6NfvcNXKsOb8D/gMgpVwBWICMDl6LlPJ5KWWhlLIwM7PNLVXjHIeoUsUZcJJsTqZval+6JHaJ5vTLyCheURXq/HXYTXY6J3Ym0ZhIQ6AhJrdelSpexUuSOYlUSyp1vjoCSiB6PqAEom4Xn+IjFA616ftOtaRi1Bmp8lZR6a1EJ3Ttji7bwmKw0NXeFavRSnd7d2ZcO4MhQ4YwbNgwzjvvPIYN06wcIQSJpsQ2rYBUSyqJJs2aEULQydYJRVWo9lVTF6gjyZTUrhsHIN2STkgN4Qw68Ya86HX6Vr73jqDX6Uk0JuIMOA/oBgqpIXwh30Gth5Y0xlX84diYk4y43+oD9WTaMkmzpnW4zgRjAv1S+5FkSmplQY7rNo4zepzBv9b/i1JnKXO3zSUvOa9VvKAlI3NG4lN8bKqJTXWXUrKrfhe9kpuywIZmDcUdcrOzfiegJS+sqVzDyE4d85qf2etMLup3EXM2zWFJaVMiyYYaLUBdkNE6DnKkOJpZTN8BfYQQeWjC/SLgkhZl9gCnA3OEEAPQFEQ1sAh4UwjxOFqQug9wZPLKjlPOOeecqB+7kUceeYQJEyYc0fs8+OCDvPPOOzHHzj//fO68884fVW9RURFFRUUdKusL+VClGvMPnW5Jp9xdjifkIdGUiMPnQErJzMtnUlqibakbCAfQCR2PP/o4EyZMwKf4kFKSYEjAarTSEGigylsVDfx6Q150QkeKOYWwDCOEIMmU1Ko9OqHjtX+8xrx3tDkBBp0Bg85wSP1iN9mjwvLNN9/s0DUHwma0kWxOjs6gPZjAtJvsmPQmHD4HiqqQYEw47AXsks3JuIIuvIo3GitpSaN7qa3+PBA6ocOkNxEIB2KOO4NOHD4HadY0Mq2HPhg80LPeNvI2VlSsYNbnsyhuKOa2kbcdtG8Ks5viEEOyhkSPO/wOnEEnPVN6Ro8NzdLcxN/v/55+af3YWrsVV8jFqJzYlOsD8ecRf2ZDzQYeWvkQYzqPwWa0sb56PblJuaRYUjpcz6Fy1BSElFIRQtwAfIyWwvqylHKTEOI+NJNmEfBH4AUhxE1oAeuiiMmzSQjxHzSXkwJcL4/TDKYjxbvvvvuT3OfOO+/80crgx+IMORFCxAifJHMSld5Kav21WA1Wav21JJmTeG/he9Ey+zz7qPPX0S9VC4p7Qh5AG0HqdXoyrBns9+7HFXRh1BnJTsjGr/ipC9QhpcRusrfKtGnk/rvvp2hWESoqvVN6H7Wg4KGQZcvCGXRi0puwGWwHLCuEIM2SRqWnEoB0Q/ph37cxLtMQaDiggjDqjYdlpZj1ZvxKrAXhDrrR6/R0snU64iuzZtmyuHHYjTyw8gGsBitTeh08xTrVkkrf1L6sqlzF1YOujh5vXKepZ3KTguiS2IUsaxbf7/+ei/pfxMrKlQAHtVKaY9QbuX3k7Uz7aBpzNs1hxuAZbKjewMldTu5wHYfDUZ0HIbU5DYtbHLu72fvNQJvJyFLKB4EHj2b74hxbpJS4Q24MOkM0z1xG0kAbhXojOqEj1ayN/Ku8VahSJcOaEVNfsimZWl8trpCLFHMK7pAbq8EarSfdmo6iKlgNVpLMSVEhH1JDOANOEo3tr8EkhIimEv4clAOASW+iu707Bp2hQ0IzxZxCtbeasAy3K9g7gl6nx26y4ww6yZE5re4dVsN4Qh7SLGmHJcwtBguuoAtVqtG+9ipebAbbUVu2+/x+57OsYhl9U/t22C3WmO4aDAej7r3GjKheKU0uJiEEQ7OHRjOZVu1bRa/kXq1+vwdjSNYQJuROYM6mOZyYcyK1/tqjNkGukZ/HLz3OLwpVqtT569hZv5M9zj3sce6JxgIC4QChcKjNf9I0i+ZGqfPXkWhKjJm8BGA1WDHoDDgDTsJqGH/IHyMIdUJHTmIOKZaUGCFv1BlJt6bHzCtoi0bX0s+JRFMiFoOlQ2X1On30OQ9nZN+cZFMyYTWMM9h6jog75I5aZIdDY9sa3UwhNUQwHMRmPLCV9GPQCR1//9XfuWHoDR2+pjC7kEA4wCZHUxxiV/0u7EZ7KzfY0Kyh7PPsY69zL2v3r2VkzuFl7d847EYUVeHPX/8Z4OhmMBFXEHF+YqSUlDhLqHBXoBM6shOyUVQlmucdTY00thYuRr2RJLPm025r9CWEIMmchDvk1oQU8keNlH/2SAm1xeCq7PAlmbZMeqf0/tEj8URTImaDmTJXWasFBZ1BJ3qd/qBur/aIKohIQoEvpK23dbj1ARD0gDyyC2UOy9aSC1ZXro4eK24opmdKz1b92xiHeG3za/gUH6M6dTz+0Jyu9q5cdsJlVHmrsOgt7U6gO1LEFUScQ0aVanS10UPFG/LiC/nITsimZ3LP6Gxih99BMBzEFXJhMViicwJakm3LpnNi53YFf5IpKZrxIoQ4qqPOY467CvwN4KnWlMVPiE7oyEvKI8mcRJWnijK3NjdhZ/1OnAFnU4KBqsAh/k5MehNCiKgF4VE8CCE6bCm1IhyCmu3gqoKaHR27RglAxTrY+iGsfB52tp6wmmpJpXdKb9ZUrYke21W/Kyb+0Ejf1L5YDVYW7FiAQEQn2x0OVxVcRao5lYLMgqNu0cYVxM+E42k/iGpfNV3Tu1LqKj2gkggogejktkYcfgd6nT7GP51ly0IgqHBXHDQ10qQ38d7b77W73LfNYMOgMxAKh7AZbO3GC36K/SBKSkqOSMZSmwQ9muWgN2lCuCO7x0kJrn2asFQ7vjT7unXrWLx4cavjep2eroldyU7Ixhlwsuy7ZXyz5BuyE7LJtmWzaOF8Hr7rT1C1EepKIeBqW5EpQWg2U70xk6kx1dUX8mE1WA8/9tNYt1TgxdOh+EsI+bS/XzwEZatjy0sJr50Nz58Kb18CH90Cr58LH92mtbUZw7OH8/3+76Np17X+2pj4QyMGnSsbZyYAACAASURBVIHBmYMJqkH6p/Xv8FIibWE32ZkzcQ73nXTfYdfRUeIK4idE/g/sBxEIB3D4tGUdfCEfZe6yqBJovg5PY4yhzFUWdT8Ew0FcQRep5tTYGIDeSIY1I5p1dLDUyAPtB9E8VfVA7qWfYj+Io6Yg1LAmcHUGyOgDQge+Diyb7a7UlErQo13fQaujPQUBIKQkw5BA34QcGorrWPPVGjKsGeiFYMqYAm674QqwpGiWjmMnNOxtXUl9KdTsjFFaFr0lOk/Fr/h/pHsp8t0mdgJ7Z/j3ufBwd3jtLPjqYZh3haYwGtmyCPYsh1Nvhau+gJu3wonXwcpn4dXJULoc1syBRTMprNiGV/GytXZrNEDdlgUBMCxLc0l1dP7DgeiZ0pOu9q4/up6D8fOKuB1NProNKn84snV2KoBJDx+wSElJCRMmTGDUqFGsWbOGCy64gA8++IBAIMA555wTsy8CaPsrPPbYY3zwwQeAth9EYWFhm3MIvvvuOx566CEWLFjAe++9x0UXXURDQwOqqnLCCSdQXFxMUVERkydPpqKiIrrvQUZGRnTJ7zvvvJMPPvgAq9XKe++9R3Z2drT+UDhErb+WNGsaZaVlXHLJJdQ56xg3cRwCQefEziz8eCHPPvIs2RnZbNu6jQ0bNnDl1VeyevVqDAYDt9x3C2dPPJv33n6Pt+e9jaPOQf3+ei677LLofhiPP/44L7/8MsFwkKnTpvLg7Q9SUlLC5MmT2bhxI6DtB+F2u8nPz4/uB2G1WlmxYkWrJb93/rCT+x68j/cXvn9E+6WkpIQrr7ySmpoaMjPSeeW5Z+jeqz9FV17J5MmTmTp1KqBZIG63m9tuu40tW7YwZMgQpk+fzk03RRZ+C/nBVQH2zpSUVzJt2jQ8Hk05Pv3005x00klNv4NF74Gnhhv+cCOFgwdSdOmFLF7yJTff9VcSklIYc/JYirdt5INXn+Ke2S+yu6SE4uJi9uzZwxNPPMG3337LRx99RJfsTN5/6WGMSdms2bSDm2/5PW5fiIzsHOa88go5ySbGTZjMqJPG8sWXX1FfX89LL73EqFGjuPvuu/H5fCxdupTbb7+dvLw8/jBrJn6PE6vZxCuP30Ne9y7cc9dd+IIhrdwfrsFXv5/V2/by9L9epKR4F1cWTdP6rlNXXpkzh+7du1N0+TSSjAqr12+m0lHP3x79P6ZOnYpZb6Yh0IAn5EEi23cVKgFwlkNyd9C3I85CXjBYtGT7330CX/wVdHrIOxVkGN66CJY/DafeAmEFPn8AMvppCqIxk27iQ9C1EN6bCa9M0o4ZExiu+qB7V9ZUrYm2sZcxGb59FoZdDqamQcqonFH8c/0/D7iKbLv4G2DxLZB7slbvT0TcgvgJOF73g9jv3U+Nr4bi+mKun3k9RVcVMf+r+eR11Zb2TrWkkmZJY+P6jcy8Zyafrv6Uh554CJ/iY8m3S5g3dx5/mfkXSh2lhMIh1ny3hpfeeKnd/SBWrFjBwjcWRp+pLTqyH8ToEaPZtXkXFoPliPbLzJkzmT59OhvWr+fSKeOZNfN6qNwAfqf2ajEif/jhhxk7dizr1q1rUg5SaqNofwM4dpCVmsSSJUtYu3Ytc+fOZdasWVowNegDxQ9VmzRlIgxgsOD3+bnmT3fx0YK3WbP2e6qrq0Fn1Ebf4SC7du3i888/Z9GiRVx22WWcNm4cPyxfgtUIH369jlBCDjP/fDfz/v0iaxa/ypUXnc2df7xBa5OqoLgdrFr5LU8++ST33nsvpkAt9930Oy48+zesW7uGCy+8kP49u/PNvH/x/ZL/cN/dd3DH4y9j6tSP+/58AxdOPp11n7/LhRNOBLNdE8zAzD/cyPTpV7Dh07lcOnUKs2bN0izLcJB9VTUs/fBtPpjzJLfddhtANN5Q59cmArbMWIvirND6MniAxRtDPmhUMJYkbVA34UHo+2voNwkGTIGlj0NDOWx4W3PBnX5Xk3JoJP88mLEUzp8DM9fC7XvJzBhIjzCs3reS4vpirAYrnT78M/z3NnjhdKjeHr18WPYw3jv7PUZ3Hh1br2OXVvZfY+GtizVFsOvzpt9TQzm8PAk2zIXP7mvl5jqa/HIsiIOM9I8mx+N+EKFwiIagtipmIBxg+fLlPPLSI5j0Jq658hruv+t+QJtVWziikIK+BbiCLpYuXcrvr/093ezd0A3Qkdcjj5JdJVT7qhk9bjS9u/TGarS2uR9EAgmcd+55h7cfhBLUgrWJWRgMxiPTL+EQwwf2YclXywBYsWKFtt+Br45p55zBn+//P0jIBCR4HVrQ2N6pqaJwSHtJCY1ZLb5aLV6QmAXeWkJVW7nh3qdZt2EDegHbd+6CfRvAWaaNjk0JWp3WZEjIYKsjTM/e/cgbpCm6iy++mOefe05zMyl+Jk2ahNFopKCggHA4zMQRvcG1j4L8fEpqPGzbvp2NGzdyxtQrQAkQDivkZGdBah4YLJw7YSzU72H4sGGUFO/SYhYGk6asqrdBYjYNJRuYfvcj7NhTiRA6QqEQtWELSmInMGwHfz0YrWBtmuEb7bv6Yqad9Sv+dNf9lDo8oAQ4e/JEdKndOaGPn6oqLSPLrNOSFNwhL+b20ouDHu1eoLmRrKmty4SDoIa09tDajbi10kn2yXeTuuMTTaiXr4Uuw6H/5NZ1AaT11F6N/Pp+Cj+6kk/2rcSnBumps6Hbs0JzSW2YC8+Pgyl/hwLNumzlfqorhVenaFZO10Lt8+6vYdXzkJ0PQy+DZX/XfjMn36wpsi2LovUdbX45CuIYcjzuB9G47lGnhE7ohR6BQJUqOQk5qP7YOEpSorYSZyfZiURjIlm2rGiMQQhBpi0TKWXMhLjGc0ekH6SEut3aP5m/HtJ6dqxfFD94azU3Q/1e0HtACIxGA6JmO4S86AP1KJ46zS0E2ujetQ8MVk0oJ3fBkJCGarCBax+qlASDQU1h1O/R2lRXAik9tGudFWBM0Hzh1jSeeOxmsu1G1n/0KqqqYuk5GhIyMKR2RTUmQnqvgz+/EGBOAsWP2WQCKdH56zEa9AjFBynd0dlSUcJhpJQMHDiQFStWaO6UoEcbVQsBOgPmlBzw1aF3+lFCAbCla757S7LW/oY93PXoPzntjN/y7k03U1JSwqnjxlFW58PpUzSrIaMv6M0gvm3dVls6eItBghpwg1Qx29M1RWiyI1UVVAVj/V50SFQENiUIvrrWCsBZocVhdIaYIHcMjbEFow1wxJzyh8JMfXYFo/LSeGnMH+CrR7QTZz/TpNAPRq/TGJ7ch/nqPtZUfscklxsGngsT/gonzYR3roD5v9MUcNfhrdv/2hTN+pn+AeRE5jQoAfhhHiz/h6a0krrAlf+FzAGwcT6sfuUnUxBxF9NPyPGyH0RYDUeXsjDpTeh1esaePJb1n6wn0ZTY7n4QQghOPeXUaGC2cT+IwoJCUiwprPhqBXV1dYe8H0QgEIjGZNp8DlelJiASO2nKomY7Y0cNbbtf+vbUTHZfHTj3QX0pdpsFV22VNkrzNzSZ9vacJougbjcnjR7N26+9COEgbyz+JmqN5OblsWZrKVhSWDTvLUKhENTvwZ6ShsuvaErLsSPqxiGlmyaAjFYaQgZyOndGl5TDvz9Zq+0vndyVHn3z2bxlS6vfQb9+/SguLo7ubz137lytfdYUTYCHPFowuL4UEJDZj7AljXDkkfr160d1dbWmIPQGQgYbmzY328PLlq69FB8IPSR3w56UhMsbwJfSh0qZhsOn0qV7D0BLGFAj/aUzW3G6XJqwbxEPOOmkk7T9RKypvL7gv4weOZwUIplXje4feyT+tX8rIujGrNNmJ9t0Rk3JemsBTbDX1Toillg2mBK177+toHtjgNrY2kX1xdb9uAMKn23dT0n/qyA1F3qfAT3HxZR7Z/VeLnhuRfR1/Ztr+e/GfQQULYOv8JS7AFCkSk9hgcmPa99vUme49B1tMPH9v2NvHnBploPHAdPebVIOAAYzDL0UrlsBRYvh6q8geyDodFB4BZQuhf1t7/19pIkriJ+Q42U/iLpAnbaUhaVpMtpTTz3Fs88+e9j7QSQYExg1clSH94MwGgzcfdddjBw5kjPOOCO6bwHE7gfhq6/WsnOsaZCUA5l9wWBmVK90rV/GnKj1S/5ACvr3Qji2a64onUFzD2UO4OoZM5k47Q+cdtENWuKB0EFmP005GK2aAFL8/OOBW3llzqsMOuMi/v32fJ566ikArrrqKr76+msGn3Y2K9ZtI8FmhaQuDBr7W/QmK4MnTuOJZ17QFEVCVoywuu6GWbw670MGj53E1h3FUWuzW7dunHXueQxs8TuwWq3885//ZOLEiQwfPhy73U5ycrJmQSA0BRfyQXJECRks7K314nAHUFQVk8nEvHnzuPXWWxk8eDBDhgxh+fLlTV+gENq1ab01H7wQnHbaaWzevJlhhSN47b3PuGLGjdx+++0MHToURVGQKuiFYPiJY9m4cRNDhgxpUlwR/vGPf/DKK68waMhQXlvwX56+7yaScaPojJrgA836QGguoeRuWCJbtdpScrXvoL4Udf8W6qsrMPsqUXVGSMgAk01TjkrsAn9AU4C6jTW2Ptiwj1SbEZNex8urquDaZXDxW63KPf91McXVHnQCdAJWFju49vW1FD7wKTe8uZZXNmWQKTVl1mvkdbGWjiUJTpgCGxc0WaGgWQGOHXDR65pLK4KUko9+2Md+l1/7LnLHQGKzWdlDp2kxp9Uvt37Wo4A40JK9xxOFhYVy9erYfOYtW7YwYMCAY9Si4xNVquys24lRbyQvOe/wKpFSE4YN5do/fXJX5rz2b1avXs3TTz99kAYoEX9+NSA114ylnbRXVdFGUo0CvVEIqGHNqvDUAKomPAMu7R8uMVsTKoc6wchVqbmWANL7gLmduRJSasKqpUAKejSrxZ7TprBqSSAUZluVC4NO0DXNRpKlaeKg2+0mMTERKSXXX389ffr00YLg7iot5pHYKTqCDyhhtlVqg5AEk4GemYe3imuV00+V00+CyYA3qNC/UxJGgw5/KMz2Khc5yRaqnAFSbEa6ph44JbVifw2dFS3dtUzXha6dmpZOr3N5CPq9ZGdm4g15cQadZNuyEVKiuGtQ3NVY0IK09aYcUjI6aQqxeitKcndKPWa6ploxGyN9XLlR+65Sc2PkgSegMPyBJZw/vBu+UJgPN+zj29tPJ9kWO0HT4Q4w/IFPuWVCP64/rTcASlhl2S4H731fzqqSWva7Augz38KQ8j2PjHyT3wxosfx28ZdaSu3Ul7VAtxKEpwZr7sOiD2KKvrJsN/e+v5lrTu3J7ZPakV3zfgc7lsAft8RkSR0uQog1Uso2Z+7FLYg4UaSU1PhqCKmhQ15ILEo4pMUD6koi+fm1WlZIOKgJz4Bb8702lGsCPODSMoDcVdo1VZu08wazJsRrI4HSlgMZKbWAnqpAao9YoavTQ3IXyD5BUwZBjzaqyxqgWQWHM/s0MTvifsloXzlAxJffhgIwJUBy1w4pBwBPUIsH6XSCkhoPlQ3+6HySF154gSFDhjBw4EAaGhqaYlqJ2do9mrl3aj1BBIJOSRY8QYUa96FnwPhDYfa7AqRYTXRNsyKBWq9WT4NP24ktxWoiyWLE6QsdZLtQSW3ISEiYUISROsWEElaj5yrdYaoCRpSwtp90pwRt9daQhJ1eC7voSiClD/v1WVSHI8LRYAF0hHxuPEEFhyfyjNEAdWuF9dnW/fhDKpMH5XDFmFx8oTBzV+9pVe67kjoATuzZtJy6Qa/j1L6ZPH7hEJbe+iu23T+Rf0yZgd47mH99Voeqtnj+3FMgqSusi8yJ2TgfXBXUDL4m+uygubzu/0Bz922uaL3GVZQRv4NAA541c9svc4SIB6mPE472fhCKqlDhruCxhx/jsw8+w6hrGkl1eN+DkF8zm9WwFoRNzNL8xHUlFP1mFEW/Ha2dp3EE2+IfSWfUgqEJWZrbQA1DQ5k2eg94NEXQuASHuxICTs655k52742dNBftF71Rc5ckd+NHIwSkdD/syz/++GNuvfXWmGN5eXm8++67hFUVvS52rOYOhDHodPTNslNR72O/y48QkJ1k4aabbmpKmz0Aqiqp8wRJshrItJvxBsNUOv3YLQYsxo4pKiklZXU+9ELQOcWCQa8j0WygzhMky26mwRciwWTAaNCRbDNS7wviDijYLcY26/MFw6hS4rf3wKATyNoQnoBCss2Ey68QighMbzBMkrWpTxyeICFFpVdWImaTAakY8Dn9KGEVg16nue1CPiCZBl+InGQLIiZAHcsH6yvITjIzIjcNnU5wYs80Xl1eypVj8rT6Iqzc7cBs0FHQpf09F4QQjM8bzd2junHzf9az4Ptypg5vNolNp4PBFzWl0i7/B4G0foz8j6DLki/53Zg8BnVLYeZb3zMgJ4nc9ARW7q5t/0vpPprd+jySlzxAwsCJWqzjKBFXEMcJR3o/CFWq+BU/ITVESNUmxClhhdl3zeapB586dDeEEtSCo6C5exr97Ga79rmhHBCau8iSpAVAw0HNbyyEFshrNvL1h8KEwir2lO7a6LuhDKq3aoFEqWpKw5rGu4s+7HjGyVGi3hvEGwzTOaWdXH20BIW2lHm1K0Blg4/cjISoUJVS4gkoJJj16CIuJkWVONxBMhPN6HQde94GXwhFlaQnaGsbdUm1sqPKzR6Hl+7ptg4piXpvCG9QoVuaLSo40xJM7Kn14vAE8YfCdE7WnttuNqATggZfqF0F4Q5olpHVakOnE+iEE3dEQTjcAYx6HUpYRhREUx2egILFqMdm0n4jiWYDVZHjyTYTqtGKKejAYtDhV1Q8AYXESIA6bLCwe78bl1+zbtwBhS+3V3PpqO7RvrxyTB5X/3sNn2yu4jcFOdH7rtpdy7DuqZgMB3e2nD2kC6+tKOWR/25lYn4nEs3NxOuQS+Cbx+C962D/JpYPuBe1QpCeYOae9zWrITvJzEvTR/DhD/v4MBKHyLK3Xn8qpEpm+mfwlmE2/tfOx3L1J0fE1dQWcRfTLwxVqjh8DnbU7WB3w27KXGVUeaoAyE3OJd2afujKIaxoriCpan7VlhkjehOk5UFaLtjSNBePEJobyZKkKZFmykGVklKHhxKHl5AqNTdRZj9NqTh2aq4oo7UpEPsT4QkoeAKxaxiFVUlFvZ8adwB/6NAWpXO4A+xr8CHRBHEjwbBKKKzGCJiMRBOKqlLvC7VRUzv1e4KYDXoSIvUY9Tq6pVkJqSo79rupcvpRVYnbr1BR76PU4SHcLLVYSkm1K4DFqCelmbBOshox6AT7GvzRz6C5w5IsBpw+pV03U6OgN+h16IQg0WzAHQgTUMK4AwppCSbMRh3eYFM/q1LiC4ajzwFgNenRCRFVOD7M6IWkq12HXgitPyMB6mq3puQafAq3zt/ARz9UElRUJg9qGnmfPiCb7mk2Xl7aZKU7/SE273MyMq9j25vqdILZZ55AtSvAPz7bQUW9j62VTrZXubT/i24navEIe2f+7R5B76xEFl4/hvkzTuLSUd15pWgknZItnJCjxdy27Gs743B3jYeN4a7MDN2AqWYzLLgajtISPkdVQQghJgohtgkhdgohbmvj/BNCiHWR13YhRH2zc+Fm5xYdzXb+UnAGnOyo20GlpxKz3kw3ezd6pfSif1p/+qT0ab2cQTikxQfCLRZ2k1Ib+fsbwL0fandqn9Py2jTnDxWHO0hAUbWYiCuSmWK0akrCGlEwqXlN2S8/ATXuAMXVbnbXeAgq4ZjjiqoiENR6Yv37Usqoy6QltZ4g5fU+kixGUm0mnL5Q1HftCWj1NxeICWYDFoMehzsQFb5SSqqcfuq9reMKvqCCN6iQnmiKUfh2i5G+2XaSLQaqnH42VTgprnHj8ARp8IWocjZlAjn9Cn4lTKbdHFOHTghSbCaklNhMhpjRdbLViKKqrRQpaILeGwzHKL4Es4GAEqbKGUAgSLOZsJn0+ELh6HM2uqUSTM03kBIkRJQLQIOiKSmrCJBkNdLgDyFDPlSDlRp3kBSrkSSLgf+sLuOOd3+gS4qVYd2b3EZ6neDy0T1YXVrHxvIGANaU1CEljOrZ8f2vh3ZP5dyhXXju62JOevhzJj75Db9+4mvW7qnTrAhAGXk1K0pcnNxbi/MN75HKg+cUcEJnTTE0KYi24xBbI0kHFZljeVRMh60fwGf3dLiNh8JRczEJIfTAM8AZQBnwnRBiUWQXOQCklDc1Kz8TaJ7T6ZNSDiHOESGshqnwVGDQGehq79r+QnaqqmUg+Wq1AHIjRpsWFwgFIjnnzUbLQq+5fsyHt0FMc5Swyn6Xn0SzAYNeh8MTJNNu1twbOr0Wh2g+M/lH4A0qmA169C1cNr5gmLCqYjbqoyPlGncAu8WIN6BQVucjLyOBsKopsCSLESE0K6BTsgVdpG1VzgA17gD9OtkxNvNre4MK5XVeEs0GuqfZ8AQV6rxBXAGFZKsRT0DBoNNhbiZ4hRCkJ5oor/fhjYyma9xBqpx+9JGReHPfeY07GBHkrV09Rr2O7ukJpPhCuAIKiSY9iRYjlQ0+HO4AqTYjFqOealcAk0EXYz00kpaguYRa1m+3GNELQUWDn1yDPkZ5eCOCvrmCaHxf7w2SbDViNOiwmQzUerRBgsWojwbsbeZYcZVoNrDP7yOghKkL6OgkdOhCPlJsSQS8LgQhnGEjUmqxG4/VyP+dP5jbFmzgnKFdWlnK5xd24/8+2c6ry0t49PzBrNxdi1EvGNqtjRnaB+DuM09gYJdkbCY9iWYDt8xbz4K1ZQybfBGEg6xJ+Q2+0DrG9G47ESTZZqRLirXdQPXWfU4MOsFtk/pz5ZzxXDDAT15DuRaz62ASREc5mjGIkcBOKWUxgBDibeAstH2m2+JiYPZRbM8vBillqx+/w+8grIbpYe+BtY1JQ1pqaoO28Fk4qLmFGichBT0QcGqTegwWbVKW0aa9b8w2OkKunv2uAKoqo/78em+QGneQTsnNfLFH4F61niBldV7sFiO56U1bWfpDYXZVu5smfwmBKiUZiWZyki3UeYOU1fmo9QQJhlXCUpKdbEEJqzT4PLh8IZJtJoJKmOrIaN/lD5GW0LSDW703BELQI13zwyeaDRh0ggZviCSLIRp/aPkdpthMVDr9ONxBLeOnwUeC2YA3oLDfFYj2mTugKZxMuxnDAaysJKsxxs+fnWyhwadQXu+jU5IFb1ChS4q1TZejxainb7a9lW9epxN0T7exx+FlV7Wb3PQErJGRf6NVYTPrm9Wjw6DToagqaQmRiXGR8r5gGItRjzcQxmzQxyhZaFIuVQ0BwlKiGi3ogl4SbWGsukpCGKkIWElLMEbTXs8b3pVx/TJJbkPpJVuNnDOsC/PWlHH7bwawcreDQV1Tou3vKCk2E787uSlF/JPNVXy4YR+zzxyIceRVLP1kGzpxYMtkQI69XQtiW6WLXpmJjOubRddUG38JTOONy086Ku7Wo2mjdwGar+1bFjnWCiFEDyAP+LzZYYsQYrUQ4lshxNntXHd1pMzq6urqI9XuY8KR2oNASkmps5RSZymqVLn77rv5+JOPcfgc/OfF/yBDTb7h6D2VgLZgWN1uLTU1rRdknaBlR1iSIhPQ+kHnIZDVX8vmSYike+qNlJSWHvKy1uPGjaPlvBV/KIzDHSQtwYTFqMdi1JNsNeLwBGJ84z+WDz/+lA+XfIFJr8PlD0XTIlVVsqfWi04IctMT6JxsJdVmpFuqjc4RQZlqM2G3GHnznQU88vAjpNhMfPzh++zZtR2jXketN8S4ceP4+MvlCLSUSKevyd0ipcTpC2E3G6KZS0IIkqxGnP4QAUUlGFZj3EuN6HXa/Rt8IfbUejEb9eSmJ5CaYMLhCRJQwqiqpLzOh8mgI7uNAOeBMOh0dE6x4A2GKa31YtDpSLWZ2l3u22yMVWKN5ewWIz0zE/n848XcMft+9tR62bnfTbUrgNWoj1FaQmhxC4tBHxX4ZoMWn/AGNTeTJ6jEuJcaaVQu9T7NWtKbEyDkQ9TuQiAoVrMJCz1ZSbH9kJ5ojrG2mlN0Ui5BRWXOst38UNbAqA7GHw7E2UM6U+cN8fV2TUYt21nD4G4pMXNbWnJCThK7qt1txrW2Vrron2NHpxNcWNiNZcX1lNYe+eXq4ecTpL4ImCdlc78FPSKTNy4BnhRCtNqFQ0r5vJSyUEpZmJmZ2fL0z46fYj8In+LDE/LgDrrZ69rLPffew+Axg1Glypx/zWm974GUWtA35NFytTP7N63N00E6su+BPxSO8d23xBtUKHV4kWqY7Gb/0Fl2M+FIBs/hEFRUtle52OPwUu8N4vSH+PDjT9m0bjV9su3YLUb2NfjxhbQUUH8oTNdUK0lWIxl2M11SbaRGRragCbQuKVZ+NeE3/O76m8i2m1m4cCFbtmwh1WbC5dcyh9wBhUy7mRSrEXdAicYX/CFNASRZYxVAitUYsQq0wG9iGwoCID3BhIykB/dIs6HXCbKTLAi0kfR+V4CAEqZLirXD2U7NSbYaSTQbCKuSDLsJnU4ccD+I5jQvZzXpueqyC7jhpj/hDSjaBG2rkZzk1tZr51QrvbISo8pGCIHNpMcbVPArKmFVtnIvNZZLiFgjdosBYbQB2lpOoZQ8AhjJSDS1sjwORN9sOyf1SufZr3ahqLLDAeoDcUrfTFJtRt79vhynP8T6soZo/KE9BuQkoUq0AHczGnwhyut99OukuXPPL+yGTsDc79rYZ+MIcDRdTOX8P3vnHR5HdfXh985sU+/NVbJxkysuQDAudAimdwgxEOLQW+gkgZCQj5IAgQChQ8CU0E0gkAA2toMNtsG925ItyZLVu7S7M3O+P2a1lqyVvOBuz/s8+6x25t7Zu6Pde+6959zzg/YB6L1CxyJxAXBN+wMiUhJ63qiUmoXtn9jwYxvz4LcPsrp61+YvGZw6mNsPu73bMntaD2JF8QqCPP2UagAAIABJREFU/iBnHHUGHy/4mPN+dh7jjxtPQ2UDpVtKO+se3HYj//r438TEJ/LhRx+TFa866h5kZPDSSy/ZuftDGgpR6x6EMC1hxaYK7r75atatXsGgQYNpam7Gb5gEDIuU5ETOuehSvvnfVzz++BN8/s53vPiinUrgiiuu4PSf/ZLFq9Zy3dTzGDtmDN999x1Dhw7l+RdfxtDcfD9/DrfdeiuGYTBu3DiefvppvF4vubm5fPT5bIhJYvF3i3j4vt9w3yNP8fZrL+Fxu/j0g7d59LG/kjXwUAormwiaFunxXuI8Gnl5eWzcuJG6ujrS0tKYOXMmEydOZOLEibzwwgvMnTWbhYsWctnPL2HGjBl89dVXJCb+kT89+RL+oMkXn3zIo7+/neqaGn7zwF/p/dPjSQrNEoBwGGhhYWFYD6I1aHLHHx5i7GFHMG/ubP7yl79E/B6s/vYrfnvn7cTHxzF+/Hg2btzI89Pf5ve//z0lRZsoK97M1i3FHfUgevbko48+wu12s2jRIm6++WYaGxtJT0/n5ZdfJicnh8mTJ3P44Yfz5ZdfUl1j60Ek/+SIyHoQN9xAa2srMTExvPTSS+Tl5XUq19LSEt49X1hYyNQuvlOJiYksXLiQsrIyHnroIc455xxiPDqVDQEaW+3ZV5w38jJPvNdFXUvQHo17PXYwQ2JPfN4E+uvGD14eAph6ZC5fb6hCU7YDeWdx6xpTRvTgnwuLOHZIJqYlXfof2hgSclSv3FLPiF7bnOltBmNItn0+O8nHMYMzmb2ugltPHLTTWuPbsztnEAuAAUqpPKWUB9sIdIpGUkoNBlKAee2OpSilvKG/04HxdO272OfZU3oQQ4cN5etvvmbjso385IifkBWXhRGKQLrj5jsi6x4M7ceSuZ8xcfIxnXUPli7l4osvtjUKuiGi7kE76loCvPGPF0hOjOfTOQu5/Ppb+f6779hc1czqsnqam5o44ojDWbFsCWnJCWF9iPnz5/Pcc8+xdeMqRGDtmjVcddVVrFq1iviEBP70l79SUFbD1KmX8tZbb7Fs2TIMw+Dpp58Ov3dNi0FqrJu89Dh8bp0xQwdy5ZVXctNNN7F48WKOnjzJDv00bYdodqIPXdcZNGgQK1euZO7cuYwePZo5c+bg9/spKipiwIABeN06XpfOkUceyWmnncbDDz/MkiWLGTLIFpH36vDtt9/y2GOP8cxjD9EQCk8NbywLjWozMzPDehDPvPQqD/7uDuK8ri5/6K2trfz6hmv59NN/s2jRItqWVjMSvGgKijcV8tXMdnoQRx/NsmXLiImJ4eOPPyYYDHLdddfxzjvvsGjRIi6//PIOmyANw2DBggU88fhf+eMf7sPj8XDfffdx/vnns3jxYlsPYvBg5syZw/fff899993HXXfdFbFce7r7TpWWljJ37lz+9a9/hfUgYj0uBKGqyd4b4eliFpAc6yEr0Wf7FHS3PQMOBUvEhfZl/FCOG5JFz+QYhvdM6nI/xw/ljEN74DcsHvz3GmLcOof26XrjHUCf1FjiPHonP8Tq0Ou2GQTA/501gvevHr/LjQPsxhmEiBhKqWuBz7C1nF4UkRVKqfuAhSLSZiwuAN6UjoHTQ4BnlFIWthF7oH30049hRyP93cmu0oMQEbY0bSHFm0KsO7aTHsQV11zBwq8XkuxJZsKECaR7U4nRdFJFw1NdaKelqNkEsRp4E/B43Ew5YTIk9eqgBxHO3Q9ccskl3HbbbR3aYVlCZVOExGhdUN0UZPG387jz1psYkJVAn8k/Yejw4WQn+eiZHIOu60ybehG6pjF37tywPgTAWWedxbfzv2by8SeR3aMn/YePwbKE46acw4vPPcXRxxxLTq8+pPW0s4tOnTqVJ598khtvvBEztKyTkeCjVCl0TZGZ6OsUtZTgc9MvPQ6vWw8vy0yYMIHZs2dTUFDAnXfeyXPPPcekSZMYN25ct581K9GHW9e48Dx7ljVu7FhKS4qobzXwGyatQbPDMkswGOTaa69l8eLFKE1j47p1Ef0PbaxevZp+/fqRl2c7QS+88EKeffZZdE0jOdbD6VN+SozPu00P4qSTABg+fDiFhYWsWbPG1oM4/ngATNMkJ2fbxrCzzjoLsPVB2jLGbk9dXR1Tp05l3bp1dhqM4I73ZnT3nTrjjDPQNI38/Hy2brX35LQ5qgOGRXKMu8vOr22JbVeia4pXLj+s0/dkZxjdJ4XeqTEUVbcwaWAGXlf3MxtNUwzOSey0F2J1WQOJPhc57YI2MhK821ffZezWndQi8gnwyXbHfrfd63sj1PsaGL798f2VXaUH4Tf91LbW0hJsoX9yf5RSHXQPRh45kn++8U88ysPDf/ojVK7FawSJQ23zKQSaQ6mgwe1yoZJ7g+7upAfRXduqmgJsqWm2dQ/aISJUNgZIinHhCf0AWoMmzQEDt0sL/8i9bh1dKRJ8btLivfh89qi9OxJjPOiaRkWDnya/SYthEuuxfyi6pthS09rhRxc0LJSmk+jV8Li0HepqxG83Upw4cSJPP/00W7Zs4b777uPhhx9m1qxZOxQcivPa+wJ8PvsHrOs6lmliWFbYv9De//Doo4+SlZXFkiVLME2TmJgYUmM9P1gXBOyljLhY2/homtZB70PTNDvzans9iAh4vd5wu7v6Pvz2t7/l6KOP5v3336ewsJDJkyfvsG3d0faeQHjvg1vXcOsaQdOK6H/Y3RySuWuCRtpQSnHGqJ488eX6Hfof2sjPSeSD70s6RCWuLmtgcHbibpktRGJfcVIfFOysHkSzYTuY/aafWr+9p7BND2L0uNEkpibSUNvAmtWrGJbtshOVeRPsaKT0ASQkpdAQ09MWdInPso1GBBWucO5+YPr06dt0D3JzWbhwIVWNfmb959/hkWObPkNr0KS0roXCquawU7a6KYBSimMmb9OJWL58OUuXLo14j7rShwAoKS5i1ZKFNAcMZn3yPkdPmsjgwYMpKymitKiATVVNPPfiy4w/aiIVjX569O7D5rW2pvW7774bfo9odDEOO+wwvv76azTN7uxHjRrFM888E3G2t6PracruIOpagvhCS1Nt1NXVkZOTg6ZpvPbaa5imiaapLr8HXepBREkHPQjsGcyKFSu6rbP956urq6NnTzsg8eWXX47qPnT1neqOtllEnOfAyAh0/rjejO6TzEnDsndcGNsP0RDadwO28VwTimDaUzgGYg+ys3oQLUYLuqYT446hvLkc0zLDehBDDxuKW3czKn8Iwwfmojxx9nqsvm1kPG3aNE46+WSOPnFKKMGXihitFM7dP2IEr776agfdg5mzvuL0Y49k2fcLiI21Z0YjRoxA13XGjhnNq889ZRuK+lYsEWqbAyT6XFx7zdU0NjYyZMgQfve73zFmzJhO7wt0qQ8Bduf24esvce5xR9DaWM9VV12Fz+fjpZde4rarLuOs446k1RAmnX4hVY0Bbr3jbm65+WbGjh3bYYZy6qmn8v777zNq1CjmzJkTsR1er5fevXtzxBG2nsSECRNoaGhg+PDOE9sLLriAhx9+mEMPPZQNGyLHUbSFaW4f2nj11VfzyiuvMHLkSFavXt1BDyLS96BLPYgo2aEeRATa9CDadB5uu+22DnoQXZVrT1ffqe5IDkVU+dwHRjfVKyWW964eT+/U6LINtO2sXhHaMFdc00Kj3+jgf9jdOHoQ+xFtOg0ZMRkU1BWQGZtJsjeZzQ2baTVa6eVOJKmpErxJdt4jtWt/WCLC+opGLAtS4+zw0CE5iWGHq52GwiLB56Ky0U9KrIea5gB57RLR/VgKCwuZMmUKy5cv77ac3zCpaQ7S7DfolRITXura21Q1+impbWFAZjwxOzki7lIPwqFb9rf+oCVgMvSeTzl7dC8eOmcEX6wq54p/LOTdq47cJdFVbXSnB3FgzN0OAkzLxG/6SVFxxOg+EjwJVLZUUt1ajSUWfTxJJDRW2OmyU3J/tHFoaA1S0eAnNz2uUwRIU8CkJWDH2LffHZsc68EKZSBNifWQneSjKbSb1xNKD72n8Lp0shP3DaPQntQ4DzEefaeNA9h6EK+88gqBQIBDDz20W5+Ww/5LjEfnkiP68sq8TcR5XeGd5ntyBuEYiH0Ey+8nWFyMu1cvNG/nqIQzTj+dwvXrUQLK5QJd55q7r2HScZPIi++Br2qjPXPYCeMAdv6g5oBBs9/o5LitbPDjCu3mVcpOQ9HkN0mO3ZZQ7Zs5X3Df735jawMZFi5dMaB/v51OV56bm7vD2UN3iGFgNTVhNTWhp6WF7/H999/P22+/3aFs1PoXP4D//Oc/XepB/FCi1YNw2LNESnGzs9x72lDcusbzcwvwue1svHtywOUsMe0jBIqLMWtr0ROT8PTZtr9QRDC2bsWorMRSdggeSsM7aDCtZituzY2rdrOdWC8zv4PP4YfSHDBYX2470DMSvB3CMdskMDMTfWSHwgo3VjTibmkio6WWpuR0SoI6+TmJ4TQGlkh4FmJUVqJiYtDjdk/e+vaICGZdHeL3I8Eg4g9gtbTQJlCk+Xx4+vVD7cFssD8Uq7WVYHExrowM9Ch9DFZLC2haxAHG7sYKBMCywoOXPRVl80PYUX8glgUiqB1E1EWi+tXXqPjb38i86SaSzzt3l363RIQnvlzPI/9dy/H5WTz384irQT8aZ4lpH8cKBDBr61AuF2Z9HVZLOlqM3TmbVVUYlZW0xrmpiYe+TS0EGl1YddXEJKfZEp6tdbbWccg4tARNNAUeXcPYWg4KXJmZO/zRVoUygPrcGg2tBjnt+qWa5iAKO9VDGymBRnz1lQjgqyonLrVHhxw3bcbBbGgkWFaGFhOD3r9TxpRdjllbS7DEFihSbhfK7caVkY6WkADBIIGiIozKStyZmTu81t7AamkhUFiImCbB0lK0+PhuOy0xTYJlZZg1NSi3G++AATvsoMSy7My9Sv2oDrE9ZlMTgcLCbbKwbdfUdJSuoaek4Erd+ZQVuxP/xo0UTfsVsWNG0+PBBzuca160CP/6DSRNOQUtwgAnuLWc8kcfRWkaZffeS/3HH5Pzh/vw5ObukrYppbj+2AEM75VEnygd3LsKx0DsA5hVVaDAk5tLoKAAo7wcT9++9rLT1q3oCQlsjW8mURSaV0c1gVlRhp6YZOs3ay6Is3NRiQiFlU24NUUfox6zpib8Pu6srC7bYGcjDZIS68bj0iitayVoWLhdGiJCbUuAOK+9A1hME2PrVnw11TS6Y/Ckp+MpLSK9pQ7oONoVyyJYVgrYHZ/V2ormi25jk5gmZn29PRtobQ1FXCmU14OnVy97tBrpflZXo7xevIccEtEo6vX1GBUV6AkJYUO8Mxg1NVgNDfbSnqah3C70+HhUTORMqOHPJ4IYBhIIoHQd5XIhgQCBTZtA03Dn5BAsLsaoqMCdHTk00qyvJ7hlC2IY6IlJmPV1mDU1uNLSIr6fUVaGUV0d7syVpuMZcAiau/PM0woEMMorMOvr0BMScGVkdPrfiWEQLC5Gud24s7KQoIEYQcQ0wbSQgN9uXzAY1SBlb9CyZAlFv7oSs66OuuJi0n75S7yHHAKA1dxM8Q03YlZWUv7II6RccAGpP7sYV7vcb+V/+TMYBnkfzaB5wQK2PvgQG888iz4vvEDs6EO7etuISDBI0/xvUG43nrzcDvfs6EGZWH4/NW/9k+oXXyRYVoaemoorJQXfiOHk3HvvrrolYRwDsZcRw8CoqUFPSkLz+dDT0zG2bsVsasIoK0NpGpKdjtlQSIxhoOLS0QwDs64BqViHMlshqTeiNBChNWhhmBapDdWYgWZcGRn2e1RUoFyucMchoU1YbSPNmmY7hXRavBexLOKDLfiL6jCDfsTrI8bQSE6Ms5fC6uvBstDT0thqxuAxNOK9CaQ01WM2NXVYRjJrahC/H3ePHgS3lGLW1qJ10dm1x6isJLh1qz3ld7vR2jLPimDW1xMoKsLTt2+nkbLV3IzV0oI7J6fLzsidk4PV1ESwuBhP//4/ejlALItgaWl45I5SYFn2/S4vR+k6elISru3aYgUCBEtKkJZWxOqcwFC5PXjyctE8HqzGRoyqKvTUVDSPp0M5o7qa4JYt9pJZ374onw8ptP/XenJyh5mBmCbB4mLMhgb0pGSU1xYSCpaX2wOSntsSLYtl2cua1bYusp6QgNXQgL+uDj0xEVd6OlpsrC2IVFKCGAbevH5osZ2NrYgQ3LIFo6ICMUzcPTreCxHBqKhEWlts31BsbMT/m9nYiFFejp6cbH+2XbSE0zh7NsU33IgrPZ3ez/ydTZdeRtVzz9PjwQcAqJ4+HbOykux7fkfTvPlUPfssNa+/Ts6f7ifx+ONp/u476md8RNqVv8LTty+evn2JmzCBTT//OcVXX03fN17Hm5fXbRvEsggWFVH7wQfUvvMOZkVl+JyKicHdowfu7GxcGRk0/m8uZkUlvmHDSDnmGMyaGoya6t2mrOgYiL2MUV0NloUr3d5d6UpNxayqIrhpM2KZeHr3pkHsDWkxIhCTisslmHUNmM0BXHE+LFcCgXXrkEAA0XX6isJtGbQmppKSlWXvTg0tV1itrYjfj9XSCog9ik5Koq7RJMPyo5XWYzU2kmNZWJqOFhdLsLmZTNOAllosTbM7vZQUtNhYYsobaQoYBGMSSbVaCZZsQTvE7nTbOkotPh49JQWroQGzthZXVla3I8m2mZMWF4c7M7PTSNyoqSFYUoJRVoa7R0fBdqO6GjQNPbkbkXmXC3fPngQ2bSJYXIK7V89wh2MFg/bylCXoiQloCQkR1/TbfARWayuujIwOIz0xDMzGRqz6BozQbKb9iN4oLUVaWuyOzutFeTz2iNswEMtCT0kJj+hdWVmY9fUYZWV4+vTZdo2qqvDyk6dPn3D7XVlZBDZuxKiuxh0a5VrBIMHNm8OGs31bxDAwqqqw0tLQfL5wp2/W1aEnp+DKzEDzeMLlzKpq/PX1aHFxaF4fZkMD7uzsiMYB7OURd48eKN2FUVkBRhB3z572bEkEo7wco6IClIZZX48WE4MrMxM9YVukjlhWeBZiNTfbhiI1FVdaWpfLY2Katk9GKZTLXmbscF6EmldfY+uDD+IdNJA+zz6LKz2dlPPOo/q110i/7lr0xESqnn+BuEkTSbnwQlIuvBB/QQFbbrudkuuup+XSS2n69htc2dmkT5sWvrY7K4s+zz1H4QUXUvTLaeS++Ub49202NtHy/fc0L1xIy+LFBIuLCZaXQ9DWCImfOJHk885Fi4khsGkTgcJCglu2ECwtw792Lb4BA0l76CFijzhij8zGHAOxFxHLwqyuRouPJzE9ncbGRpSuo6WlYm4tRxLi0JOSaG7cggZ4XT5wx6BcgvJ4MA1Bi8smUFAIYuHKyKC+sRVlGrT6kqh0xfHIb3/LpEmTOPaYY3j0t/dwxRmn401KRktKInPQQCoWLsSsrw8LdVhuN1pSEjWal0rLzZAeiawvbSBBF3rEavYIr92PMs6rs3bDRlYvWcj1F59HYFMh/tWr0eLi7CUU08STnY1SCj0lBbOhAauxkWNPPZU///nPjB3b0TfWNuJUmtblMpIrJQXx+23Hd7vOVwzD7thSUrpdV581axYej4fDBg4kWFaGbDLw9OmzbXnHslBuD8GyMigrQ3k8docYGwumiVlbx4x/f8LqjQXc9ft7+ejLLxk4cCD5+fkAHH3ccfz5z39mzJgx9oi+vBw9Kcn2MTU02J1qVlaHZYqueODhh7n1il9ilG8lUFyCcumIaWHWVKMnJODu3bvDaFqPjUVPSMCsrMSVnIxZW4dRUY4Anj590BMTO97LjAzbZ1NWhjc3F6OyErOurlP7lMtlH0tPx6yuxqiqwmhqQk9IRI+wnNWeuro6Xn/vXaadfz7BsjKs9etx9+qF1dRkz3ZSUnBnZ1O1eTPTX36Zaeeeiyc3l6319Vx//fW88fTTSCAQWtNXGFWVGOXlmNXVuLKzw058CQTsSLWGBszGxm0+kRBGTQ11H39MwuTJbH3gQWrffpv4Y4+lx4MPosfbs97Uyy+j5vXXqXrhBfTkZKy6OjJvuCF8DW9eHn2nv0b5gw9RHdpF3vORv9jfjXZ4+vSh99+fZtPPp7Lp51NxZWUS2LQJo7TMbpeu48vPJ2b0aBKzs3H3yCF+4kTc7WZycUce2e193RM4BmIPIiKICFroB22UlyOGgTs9o0OZUk8zKgmafM0kNpTQHGwixrJQsXY5pRR6cordaRSVggievDzE42FLsIH0BA9xHhdGVRO33vU7EkMylo//4xUmXHgZ8bG2w9BCsT4hhxijlXhlkZGViua19Ye9zQHM6mYqGmzN5YTkWPRYT6fPFOd1saV4M5988A43XXk5ntxczPp6rIZGJBjAlZoWXrduc7a294tsT6C6GqupyR51duFjAHukLH4/wdJSxDDCHR0iO3SIzpo1i/j4eI488kjQdYIlW/Bv3IgEgyhdx9OvH5rPhxUIYNXX251O/TZ/juaL4cwLL7Q7fbebDz74gClTpoQNRBtKKdw5OfjXbyC4davtUwgZnB11qm386U9/4s477sBqbsJqqA9H2uhJSfZIPMJSiyszE/+GDbSuWweWhZaQgCc7O+JMSLlcuDIyCJaVESwrw6isRE9KQk+PnC9I6bodWZWWhtXYiBYXt8ORbG1tLU899RRXX301WmwsgeJi26kNtnHo0QOlFI1K8dy77/Krn/2MYFER2f378/Ybb+Bftw49IQE9tMyox8dhNjVjlJaGfTQYhj0LAzsoITU1vCzZ5ueRykq2/PoWcLshGCTtV78i44brO9xDd1YWSWeeSd2774HLRcLJJ+Hb7v+qeTxk//Y3xB52GP41a0g4+eSInztmxAh6PvYoW//v/7Aam4gdPQZPXi6xhx5KzMiRER3e+xxtndb+/hgzZoxsz8qVK8N/l95/vxT+7JJd+ii9//5O77k9BQUFMnDgQLnkkkskPz9f7r33Xhk7dqwMy8+XO6+8UgIlJSIiEhcXJyIi7//7fZl4/ESpbK6UssYyufDyC+WPj/9RtpYtETEC4euafr/MfuMNOe2448RsaZEPPvhAfD6fLNywVSpqGyQvL09WlNTJ2edfLG+99U+56w8PitvtlmHDhsmkyZOlrjkgcXFxcsttd8jwESPk8MMPl7KysnCbJ08+WgYMzpfDxk+U/3yzTEzTkqlTp8rbb78dbkNcXJyYliWHjh0niYmJMnLkSHnkkUdERMSyLDEDAbEsS0REmpub5fzzz5dBhxwipx5zjBw2bpx88/XXEqyokLjYWLnxuutkxPDh8vlrr8mDd94pQ4cOlaFDh8qjjz4abtOgQYPkoosuksGDB8vZZ58tjfX14i8ulo+fe05GDhkiQwcMkJ+fc460traKiEjfvn2loqJCREQWLFggkyZNkoKCAsnKypIePXrIyJEjZfbs2WLU10vzihXSun69WIGAGIYhubm5YlmW1NTUiKZpMmvWLDFbWmTC+PGydu1aeemll+Saa66R//3vf5KSkiK5ubkycuRIWb9+vUyaNEluu+02GTdunAwYMEC+fP99aV62TOrWr5efnX66DMvPl1GjRsmXX34pIhK+VhunnHKKzJw5U26//XbRNE1GjhwpF110Ufh82z1t4/TTT5fRo0dLfn6+PPPMMyIiEigpkbjYWDHqG0RE5O2335apU6eKiMj69evl8MMPl2HDhsndd98tcXFx0rJmjXz64oty1Lhxctqpp0peXp7cfvvt8tprr8m4ceNk2LBhsn79ehERKS8vl7POOkvGjh0rY8eOlblz54qIyD333COXXXaZTJo0SfLy8uSvf/2riIicf/754vP5ZOTIkXLLLbdIfV2dHH3UUTJq2DAZNmyYfPDBBx3LjRghN152mayeNUvyBw6U5uXLpam2Vi699FIZNmxY+N5ZliXP/+1vcvoJJ8jxkybJIf36yS033dTp/rSxcsUKaZg9R0ruvlvq/v3vLn6xIv5Nm2TlkHxZOSRfWjds6LLcgQJ2du2I/eoOPT1KqQMmq+reYns9iHkzZzL/zTdZvHYtX69dGy7XHGymxl+DS3ORikaWKBJ1D7EiJLtiO+xx0Dwe8icfy9L169F8PubMmcPAIfmsXPo9y75fyOGHH05KnJugaVHVHOD8S6eRk2PrQcyaOdOeVTQ1MfGoI1m6ZAkTJ07soAdx6aVT+eSrb/jpGefy53vv7FKdTFOKRx5+qJMehFIKrV020aeffprY2FhWLlvGb66+mkXffUdg82aCZWU0NTczpk8f5r/+Oj6Xi1dnzOigB/H9998DsGbNGq6++mpWrVpFYmIiTz/zDFZaGr+6915e/ctfWPDee5i63kEPYntyc3M76EFMmDABPSEB38CB9t4ItzuiHsTcuXMJKkVRSQkDBgwIX6+9HsTixYvpHwrjNQwjrAdx/xNPoFwunvr739E8HpYuX84bb7zB1KlTu83S+sADDxATE8PixYuZPn16+Pj2I/YXX3yRRYsWsXDhQh5//HGqqqps34xS6Amds5LecMMN3HDDDSxbtoxevXoB4M7pgfJ4WLZ2LX9/5hlWrVrFq6++ytq1a+1U8ldcwRNPPBGuf9NNN7FgwQLeffddrrjiivC1V69ezWeffca3337L73//e4LBIA888AD9+/dn8eLFPPzww8TExvLBxx/z/bJlzJw5k1//+teIyLZyS5bw58cesyPXLAtXaipPP/88SimWLVsWvnd+vx89Lo6l69bx9ocfsmzFCv757rsUFxdHvqFKET/hKHr88Y8khtKgR8LTpw/pV11F+rXX4O3Xr8tyBwPRLDE9FRLveRmYLiJ1u7dJu4fsu+7aa+/dQQ/is88YM3YsKEVTMMj6DRuYFEqXXNxQjEtz4RNQNYUAuIItpJgWnriOMfutQZMSw01O71yWLl/Bt99+yyVXXMOyRfPZ5NWZMGECKaEloWZKtHzYAAAgAElEQVS/QXq8t1Ogg8fjYcqUKQAR9SCqW0ymnH0+jz9w707fg9mzZ3P99dej+XyMGjuW4YMG4UpNxTtwILquc94vfoFqbeWbjRs586yzOuhBzJkzh9NOO43evXszfvx4AH72s5/x+OOPc/zxx5PXrx9DjzsOq6GBS6+4gqeeeoobb7zxB7Vv++WsH6sH0UYHXYVNm3BlZ/P1999z/S23oJRi8ODB9O3bl7XtBgg/lscffzy8I7uoqIh169aR1s0S1rx58/jggw8AuOiii7jlllvQE+Lx9OzJuHHjwvoQ/fv354QTTgBsPYk2oanPP/+clSu3ybPU19eHMxSfcsopeL1evF4vmZmZYX2H9ogId911F7Nnz0bTNEpKSjqV0xNDvg2lcGVmMnfuXK677jqATvfu2GOPDScszM/PZ9OmTfTu3ZudIeO6a3eq/oHCDg2EiExQSg0ALgcWKaW+BV4Skf/u9tYdIMTFxdlhq7W1/HrqVH550UV48vLCa8IBM4AgmGLQwxWDGH6Iz4S4LFq1eEjubetEt6OmOYBCMfrwI3nrvRnououx4yfyp9uvQ0N4+OGH8bl1XLrCpWvh3c/taa8XECn/f1qcB9OIoc2utNcosCyrkx5EtHj69rWdyykpaB4PPp8PT4qdfEyPjYWWloj1th85t3+tud1oqakdjrVvbzR6Cu35sXoQbWyvq+BKTrYjxjyd/Tg/RvuhjVmzZvH5558zb948YmNjmTx5crh++3sR7TXbazNomhZ+3aYnAfb/fv78+WHNi67qd6UpMX36dCoqKli0aBFut5vc3NyI7XOnp6M8nh1u5IvmPR1+HFEFE4vIOuA3wO3AJOBxpdRqpdRZ3dVTSp2klFqjlFqvlLojwvlHlVKLQ4+1SqnaduemKqXWhR5Tf9jH2rcQ0yRQUMBxP/kJr378McGcHDSvl5KSEjZv2UxBXQEAfV0JDEyPY+W6Tfg9adQ2NPLFl192inG2RKhpCpLgc3HiMZN58ZknGTRyDKlp6dTV1LBmzRqGDRsG2E7k7EQvmqai0kGAbbn7XbrGf2a800EPYtGiRQDMmDGjkx5Ed0ycOHGn9SA2b94c1jF4/fXXOeqooxg0aBCFhYWsX78egFdffZVJkyZ1au/e1INo/9nalorWrl3L5s2bGTRoELm5uSxevBjLsigqKuLbb78N13G73d0qttXV1ZGSkkJsbCyrV69m/vz54XNZWVmsWrUKy7I65Hw64ogjwvejTaPhh3DCCSeEl5uAsOxtV0TSk8jMzMTtdjNz5kw2bdoUsVx7urp3DruXaHwQI5RSjwKrgGOAU0VkSOjvR7uppwNPAicD+cCFSqkO4QAicpOIjBKRUcATwHuhuqnAPcDhwGHAPUqpXZffdg9i+f1gGOiJiZxy2WVcdMkl/GT8ePKH5XPamaexassqABQQ21RJ70PyOe/8Cxg2fHgnPYg2GloNDMsiNc7DcZOPorqyglHjfoLPrTNy5AiGDx++TUlMKfRQlMa0adM46aSTOProo7ttc3d6EF999RUjR45k3rx54WWgNj2IkSNH8uijkb8SV1111S7Rg3jyyScZMmQINTU1HfQgzj33XIYPH46maVx55ZUA3HPPPdxwww37hB4E2NoPlmUxfPhwzj//fF5++WW8Xi/jx48nLy+P/Px8rr/+ekaPHh2uM23aNEaMGMHFF18c8ZonnXQShmEwZMgQ7rjjjnB7wfZhTJkyhSOPPLKDrOhjjz3GI488wogRI1i/fv0P0pMAe0lr4cKFjBgxgvz8fP7+9793Wz4tLY3x48czbNgwbr31Vi6++GIWLlzI8OHD+cc//sHgwYMjlovm3jnsXnaYrE8p9RXwPPCOiLRsd+4SEXm1i3o/Ae4VkRNDr+8EEJH/66L818A9IvJfpdSFwGQR+VXo3DPALBF5o6t27ovJ+kSEQEEh4m/Fe8ghiEtnS+MW6vy2G0dTGrHKRU6gBY9lQUL2NqW3biisbKIlaDI4OwGlFJYlbK5uJsHnIi3+wPzRRKsH4bBjmpubiQltPnzzzTd54403+PDDD/d2s3Y7e7s/2FfZ2WR9pwAtImKGLqYBPhFp7so4hOgJFLV7XYw9I4jUwL5AHvBlN3V7Rqg3DZgG0KfdLtN9BbO2FqvZjum3dI2i+s00BZvIiM0gyZuEp7UBVVcEnnhI6g3uHecoChoWDa1BMhK87fSGFbnp+0FMtcM+waJFi7j22msREZKTk3nxxRf3dpMc9lGiMRCfA8cBjaHXscB/gF25ze8C7BlK58Q03SAizwLPgj2D2IXt2WnEMDDKyuwdlkmJFNYX0mq00iO+Bym+FBALGsrAHQdph+xw1nDmmWdSUFCAYQmGaeF16Tz00IOceOKJe+gTRcdnn322y3QP2rOzehBdsaf0IHaWqqoqjj322E7Hv/jii24jliIxYcIElixZsqua5nAAE42B8IlIm3FARBqVUtHknC0B2sea9Qodi8QFwDXb1Z28Xd1ZUbxnJ2Q3iHhEg1FRgVgWnh49KG+txG/66ZPYhwRPKMdMcxVYQUjus0Pj0Og3eO7VNwmaFjXNQXwujX4ZnePb9wVOPPHEfc5odcfdd9+9zxmDSKSlpe3QGezQNTtaSneITDRRTE1KqbDXTCk1Bogch9iRBcAApVSeUsqDbQRmbF9IKTUYSAHmtTv8GXCCUiol5Jw+IXTsB+Hz+aiqqtrjXw6xrJD4TyLK66U+UE+8O36bcbAsaNgKnjjwdi8f6DdMNlY0sqW2hcrGAJqyxXwcHByiQ0SoqqqKGJbr0D3RzCBuBN5WSm3BDrbJBs7fUSURMZRS12J37DrwooisUErdh721u81YXAC8Ke16cRGpVkr9AdvIANwnItVRf6oQvXr1ori4mIqKih9adaewWlsxq6vRU1Mx62uoaK4g2ZtMk7vJLuBvgJYae69Dxepur9UcMKhuCpKR4MGj64iC4q5TGTk4OETA5/OFd407RE9UkqNKKTfQFnS8RkS6DszeS0SKYtpblNz8a5q+/poBs7/i7ytf4OklT/PleV+SHpMOgWZ4fBSkD4RL/7XDa/3xXyt5df4mlv/+RNz6viuR6eDgsH+yKyRHB2HvZfABo5VSiMg/dlUDDyTMxkYavviC5LPPQnk8zCyayajMUbZxAFg8HRq3wjkvRXW9pSV1DMlJdIyDg4PDHieajXL3YG9iewI4GngIOG03t2u/peG/nyN+P4mnnsqWxi2sql7FMb2PsU+KwMKXIGcU5I7f4bUsS1hRUseIXj9sI5ODg4PDriCaYek5wLFAmYhcBoxke+FhhzD1H83A3bs3MaNGMbPITm52dJ/QzuWSRVC+AsZElzlkY2UTTQGTYT2d2+3g4LDnicZAtIiIBRhKqUSgnI7hqw4hglvLaZo3n6RTT0UpxZebv6R/Un/6Jva1Cyx6GdyxMOycqK63vMTece3MIBwcHPYG0fggFiqlkoHngEXYG+bmdV/l4KF11SoaZ8/Bam6mdeVKECHx1CnUttayaOsiLh92eahgPSx/F4ad3Skza1csLa7D59Y4ZB/d8+Dg4HBg062BUPYOs/8TkVrg70qpT4FEEYmcivMgQ0Qo+fUtBDZuBE1Di4sj4eST8OTm8saKVzDF5Jg+If/D8ncg2AxjLg3Xbw2afL5qK6cMz4m4mW9ZSS35OYm4HAe1g4PDXqBbAyEiopT6BBgeel24Jxq1v9CyaBGBjRvJ+eMfSDr7bAJWgBkbZjD9wzPZULeBwamDyU8LJbBd9DJkDoWe27KYvvntZu79aCXxl7mYPKijIJBpCSu21HPeWGc1z8HBYe8QzdD0O6VUdDJaBxm1b7+DFh9P4k9/ilKK++ffz33z7sOje7j/qPuZ/tPpaEqDLYuhdIk9e2g3U5i5xt7A9+HiLZ2uvbGikeaAyXDHQe3g4LCXiMZAHA7MU0ptUEotVUotU0od9EtMZn099Z99RuKUU9BiY2k1Wvm08FNO7386b015i9P6n4ZH90CwBT7+tZ2Ub8S54fqtQZP5G6twaYpPl5fR5O+ogrW02HZQD3cc1A4ODnuJaAzEiUB/QmJBwJTQ80FN3UcfIa2tJJ9rd/pzS+bSYrQwpf+Ubf4Ey4IPrrLDW896FmK2aR7N21iF37C4clJ/WoIm/13ZUZN3WUkdMW6d/o6D2sHBYS8RjYGQLh4HLSJC7dvv4M0fQszQoQB8VvgZKd4Uxma127H+1QOw4n047l4YMqXDNb5aU4HPrXHN0YfQMzmG97/vmOh2WUkdw3omomt7PhOtg4ODA0RnID4G/hV6/gLYCPx7dzZqX6d1+Qr8q1eTEpo9tBgtfFX8Fcf1PQ6XFvL7r3gfvnoQDv0ZjL+h0zVmrSnnyP7pxHh0zji0B3PWVVDR4AfAMC1WbKlzNsg5ODjsVXZoIERkuIiMCD0PwNaIPqj3QdS+8w4qJobEKfasoG156cTckA5CSy18fIsdsXTKo530HgoqmyisambyoAwAzhjVE0vgoyVbMEyLv36xjtagxcheyXv0czk4ODi0J9pkfWFE5DulVETp0IOFpvnziD9qPHqCreXwWeFnpPpSGZMVCmGd+SdoqYZL3gOXp1P9WWvKAZg80A5tHZCVwLCeiby5YDMfLytl0aYazhjVg5OHZ++ZD+Tg4OAQgR0aCKXUze1easBooHNc5kGCBAIEi0tIPPlkAJqDzcwuns1p/U+zl5dKl8KC52DsLyBnZMRrzFpTQb/0OPqkbRPmO2NUT/748SoSvC7+esEoTh/VSYLbwcHBYY8SzQyiveSZge2LeHf3NGffJ1BUBKaJNy8PgDklc7YtL4nAJ7dCTCocE1nGsi289aLD+3Q4fv643tS3Gpw7phe9U6NRdHVwcHDYvezQQIjI73/sxZVSJwF/xVaUe15EHohQ5jzgXuzIqCUiclHouAksCxXbLCL7RIrxQEEBAJ6Qgfhk4yek+dIYnTkalr4FRfPh9Cc7hLS25+sNlfgNq9PO6QSfm5uPH7h7G+/g4ODwA4hmiem/wLmhfEyENKLfFJFulemVUjrwJHA8UAwsUErNEJGV7coMAO4ExotIjVKqfa/ZIiKjfvAn2s342xmI4oZiZhXP4tKhl6IrDeY+CtkjYORFEeuKCE/P2kB6vJfD81L3ZLMdHBwcfjDRhLlmtBkHABGpATK7Kd/GYcB6EdkoIgHgTeD07cr8EngydE1EpDy6Zu89AgWF6Onp6AkJTF81HQ2NCwdfCEXf2PrSh00DLfJt/e/KrSworOGm4wfgc+t7uOUODg4OP4xoDISplAovmCul+hLdRrmeQFG718WhY+0ZCAxUSv1PKTU/tCTVhk8ptTB0/IxIb6CUmhYqs7CioiKKJu08gYICvLm5NAQaeH/9+5yYdyLZcdl2Mj5PAgw7K2I9w7R44NPV9MuI43wnAZ+Dg8N+QDRO6ruBuUqprwAFTACm7cL3HwBMBnoBs5VSw0Mzlr4iUqKU6gd8qZRaJiIb2lcWkWeBZwHGjh27R3Z3BwoKSDj+eN5b9x5NwSYuyb8EWmrsjXGjLgZPXMR6by0sYmNFE89eMsZJ3+3g4LBfEI2T+lOl1GjgiNChG0WkMoprl9BRea5X6Fh7ioFvRCQIFCil1mIbjAUiUhJ6/41KqVnAocAG9iJGTQ1mbS2u3L5MXzWdMVljGJo2FL55BozWDloP7WnyGzz2+TrG5aZwfH7Wnm20g4ODw49kh0NZpdSZQFBE/iUi/8KWHo245LMdC4ABSqk8pZQHuACYsV2ZD7BnDyil0rGXnDYqpVKUUt52x8cDK9nLBAoLAVgZX0dpUyk/z/+5Hdq66GXoMRpyRnSqY1rCbz9cTkWDnztOHhJRGMjBwcFhXySatY57RKSu7UVo+eeeHVUSEQO4FvgMWAX8U0RWKKXuU0q1hax+BlQppVYCM4FbRaQKGIItdbokdPyB9tFPe4tAQSEAHwUW0TuhN5N6TYLiBVC+MuLswTAtbv7nYt77roSbjhvImL6RQ18dHBwc9kWi8UFEMiJRpegQkU+AT7Y79rt2fwtwc+jRvszXhFTs9iUCBQXgcvGtVsixOSegazp89wp44m2t6XYETYsb3vyeT5aVcdtJg7h68iF7qdUODg4OP45oZhALlVKPKKX6hx6PAIt2d8P2RQKFBei9elBj1JObmGvrPaz5Nwz6KXg76jY8NXMDnywr4zenDHGMg4ODw35JNAbiOiAAvBV6+IFrdmej9lX8BQUEetoZWPOS8mDL99BcBQNO6FR24aZqhvVM5IoJ/fZ0Mx0cHBx2CdFEMTUBd+yBtuzTiGkS3LSZ2hE9AMhLzIPvpgMK+h/Tqfyq0gaODqXzdnBwcNgfiSbVRgZwGzAU8LUdF5HOveIBTLCkBAkG2ZIGbs1Nj/gesO4/0GssxKV1KFvR4Key0c/gnMS91FoHBweHnSeaJabpwGogD/g9UIgdwnpQ0Zakb11CC30S+qC31EDJdxGXl9aUNQAwJDuh0zkHBweH/YVoDESaiLyAvRfiKxG5HDioZg+wLUnfspgqcpNyYf0XgMAhx3Uqu7qsHoBBjoFwcHDYj4nGQARDz6VKqVOUUocCB10q0kBBIVpiImusLXYE07r/QFwG5HROOLuqtIHMBC9p8d4931AHBweHXUQ0+xn+qJRKAn4NPAEkAjft1lbtgwQKCpA+PTBYT15iX9jwGAw8KWLm1tVl9Y7/wcHBYb8nmiimf4X+rAOO3r3N2TcR06R1zRqafzIUgFx/q52gb8DxncoapsW6rY0cdUj6nm6mg4ODwy7FSSsaBS1Ll2LV1VE8xO70c8tWg9KgX2d7WVDZRMC0GJzj+B8cHBz2bxwDEQVNc+aCprEsT5HqSyWpYA70GgexnV0xK0ttB/XgbGeJycHBYf/GMRBR0DhnDjEjRrDWLCU3oTeULoG+4yOWXV3WgFtX9M+Ij3jewcHBYX8hmo1yXuBsILd9eRG5b/c1a9/BqK6mdfly0q+9hsL6f3J08hCwDOh9eMTyq0vr6Z8Rj8fl2F4HB4f9m2h6sQ+xtaQNoKnd46Cg6X//szUfjhhNdWs1uQG/faLXuIjlV5c1MMSJYHJwcDgAiCbMtZeInLTjYgcmjXPmoKeksKWXD5ZDbl05pA3olF4DoLY5QGldK4OdDXIODg4HANHMIL5WSu1z2gx7ArEsmub+j7ijjqKwYTMAuWWrul5eCqXYcPZAODg4HAhEYyCOAhYppdYopZYqpZYppZZGc3Gl1EmheuuVUhEzwiqlzlNKrVRKrVBKvd7u+FSl1LrQY2p0H2fX0rpiJWZ1NfETjqKgrgCXctGzsQr6dO1/ACcHk4ODw4FBNEtMJ/+YCyuldOBJ4HigGFiglJrRXjpUKTUAuBMYLyI1SqnM0PFUbFnTsYBgG6gZIlLzY9ryY2mcMxuAuPHjKVz6Jb09ibgh4gxieUkdby4oIjXOQ0aCk2LDwcFh/2eHMwgR2QQkA6eGHsmhYzviMGC9iGwUkQDwJrazuz2/BJ5s6/hFpDx0/ETgvyJSHTr3X2CP+0Ga5szFN2wYrrQ0NtVvItdS4Eu2fRAhqpsC3PneUk7921wqGvz86cxhKKX2dFMdHBwcdjk7NBBKqRuwU35nhh6vKaWui+LaPYGidq+LQ8faMxAYqJT6n1JqvlLqpB9QF6XUNKXUQqXUwoqKiiiaFD0SCNCybBlxR9izha1NW8lpqoPeh4XzLxVVN3Pa3+by9sJiLh+fx5e3TOakYTm7tB0ODg4Oe4tolph+ARweUpZDKfUgMA87cd+ueP8BwGSgFzD7hzjEReRZ4FmAsWPHyi5oTxh/QQEYBt7BQ2g1WmkINpDeWAuDLgLslBoXPTef5oDJu1cdycjeybvy7R0cHBz2OtE4qRVgtnttho7tiBKgd7vXvULH2lMMzBCRoIgUAGuxDUY0dXcr/rVrAfAOHEBlSyUA6aYJvQ9nfXkD5z0zD79h8cYvj3CMg4ODwwFJNAbiJeAbpdS9Sql7gfnAC1HUWwAMUErlKaU8wAXAjO3KfIA9e0AplY695LQR+Aw4QSmVopRKAU4IHdtj+NeuBbcbb15e2EBkWAI9RnPX+8sREd6cdgT5PZyQVgcHhwOTaNJ9P6KUmoUd7gpwmYh8H0U9Qyl1LXbHrgMvisgKpdR9wEIRmcE2Q7ASe2Zyq4hUASil/sA2adP7RKT6B362naJ17Vq8eXkot5uKFtu/kZGcB9541pc3cuLQbAZmOeGsDg4OBy5dGgilVKKI1IdCTgtDj7ZzqdF02CLyCfDJdsd+1+5vAW4OPbav+yLw4o4/wu7Bv3YdsWPHAlDRbBuI9KyRNPkNqpsC9E6N2VtNc3BwcNgjdDeDeB2YAizC3ovQhgq97rcb27VXMevqMEpL8Q60w1kr64vQRUjJGMrammYAeqfE7s0mOjg4OOx2ujQQIjIl9Jy355qzb+Bftw4A38CBAFTWFZBmmmjpAyiqbgGgd6pjIBwcHA5sotkH8UU0xw4kWsMRTLaBqGgstSOY0g6hqLptBuEsMTk4OBzYdOeD8AGxQHookqgttDWRCJvWDiT8a9eiJSTgys4GoLKliixTILkvRTVriPXopMZ59nIrHRwcHHYv3fkgfgXcCPTA9kO0GYh64G+7uV17Ff/adXgHDQynzKgwmhjqigPdRVF1C71TYp10Gg4ODgc8XS4xichfQ/6HW0Skn4jkhR4jReSANRAign/durD/wbRMasQg3WfrPxTXNDsRTA4ODgcF0eyDeEIpNQzIB3ztjv9jdzZsb2GUlmI1NIT9D9XNFVgKMuJzEBGKqps5ol9nsSAHBweHA41oNKnvwd7tnI+9p+FkYC5wQBqI1jVrgHYO6vLlAKQn51HTHKQpYDoRTA4ODgcF0aTaOAc4FigTkcuAkUDSbm3VXsS/1g5x9Q4I7YGoWAFARvpgJ4LJwcHhoCIaA9EiIhZgKKUSgXI6JtI7oPCvXYu7Rw/0BDuNRkXNegDSM0dQ1LZJzplBODg4HAREYyAWKqWSgeewo5m+w073fUDiX7s2vLwEUNlQDEB62kBnk5yDg8NBRTRO6qtDf/5dKfUpkCgiUWlS72+IYeAvKCB+8uTwsYrmrSSJwuPyUlTTTEqsm3hvNDIaDg4ODvs33W2UG93dORH5bvc0ae9hVFeDYeDusU0VrtJfT0aMPWMoqm52Zg8ODg4HDd0Nhf8SevYBY4El2JvlRgALgZ/s3qbteYyQbKkrI8M+EGiiQgKke3oAUFzTQn6Oo//g4OBwcNDdRrmjReRooBQYLSJjRWQMcCh7WN1tT9HJQFRvpErXSY/NxLKEkpoWejmb5BwcHA4SonFSDxKRZW0vRGQ5MGT3NWnvsb2BkIq1VOg6GYm92drQSsC0nDTfDg4OBw3RGIilSqnnlVKTQ4/ngKic1Eqpk5RSa5RS65VSd0Q4f6lSqkIptTj0uKLdObPd8e2lSncLbQZCT08HoL5iJQFNkZ5yiBPB5ODgcNARTTjOZcBVwA2h17OBp3dUSSmlA08CxwPFwAKl1AwRWbld0bdE5NoIl2gRkVFRtG+XYVRUoCcloXnsTK2VVfau6oyEHs4mOQcHh4OOaMJcW4FHQ48fwmHAehHZCKCUehM4HdjeQOwzmJWVuDIzwq8ragvAA+kx6fyvphmloKdjIBwcHA4SulxiUkr9M/S8TCm1dPtHFNfuCRS1e11MZB2Js0PXfEcp1X6Htk8ptVApNV8pdUYXbZwWKrOwIrQ8tDMY5RXbHNQilDeUApDmTaOouoWsBB9el77T7+Pg4OCwP9DdDKJtSWnKbnz/j4A3RMSvlPoV8ApwTOhcXxEpUUr1A75USi0TkQ3tK4vIs8CzAGPHjm2vm/2jMCoqiM0dC4BVW0w1ASCOi55ZhWF4yUt3/A8ODg4HD92FuZaGnjdFekRx7RI65mzqxXbhsSJSJSL+0MvngTHtzpWEnjcCs7DDa3cbIoJRsW0GUbtpKRUuHZe4yM/OoK4lwLCeB2yOQgcHB4dOdLeTugGINCpXgIjIjnaMLQAGKKXysA3DBcBF271HTpshAk4DVoWOpwDNoZlFOjAeeCiKz/OjserqkGAwHMHUsHkJFbpOii+dFy84jJaAiccVTdCXg4ODw4FBlwZCRBJ25sIiYiilrgU+A3TgRRFZoZS6D1goIjOA65VSpwEGUA1cGqo+BHhGKWVhz3IeiBD9tEvptAdi60q26F4yYjMBiPE4vgcHB4eDi6izzimlMumoKLd5R3VE5BNskaH2x37X7u87gTsj1PsaGB5t23YFRmUlsM1AxNSsoSzFw/CErD3ZDAcHB4d9hh2umSilTlNKrQMKgK+AQuDfu7lde5z2Mwgj2Mq7ngrK3RZ9E/vs5ZY5ODg47B2iWVT/A3AEsFZE8rDV5ebv1lbtBdoMRKm3hamfXMTTKQkMDPblF8N/sZdb5uDg4LB3iMZABEWkCtDU/7d390F21fUdx9+f3c0m+5TsQxIekkCCJlZABMwganEYbDRamjiDtFRtwdamDzCCtQ/QaXEM0z+YOlg7MiqDsThSwQK228qIQAF1WiCLpFZCU0KgkhTMJvsQkuzd3Xvvt3+cc+Nlc5PcJHtyN/d+XjM72fO759z7/c0vu989v3PO9yc1RcRjJNVd60p+5yBqa+OaH/whL73+Cn+zcxerun6HrtbjuhRjZnbSqiZBjEjqJCmxcbekLwL7sg3rxMsPDtI0v5fd40Osa38rq/aO0XZ6XdYkNDOrSjUJYi2wH/g08D3gReDXsgyqFvKDgxR6k+ccukZ38nKcyuKFvTWOysysdqq5i+n3SQrq7SB50rku5XftYuKM5BmIBaP/x//EYlb0ddQ4KjOz2qnmDKIL+L6kH0q6TlJd3veZHxxkbN5sAE7f9yovxBIWuzCfmTWwIyaIiPhcRJwDXAucBjwh6ZHMIzuBimNjFOJrTwgAAA0YSURBVPfu5fWu5IRqfiHPzvY3uTCfmTW0o6kdsRN4DdgNLMwmnNoo3eI60gktNDG3WCTXs6LGUZmZ1VY1D8r9kaTHgUeBPuD3IuK8rAM7kUoJYldHkZ6mWUwyizmnLK9xVGZmtVXNReolwA0RsSnrYGqllCBem5Ojtwhbi6dzxvwj1SI0M6tv1awod1CtpHqTH0zqMO1o3U/PnjG2xFLO9B1MZtbgXL+a9AyipYUdTaPMnxxjW/E0zuzz4kBm1ticIEgSREtfH7smhugrFBimizN6nSDMrLE5QVAqs9FHrjBOb6FAsa2H9taqK6GbmdUlJwjSMhs9SVG+3kKRtrnzaxyRmVntZZogJK2WtEXSVkk3Vnj9GkmDkjalX58se+1qSS+kX1dnGWd+cJCJnuSidF+hQFf3giw/zszspJDZPIqkZuB2YBWwHdgoqb/C0qH3RsR1U47tBT5LUlY8gGfSY4enO86YnKQwPMz+tMxGb6FAd19dVhMxMzsqWZ5BXARsjYhtETEB3ENSGbYaHwAejoihNCk8DKzOIsj80BBEsCcts9FXKDJnns8gzMyyTBCLgFfKtrenbVNdIeknku6TtORojpW0TtKApIHB9GG3o9WycCHL/+PfefmdyUd35qGryw/JmZnV+iL1vwBL09IdD3OU5cQj4o6IWBkRKxcsOLa/+iXR0tPDoPbSQTNjdNLTMfuY3svMrJ5kmSB2kJTpKFmcth0QEbsjYjzdvBN4R7XHTrfdud10RxPD0Ul3+6wsP8rM7KSQZYLYCCyXtExSK3AV0F++g6TTyjbXAM+n3z8EvF9Sj6Qe4P1pW2aGckP0FGCETnraW7P8KDOzk0JmdzFFRF7SdSS/2JuBDRHxnKT1wEBE9AOfkrQGyANDwDXpsUOSbiFJMgDrI2Ioq1gBhsaGOD1fYDS6eZsThJlZdgkCICIeBB6c0nZz2fc3ARWLAUbEBmBDlvGV253bzdvyk+xRJ22tXijIzKzWF6lnhHwxz8j4CAvzOXIt82odjpnZjOAEAQznkufvFuTHmWh1gjAzAycIILlADUmZjeLs7hpHY2Y2MzhBkFx/gKRQX7T11DgaM7OZwQkC2D2WJohiAbX31jgaM7OZwQmCN04xtXQ6QZiZgRMEkCSIFproKgazO/tqHY6Z2YzgBEEyxdTTNAcBs13J1cwMcIIAkjOIbmYxEc3M7fJtrmZm4AQBJAliXlGM0kl3h8tsmJmBEwSQVnItFBmJTrpdh8nMDHCCICIYGhuiZzLPsCu5mpkd0PAJYu/kXiaKE/ROjjMancxr81oQZmbgBEExily54krOyY2xv7mL5ibVOiQzsxmh4RPEvNnzuPldN/PufSPkWrwWtZlZScMnCAAmc8yOHJOtrsNkZlaSaYKQtFrSFklbJd14mP2ukBSSVqbbSyWNSdqUfn0lyzjJjQBQmONnIMzMSjJbUU5SM3A7sArYDmyU1B8Rm6fs1wVcDzw15S1ejIjzs4rvDcaS9SBwJVczswOyPIO4CNgaEdsiYgK4B1hbYb9bgFuBXIaxHN7+pFhfU7vrMJmZlWSZIBYBr5Rtb0/bDpB0IbAkIr5b4fhlkp6V9ISkSyp9gKR1kgYkDQwODh5zoJP7kgThSq5mZr9Qs4vUkpqA24DPVHj5VeCMiLgA+GPgHyQddItRRNwRESsjYuWCBcdeZG9sNEkus+fOP+b3MDOrN1kmiB3AkrLtxWlbSRdwLvC4pJeBi4F+SSsjYjwidgNExDPAi8CKrALN7UkWDGpzgjAzOyDLBLERWC5pmaRW4Cqgv/RiRIxGxPyIWBoRS4EngTURMSBpQXqRG0lnAcuBbVkFOrl3V1LJda7XozYzK8nsLqaIyEu6DngIaAY2RMRzktYDAxHRf5jD3wuslzQJFIE/iIihrGIt7htyJVczsykySxAAEfEg8OCUtpsPse+lZd/fD9yfZWxv+OyxYUbChfrMzMr5SWqgOTfMCB10t7tQn5lZiRME0DIxyihdtM1qrnUoZmYzhhME0Do5Sq5lLpIruZqZlThBAG35PYzPch0mM7NyThD5ceZEjslW3+JqZlbOCSIt1Fec4wRhZlYu09tcTwodC7ms+etcsnDRkfc1M2sgDX8GERI/G5tDR5evQZiZlWv4BPH6eJ58MfyQnJnZFA2fIAqF4PLzTuMtp3bVOhQzsxml4a9B9HS08qWPXljrMMzMZpyGP4MwM7PKnCDMzKwiJwgzM6vICcLMzCpygjAzs4oyTRCSVkvaImmrpBsPs98VkkLSyrK2m9Ljtkj6QJZxmpnZwTK7zTVdU/p2YBWwHdgoqT8iNk/Zrwu4HniqrO1skjWszwFOBx6RtCIiClnFa2Zmb5TlGcRFwNaI2BYRE8A9wNoK+90C3ArkytrWAvdExHhEvARsTd/PzMxOkCwflFsEvFK2vR14Z/kOki4ElkTEdyX96ZRjn5xy7EHV9CStA9alm3slbTmOeOcDu47j+JNRI/YZGrPfjdhnaMx+H22fzzzUCzV7klpSE3AbcM2xvkdE3AHcMU3xDETEyiPvWT8asc/QmP1uxD5DY/Z7OvucZYLYASwp216ctpV0AecCj6dLfZ4K9EtaU8WxZmaWsSyvQWwElktaJqmV5KJzf+nFiBiNiPkRsTQilpJMKa2JiIF0v6skzZa0DFgOPJ1hrGZmNkVmZxARkZd0HfAQ0AxsiIjnJK0HBiKi/zDHPifp28BmIA9cewLuYJqWqaqTTCP2GRqz343YZ2jMfk9bnxUR0/VeZmZWR/wktZmZVeQEYWZmFTV8gqi2HMjJTtISSY9J2izpOUnXp+29kh6W9EL6b0+tY51ukpolPSvpX9PtZZKeSsf83vQmiroiqVvSfZL+W9Lzkt5V72Mt6dPp/+2fSvqWpDn1ONaSNkjaKemnZW0Vx1aJv0v7/5P02bOqNXSCKCsH8kHgbOA30zIf9SgPfCYizgYuBq5N+3oj8GhELAceTbfrzfXA82XbtwJfiIg3A8PA79Ykqmx9EfheRPwS8HaS/tftWEtaBHwKWBkR55LcGHMV9TnWfw+sntJ2qLH9IMldoMtJHir+8tF8UEMnCKovB3LSi4hXI+LH6fevk/zCWETS37vS3e4CPlybCLMhaTHwq8Cd6baAy4D70l3qsc/zgPcCXwOIiImIGKHOx5rkrsw2SS1AO/AqdTjWEfEDYGhK86HGdi3wjUg8CXRLOq3az2r0BFGpHMhBJT3qjaSlwAUkBRJPiYhX05deA06pUVhZ+Vvgz4Biut0HjEREPt2uxzFfBgwCX0+n1u6U1EEdj3VE7AA+D/yMJDGMAs9Q/2NdcqixPa7fcY2eIBqOpE7gfuCGiNhT/lok9zzXzX3Pki4HdkbEM7WO5QRrAS4EvhwRFwD7mDKdVIdj3UPy1/IykgrQHRw8DdMQpnNsGz1BNFRJD0mzSJLD3RHxQNr889IpZ/rvzlrFl4H3AGskvUwyfXgZydx8dzoNAfU55tuB7RFRKqF/H0nCqOex/hXgpYgYjIhJ4AGS8a/3sS451Nge1++4Rk8Qhy0HUk/SufevAc9HxG1lL/UDV6ffXw3884mOLSsRcVNELE5LuVwF/FtEfAx4DPhIultd9RkgIl4DXpH0lrTpfSRVCep2rEmmli6W1J7+Xy/1ua7HusyhxrYf+O30bqaLgdGyqagjavgnqSV9iGSeulQO5K9rHFImJP0y8EPgv/jFfPxfkFyH+DZwBvC/wK9HxNQLYCc9SZcCfxIRl0s6i+SMohd4Fvh4RIzXMr7pJul8kgvzrcA24BMkfxDW7VhL+hzwGyR37D0LfJJkvr2uxlrSt4BLScp6/xz4LPBPVBjbNFl+iWS6bT/wibTeXXWf1egJwszMKmv0KSYzMzsEJwgzM6vICcLMzCpygjAzs4qcIMzMrCInCLMZQNKlpWqzZjOFE4SZmVXkBGF2FCR9XNLTkjZJ+mq61sReSV9I1yJ4VNKCdN/zJT2Z1uH/TlmN/jdLekTSf0r6saQ3pW/fWbaGw93pQ05mNeMEYVYlSW8leVL3PRFxPlAAPkZSGG4gIs4BniB5shXgG8CfR8R5JE+wl9rvBm6PiLcD7yapPgpJhd0bSNYmOYuklpBZzbQceRczS70PeAewMf3jvo2kKFoRuDfd55vAA+maDN0R8UTafhfwj5K6gEUR8R2AiMgBpO/3dERsT7c3AUuBH2XfLbPKnCDMqifgroi46Q2N0l9N2e9Y69eU1wgq4J9PqzFPMZlV71HgI5IWwoF1gM8k+TkqVQz9KPCjiBgFhiVdkrb/FvBEuprfdkkfTt9jtqT2E9oLsyr5LxSzKkXEZkl/CXxfUhMwCVxLsiDPRelrO0muU0BSdvkraQIoVVSFJFl8VdL69D2uPIHdMKuaq7maHSdJeyOis9ZxmE03TzGZmVlFPoMwM7OKfAZhZmYVOUGYmVlFThBmZlaRE4SZmVXkBGFmZhX9P6auUcMDXM8zAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.7169 - accuracy: 0.7647\n",
            "Test accuracy: 0.7646999955177307\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.7801\n",
            "Test accuracy: 0.7800999879837036\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.8072\n",
            "Test accuracy: 0.807200014591217\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.9446 - accuracy: 0.7218\n",
            "Test accuracy: 0.7218000292778015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MVZxmL9nxnF"
      },
      "source": [
        "The model without dropout and without data augmentation has the highest training accuracy comparing with other models. Since it will overfitting, the training data accuracy will be higher than other models, while the validation accuracy will become worse than other models when the number of epochs increase due to overfitting.\n",
        "\n",
        "The model with only data augmentation has higher training accuracy and validation accuracy than the model with only dropout. Since dropout drops part of the network and data augmentation modifies the data's orientation and position.\n",
        "\n",
        "The model with both data augmentation and dropout perform worse than the model with only one of them applied. This is because applying both data augmentation and dropout might cause underfitting which will also bring down the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWexNgucnyTL"
      },
      "source": [
        "### **Part 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlbJ7sUoKsxC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8de012c8-f78a-4854-b103-b4ea74e33e0d"
      },
      "source": [
        "d()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.8633 - accuracy: 0.3215\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.43520, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.8626 - accuracy: 0.3218 - val_loss: 1.6011 - val_accuracy: 0.4352\n",
            "Epoch 2/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.5375 - accuracy: 0.4462\n",
            "Epoch 00002: val_accuracy improved from 0.43520 to 0.49620, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5365 - accuracy: 0.4464 - val_loss: 1.4169 - val_accuracy: 0.4962\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.3775 - accuracy: 0.5062\n",
            "Epoch 00003: val_accuracy improved from 0.49620 to 0.54540, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3775 - accuracy: 0.5062 - val_loss: 1.2928 - val_accuracy: 0.5454\n",
            "Epoch 4/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.2679 - accuracy: 0.5486\n",
            "Epoch 00004: val_accuracy improved from 0.54540 to 0.59010, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2679 - accuracy: 0.5484 - val_loss: 1.1709 - val_accuracy: 0.5901\n",
            "Epoch 5/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.1837 - accuracy: 0.5819\n",
            "Epoch 00005: val_accuracy improved from 0.59010 to 0.60950, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1837 - accuracy: 0.5819 - val_loss: 1.1176 - val_accuracy: 0.6095\n",
            "Epoch 6/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.1166 - accuracy: 0.6083\n",
            "Epoch 00006: val_accuracy improved from 0.60950 to 0.63030, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1169 - accuracy: 0.6083 - val_loss: 1.0582 - val_accuracy: 0.6303\n",
            "Epoch 7/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.0641 - accuracy: 0.6254\n",
            "Epoch 00007: val_accuracy improved from 0.63030 to 0.65080, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0639 - accuracy: 0.6256 - val_loss: 1.0123 - val_accuracy: 0.6508\n",
            "Epoch 8/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.0133 - accuracy: 0.6466\n",
            "Epoch 00008: val_accuracy improved from 0.65080 to 0.66630, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0135 - accuracy: 0.6466 - val_loss: 0.9592 - val_accuracy: 0.6663\n",
            "Epoch 9/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.9759 - accuracy: 0.6616\n",
            "Epoch 00009: val_accuracy improved from 0.66630 to 0.67160, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9759 - accuracy: 0.6617 - val_loss: 0.9468 - val_accuracy: 0.6716\n",
            "Epoch 10/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.9396 - accuracy: 0.6730\n",
            "Epoch 00010: val_accuracy improved from 0.67160 to 0.69250, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9400 - accuracy: 0.6729 - val_loss: 0.8890 - val_accuracy: 0.6925\n",
            "Epoch 11/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.9089 - accuracy: 0.6837\n",
            "Epoch 00011: val_accuracy improved from 0.69250 to 0.69760, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9091 - accuracy: 0.6839 - val_loss: 0.8827 - val_accuracy: 0.6976\n",
            "Epoch 12/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.8818 - accuracy: 0.6926\n",
            "Epoch 00012: val_accuracy improved from 0.69760 to 0.70520, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8820 - accuracy: 0.6926 - val_loss: 0.8529 - val_accuracy: 0.7052\n",
            "Epoch 13/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.8588 - accuracy: 0.7041\n",
            "Epoch 00013: val_accuracy improved from 0.70520 to 0.70960, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8593 - accuracy: 0.7039 - val_loss: 0.8418 - val_accuracy: 0.7096\n",
            "Epoch 14/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.8356 - accuracy: 0.7108\n",
            "Epoch 00014: val_accuracy improved from 0.70960 to 0.72250, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8353 - accuracy: 0.7108 - val_loss: 0.8117 - val_accuracy: 0.7225\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8211 - accuracy: 0.7153\n",
            "Epoch 00015: val_accuracy did not improve from 0.72250\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8211 - accuracy: 0.7153 - val_loss: 0.8119 - val_accuracy: 0.7211\n",
            "Epoch 16/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8016 - accuracy: 0.7236\n",
            "Epoch 00016: val_accuracy improved from 0.72250 to 0.72890, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8019 - accuracy: 0.7236 - val_loss: 0.8000 - val_accuracy: 0.7289\n",
            "Epoch 17/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7879 - accuracy: 0.7279\n",
            "Epoch 00017: val_accuracy improved from 0.72890 to 0.73160, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7877 - accuracy: 0.7280 - val_loss: 0.7810 - val_accuracy: 0.7316\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7779 - accuracy: 0.7337\n",
            "Epoch 00018: val_accuracy did not improve from 0.73160\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7779 - accuracy: 0.7337 - val_loss: 0.8052 - val_accuracy: 0.7254\n",
            "Epoch 19/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7614 - accuracy: 0.7398\n",
            "Epoch 00019: val_accuracy improved from 0.73160 to 0.74150, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7614 - accuracy: 0.7398 - val_loss: 0.7709 - val_accuracy: 0.7415\n",
            "Epoch 20/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7557 - accuracy: 0.7401\n",
            "Epoch 00020: val_accuracy did not improve from 0.74150\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7557 - accuracy: 0.7401 - val_loss: 0.7958 - val_accuracy: 0.7283\n",
            "Epoch 21/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7479 - accuracy: 0.7460\n",
            "Epoch 00021: val_accuracy improved from 0.74150 to 0.74300, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7483 - accuracy: 0.7460 - val_loss: 0.7615 - val_accuracy: 0.7430\n",
            "Epoch 22/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7335 - accuracy: 0.7486\n",
            "Epoch 00022: val_accuracy improved from 0.74300 to 0.74410, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7336 - accuracy: 0.7485 - val_loss: 0.7684 - val_accuracy: 0.7441\n",
            "Epoch 23/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7253 - accuracy: 0.7525\n",
            "Epoch 00023: val_accuracy improved from 0.74410 to 0.75270, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7249 - accuracy: 0.7526 - val_loss: 0.7298 - val_accuracy: 0.7527\n",
            "Epoch 24/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7193 - accuracy: 0.7570\n",
            "Epoch 00024: val_accuracy improved from 0.75270 to 0.75730, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7195 - accuracy: 0.7568 - val_loss: 0.7350 - val_accuracy: 0.7573\n",
            "Epoch 25/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.7121 - accuracy: 0.7588\n",
            "Epoch 00025: val_accuracy did not improve from 0.75730\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7122 - accuracy: 0.7589 - val_loss: 0.7474 - val_accuracy: 0.7505\n",
            "Epoch 26/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6998 - accuracy: 0.7610\n",
            "Epoch 00026: val_accuracy improved from 0.75730 to 0.75860, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6999 - accuracy: 0.7611 - val_loss: 0.7244 - val_accuracy: 0.7586\n",
            "Epoch 27/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7015 - accuracy: 0.7631\n",
            "Epoch 00027: val_accuracy did not improve from 0.75860\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7018 - accuracy: 0.7628 - val_loss: 0.7341 - val_accuracy: 0.7550\n",
            "Epoch 28/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6914 - accuracy: 0.7658\n",
            "Epoch 00028: val_accuracy did not improve from 0.75860\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6914 - accuracy: 0.7659 - val_loss: 0.7374 - val_accuracy: 0.7544\n",
            "Epoch 29/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6922 - accuracy: 0.7648\n",
            "Epoch 00029: val_accuracy improved from 0.75860 to 0.76780, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6928 - accuracy: 0.7645 - val_loss: 0.6922 - val_accuracy: 0.7678\n",
            "Epoch 30/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.7666\n",
            "Epoch 00030: val_accuracy did not improve from 0.76780\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6879 - accuracy: 0.7667 - val_loss: 0.7220 - val_accuracy: 0.7620\n",
            "Epoch 31/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.7704\n",
            "Epoch 00031: val_accuracy did not improve from 0.76780\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6794 - accuracy: 0.7705 - val_loss: 0.7032 - val_accuracy: 0.7658\n",
            "Epoch 32/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6788 - accuracy: 0.7704\n",
            "Epoch 00032: val_accuracy did not improve from 0.76780\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6794 - accuracy: 0.7701 - val_loss: 0.7037 - val_accuracy: 0.7612\n",
            "Epoch 33/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6748 - accuracy: 0.7740\n",
            "Epoch 00033: val_accuracy did not improve from 0.76780\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6748 - accuracy: 0.7737 - val_loss: 0.7088 - val_accuracy: 0.7622\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6695 - accuracy: 0.7750\n",
            "Epoch 00034: val_accuracy did not improve from 0.76780\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6695 - accuracy: 0.7750 - val_loss: 0.7174 - val_accuracy: 0.7622\n",
            "Epoch 35/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6736 - accuracy: 0.7717\n",
            "Epoch 00035: val_accuracy did not improve from 0.76780\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6732 - accuracy: 0.7718 - val_loss: 0.7116 - val_accuracy: 0.7667\n",
            "Epoch 36/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6706 - accuracy: 0.7782\n",
            "Epoch 00036: val_accuracy did not improve from 0.76780\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6705 - accuracy: 0.7782 - val_loss: 0.6973 - val_accuracy: 0.7634\n",
            "Epoch 37/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6623 - accuracy: 0.7777\n",
            "Epoch 00037: val_accuracy did not improve from 0.76780\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6622 - accuracy: 0.7777 - val_loss: 0.7008 - val_accuracy: 0.7654\n",
            "Epoch 38/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.7801\n",
            "Epoch 00038: val_accuracy did not improve from 0.76780\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6598 - accuracy: 0.7802 - val_loss: 0.7612 - val_accuracy: 0.7471\n",
            "Epoch 39/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6621 - accuracy: 0.7778\n",
            "Epoch 00039: val_accuracy improved from 0.76780 to 0.77380, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6621 - accuracy: 0.7778 - val_loss: 0.6892 - val_accuracy: 0.7738\n",
            "Epoch 40/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.6594 - accuracy: 0.7794\n",
            "Epoch 00040: val_accuracy did not improve from 0.77380\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6593 - accuracy: 0.7794 - val_loss: 0.6970 - val_accuracy: 0.7690\n",
            "Epoch 41/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6505 - accuracy: 0.7816\n",
            "Epoch 00041: val_accuracy did not improve from 0.77380\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6508 - accuracy: 0.7816 - val_loss: 0.6897 - val_accuracy: 0.7699\n",
            "Epoch 42/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6517 - accuracy: 0.7816\n",
            "Epoch 00042: val_accuracy did not improve from 0.77380\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6515 - accuracy: 0.7816 - val_loss: 0.7262 - val_accuracy: 0.7567\n",
            "Epoch 43/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6553 - accuracy: 0.7803\n",
            "Epoch 00043: val_accuracy improved from 0.77380 to 0.77470, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6550 - accuracy: 0.7804 - val_loss: 0.6756 - val_accuracy: 0.7747\n",
            "Epoch 44/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6493 - accuracy: 0.7832\n",
            "Epoch 00044: val_accuracy did not improve from 0.77470\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6490 - accuracy: 0.7833 - val_loss: 0.7098 - val_accuracy: 0.7655\n",
            "Epoch 45/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6494 - accuracy: 0.7846\n",
            "Epoch 00045: val_accuracy did not improve from 0.77470\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6496 - accuracy: 0.7846 - val_loss: 0.6782 - val_accuracy: 0.7728\n",
            "Epoch 46/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6437 - accuracy: 0.7848\n",
            "Epoch 00046: val_accuracy did not improve from 0.77470\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6435 - accuracy: 0.7849 - val_loss: 0.6996 - val_accuracy: 0.7672\n",
            "Epoch 47/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6465 - accuracy: 0.7841\n",
            "Epoch 00047: val_accuracy did not improve from 0.77470\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6465 - accuracy: 0.7839 - val_loss: 0.7129 - val_accuracy: 0.7656\n",
            "Epoch 48/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6408 - accuracy: 0.7870\n",
            "Epoch 00048: val_accuracy did not improve from 0.77470\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6405 - accuracy: 0.7871 - val_loss: 0.6917 - val_accuracy: 0.7709\n",
            "Epoch 49/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6368 - accuracy: 0.7892\n",
            "Epoch 00049: val_accuracy did not improve from 0.77470\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6371 - accuracy: 0.7890 - val_loss: 0.7297 - val_accuracy: 0.7694\n",
            "Epoch 50/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6407 - accuracy: 0.7876\n",
            "Epoch 00050: val_accuracy did not improve from 0.77470\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6406 - accuracy: 0.7875 - val_loss: 0.7292 - val_accuracy: 0.7723\n",
            "Epoch 51/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6352 - accuracy: 0.7899\n",
            "Epoch 00051: val_accuracy improved from 0.77470 to 0.77890, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6346 - accuracy: 0.7900 - val_loss: 0.6870 - val_accuracy: 0.7789\n",
            "Epoch 52/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6335 - accuracy: 0.7910\n",
            "Epoch 00052: val_accuracy did not improve from 0.77890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6336 - accuracy: 0.7909 - val_loss: 0.6909 - val_accuracy: 0.7745\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.7916\n",
            "Epoch 00053: val_accuracy did not improve from 0.77890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6281 - accuracy: 0.7916 - val_loss: 0.7277 - val_accuracy: 0.7772\n",
            "Epoch 54/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6287 - accuracy: 0.7896\n",
            "Epoch 00054: val_accuracy did not improve from 0.77890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6287 - accuracy: 0.7895 - val_loss: 0.7110 - val_accuracy: 0.7654\n",
            "Epoch 55/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6252 - accuracy: 0.7910\n",
            "Epoch 00055: val_accuracy did not improve from 0.77890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6251 - accuracy: 0.7911 - val_loss: 0.6918 - val_accuracy: 0.7722\n",
            "Epoch 56/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6232 - accuracy: 0.7942\n",
            "Epoch 00056: val_accuracy improved from 0.77890 to 0.78020, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6231 - accuracy: 0.7944 - val_loss: 0.6786 - val_accuracy: 0.7802\n",
            "Epoch 57/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6265 - accuracy: 0.7921\n",
            "Epoch 00057: val_accuracy did not improve from 0.78020\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6263 - accuracy: 0.7921 - val_loss: 0.7181 - val_accuracy: 0.7693\n",
            "Epoch 58/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6276 - accuracy: 0.7914\n",
            "Epoch 00058: val_accuracy improved from 0.78020 to 0.78180, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6277 - accuracy: 0.7915 - val_loss: 0.6862 - val_accuracy: 0.7818\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6219 - accuracy: 0.7934\n",
            "Epoch 00059: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6219 - accuracy: 0.7934 - val_loss: 0.7120 - val_accuracy: 0.7761\n",
            "Epoch 60/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6218 - accuracy: 0.7949\n",
            "Epoch 00060: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6218 - accuracy: 0.7948 - val_loss: 0.6789 - val_accuracy: 0.7804\n",
            "Epoch 61/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6169 - accuracy: 0.7970\n",
            "Epoch 00061: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6170 - accuracy: 0.7969 - val_loss: 0.7834 - val_accuracy: 0.7633\n",
            "Epoch 62/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6190 - accuracy: 0.7921\n",
            "Epoch 00062: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6188 - accuracy: 0.7921 - val_loss: 0.7122 - val_accuracy: 0.7668\n",
            "Epoch 63/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6164 - accuracy: 0.7947\n",
            "Epoch 00063: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6165 - accuracy: 0.7947 - val_loss: 0.6810 - val_accuracy: 0.7772\n",
            "Epoch 64/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6121 - accuracy: 0.7976\n",
            "Epoch 00064: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6120 - accuracy: 0.7976 - val_loss: 0.7060 - val_accuracy: 0.7779\n",
            "Epoch 65/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6125 - accuracy: 0.7972\n",
            "Epoch 00065: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6127 - accuracy: 0.7970 - val_loss: 0.6830 - val_accuracy: 0.7789\n",
            "Epoch 66/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6133 - accuracy: 0.7960\n",
            "Epoch 00066: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6131 - accuracy: 0.7959 - val_loss: 0.6703 - val_accuracy: 0.7784\n",
            "Epoch 67/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6115 - accuracy: 0.7974\n",
            "Epoch 00067: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6111 - accuracy: 0.7977 - val_loss: 0.7412 - val_accuracy: 0.7672\n",
            "Epoch 68/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6115 - accuracy: 0.7994\n",
            "Epoch 00068: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6115 - accuracy: 0.7995 - val_loss: 0.6953 - val_accuracy: 0.7742\n",
            "Epoch 69/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6068 - accuracy: 0.7985\n",
            "Epoch 00069: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6068 - accuracy: 0.7983 - val_loss: 0.7033 - val_accuracy: 0.7701\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6022 - accuracy: 0.8005\n",
            "Epoch 00070: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6022 - accuracy: 0.8005 - val_loss: 0.6849 - val_accuracy: 0.7803\n",
            "Epoch 71/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6083 - accuracy: 0.8002\n",
            "Epoch 00071: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6082 - accuracy: 0.8003 - val_loss: 0.6992 - val_accuracy: 0.7748\n",
            "Epoch 72/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6062 - accuracy: 0.7987\n",
            "Epoch 00072: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6061 - accuracy: 0.7987 - val_loss: 0.7121 - val_accuracy: 0.7684\n",
            "Epoch 73/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6053 - accuracy: 0.8005\n",
            "Epoch 00073: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6056 - accuracy: 0.8003 - val_loss: 0.7858 - val_accuracy: 0.7596\n",
            "Epoch 74/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6131 - accuracy: 0.7971\n",
            "Epoch 00074: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6132 - accuracy: 0.7971 - val_loss: 0.7637 - val_accuracy: 0.7485\n",
            "Epoch 75/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5998 - accuracy: 0.7992\n",
            "Epoch 00075: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6004 - accuracy: 0.7990 - val_loss: 0.7900 - val_accuracy: 0.7654\n",
            "Epoch 76/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6022 - accuracy: 0.8005\n",
            "Epoch 00076: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6016 - accuracy: 0.8007 - val_loss: 0.6911 - val_accuracy: 0.7775\n",
            "Epoch 77/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6109 - accuracy: 0.8010\n",
            "Epoch 00077: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6105 - accuracy: 0.8012 - val_loss: 0.7241 - val_accuracy: 0.7704\n",
            "Epoch 78/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5994 - accuracy: 0.8007\n",
            "Epoch 00078: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5993 - accuracy: 0.8008 - val_loss: 0.6723 - val_accuracy: 0.7817\n",
            "Epoch 79/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6007 - accuracy: 0.8007\n",
            "Epoch 00079: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6007 - accuracy: 0.8008 - val_loss: 0.8323 - val_accuracy: 0.7540\n",
            "Epoch 80/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5989 - accuracy: 0.8015\n",
            "Epoch 00080: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5989 - accuracy: 0.8014 - val_loss: 0.7194 - val_accuracy: 0.7708\n",
            "Epoch 81/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.5993 - accuracy: 0.8016\n",
            "Epoch 00081: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5989 - accuracy: 0.8017 - val_loss: 0.7046 - val_accuracy: 0.7732\n",
            "Epoch 82/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.5954 - accuracy: 0.8033\n",
            "Epoch 00082: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5961 - accuracy: 0.8029 - val_loss: 0.7860 - val_accuracy: 0.7701\n",
            "Epoch 83/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5928 - accuracy: 0.8029\n",
            "Epoch 00083: val_accuracy did not improve from 0.78180\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5930 - accuracy: 0.8030 - val_loss: 0.6904 - val_accuracy: 0.7785\n",
            "Epoch 84/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.5951 - accuracy: 0.8042\n",
            "Epoch 00084: val_accuracy improved from 0.78180 to 0.78600, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5950 - accuracy: 0.8042 - val_loss: 0.6602 - val_accuracy: 0.7860\n",
            "Epoch 85/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5962 - accuracy: 0.8002\n",
            "Epoch 00085: val_accuracy did not improve from 0.78600\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5962 - accuracy: 0.8002 - val_loss: 0.6911 - val_accuracy: 0.7810\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.5908 - accuracy: 0.8037\n",
            "Epoch 00086: val_accuracy did not improve from 0.78600\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5908 - accuracy: 0.8037 - val_loss: 0.7509 - val_accuracy: 0.7624\n",
            "Epoch 87/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5940 - accuracy: 0.8059\n",
            "Epoch 00087: val_accuracy did not improve from 0.78600\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5938 - accuracy: 0.8059 - val_loss: 0.6749 - val_accuracy: 0.7817\n",
            "Epoch 88/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.5903 - accuracy: 0.8052\n",
            "Epoch 00088: val_accuracy did not improve from 0.78600\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5906 - accuracy: 0.8052 - val_loss: 0.8051 - val_accuracy: 0.7773\n",
            "Epoch 89/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.5944 - accuracy: 0.8043\n",
            "Epoch 00089: val_accuracy did not improve from 0.78600\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5952 - accuracy: 0.8041 - val_loss: 0.7485 - val_accuracy: 0.7658\n",
            "Epoch 90/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5899 - accuracy: 0.8063\n",
            "Epoch 00090: val_accuracy did not improve from 0.78600\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5896 - accuracy: 0.8065 - val_loss: 0.6573 - val_accuracy: 0.7848\n",
            "Epoch 91/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.5935 - accuracy: 0.8038\n",
            "Epoch 00091: val_accuracy did not improve from 0.78600\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5935 - accuracy: 0.8038 - val_loss: 0.6965 - val_accuracy: 0.7786\n",
            "Epoch 92/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5934 - accuracy: 0.8021\n",
            "Epoch 00092: val_accuracy improved from 0.78600 to 0.78870, saving model to best_RMSprop_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5932 - accuracy: 0.8022 - val_loss: 0.6457 - val_accuracy: 0.7887\n",
            "Epoch 93/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5924 - accuracy: 0.8028\n",
            "Epoch 00093: val_accuracy did not improve from 0.78870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5927 - accuracy: 0.8027 - val_loss: 0.8024 - val_accuracy: 0.7569\n",
            "Epoch 94/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5892 - accuracy: 0.8083\n",
            "Epoch 00094: val_accuracy did not improve from 0.78870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5890 - accuracy: 0.8083 - val_loss: 0.6842 - val_accuracy: 0.7748\n",
            "Epoch 95/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.5911 - accuracy: 0.8031\n",
            "Epoch 00095: val_accuracy did not improve from 0.78870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5909 - accuracy: 0.8031 - val_loss: 0.6961 - val_accuracy: 0.7831\n",
            "Epoch 96/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.5928 - accuracy: 0.8039\n",
            "Epoch 00096: val_accuracy did not improve from 0.78870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5930 - accuracy: 0.8040 - val_loss: 0.7275 - val_accuracy: 0.7651\n",
            "Epoch 97/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.8069\n",
            "Epoch 00097: val_accuracy did not improve from 0.78870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5862 - accuracy: 0.8069 - val_loss: 0.7389 - val_accuracy: 0.7809\n",
            "Epoch 98/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5882 - accuracy: 0.8046\n",
            "Epoch 00098: val_accuracy did not improve from 0.78870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5885 - accuracy: 0.8046 - val_loss: 0.7460 - val_accuracy: 0.7804\n",
            "Epoch 99/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.5859 - accuracy: 0.8076\n",
            "Epoch 00099: val_accuracy did not improve from 0.78870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5855 - accuracy: 0.8077 - val_loss: 0.7491 - val_accuracy: 0.7707\n",
            "Epoch 100/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.5950 - accuracy: 0.8032\n",
            "Epoch 00100: val_accuracy did not improve from 0.78870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5950 - accuracy: 0.8032 - val_loss: 0.6935 - val_accuracy: 0.7801\n",
            "Epoch 1/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 2.2652 - accuracy: 0.1447\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.25320, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 2.2652 - accuracy: 0.1447 - val_loss: 2.1357 - val_accuracy: 0.2532\n",
            "Epoch 2/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 2.0481 - accuracy: 0.2460\n",
            "Epoch 00002: val_accuracy improved from 0.25320 to 0.30500, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 2.0479 - accuracy: 0.2462 - val_loss: 1.9593 - val_accuracy: 0.3050\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.9569 - accuracy: 0.2903\n",
            "Epoch 00003: val_accuracy improved from 0.30500 to 0.33340, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.9569 - accuracy: 0.2903 - val_loss: 1.8883 - val_accuracy: 0.3334\n",
            "Epoch 4/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.8847 - accuracy: 0.3188\n",
            "Epoch 00004: val_accuracy improved from 0.33340 to 0.35850, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8845 - accuracy: 0.3189 - val_loss: 1.8130 - val_accuracy: 0.3585\n",
            "Epoch 5/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.8095 - accuracy: 0.3487\n",
            "Epoch 00005: val_accuracy improved from 0.35850 to 0.38530, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8095 - accuracy: 0.3487 - val_loss: 1.7390 - val_accuracy: 0.3853\n",
            "Epoch 6/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.7515 - accuracy: 0.3701\n",
            "Epoch 00006: val_accuracy improved from 0.38530 to 0.40310, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7515 - accuracy: 0.3702 - val_loss: 1.6966 - val_accuracy: 0.4031\n",
            "Epoch 7/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.7089 - accuracy: 0.3848\n",
            "Epoch 00007: val_accuracy improved from 0.40310 to 0.41490, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7088 - accuracy: 0.3849 - val_loss: 1.6559 - val_accuracy: 0.4149\n",
            "Epoch 8/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.6751 - accuracy: 0.3930\n",
            "Epoch 00008: val_accuracy improved from 0.41490 to 0.42490, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6751 - accuracy: 0.3930 - val_loss: 1.6221 - val_accuracy: 0.4249\n",
            "Epoch 9/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.6505 - accuracy: 0.4053\n",
            "Epoch 00009: val_accuracy improved from 0.42490 to 0.43320, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6505 - accuracy: 0.4053 - val_loss: 1.6001 - val_accuracy: 0.4332\n",
            "Epoch 10/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.6312 - accuracy: 0.4114\n",
            "Epoch 00010: val_accuracy improved from 0.43320 to 0.44000, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6312 - accuracy: 0.4113 - val_loss: 1.5756 - val_accuracy: 0.4400\n",
            "Epoch 11/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.6157 - accuracy: 0.4164\n",
            "Epoch 00011: val_accuracy improved from 0.44000 to 0.44450, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6153 - accuracy: 0.4165 - val_loss: 1.5594 - val_accuracy: 0.4445\n",
            "Epoch 12/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.5981 - accuracy: 0.4229\n",
            "Epoch 00012: val_accuracy improved from 0.44450 to 0.45040, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5979 - accuracy: 0.4230 - val_loss: 1.5487 - val_accuracy: 0.4504\n",
            "Epoch 13/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.5817 - accuracy: 0.4273\n",
            "Epoch 00013: val_accuracy improved from 0.45040 to 0.45520, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5815 - accuracy: 0.4274 - val_loss: 1.5311 - val_accuracy: 0.4552\n",
            "Epoch 14/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.5724 - accuracy: 0.4316\n",
            "Epoch 00014: val_accuracy improved from 0.45520 to 0.45980, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5723 - accuracy: 0.4316 - val_loss: 1.5226 - val_accuracy: 0.4598\n",
            "Epoch 15/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.5567 - accuracy: 0.4378\n",
            "Epoch 00015: val_accuracy improved from 0.45980 to 0.46670, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5574 - accuracy: 0.4375 - val_loss: 1.5071 - val_accuracy: 0.4667\n",
            "Epoch 16/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.5442 - accuracy: 0.4422\n",
            "Epoch 00016: val_accuracy improved from 0.46670 to 0.47140, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5441 - accuracy: 0.4423 - val_loss: 1.5004 - val_accuracy: 0.4714\n",
            "Epoch 17/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.5422 - accuracy: 0.4436\n",
            "Epoch 00017: val_accuracy did not improve from 0.47140\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5421 - accuracy: 0.4435 - val_loss: 1.4967 - val_accuracy: 0.4713\n",
            "Epoch 18/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.5312 - accuracy: 0.4473\n",
            "Epoch 00018: val_accuracy improved from 0.47140 to 0.47690, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5310 - accuracy: 0.4473 - val_loss: 1.4839 - val_accuracy: 0.4769\n",
            "Epoch 19/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.5214 - accuracy: 0.4504\n",
            "Epoch 00019: val_accuracy improved from 0.47690 to 0.47880, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5218 - accuracy: 0.4505 - val_loss: 1.4766 - val_accuracy: 0.4788\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.5103 - accuracy: 0.4576\n",
            "Epoch 00020: val_accuracy improved from 0.47880 to 0.48250, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5103 - accuracy: 0.4576 - val_loss: 1.4636 - val_accuracy: 0.4825\n",
            "Epoch 21/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.5038 - accuracy: 0.4566\n",
            "Epoch 00021: val_accuracy improved from 0.48250 to 0.48830, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.5037 - accuracy: 0.4566 - val_loss: 1.4557 - val_accuracy: 0.4883\n",
            "Epoch 22/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.5004 - accuracy: 0.4597\n",
            "Epoch 00022: val_accuracy did not improve from 0.48830\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4997 - accuracy: 0.4598 - val_loss: 1.4455 - val_accuracy: 0.4881\n",
            "Epoch 23/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.4900 - accuracy: 0.4653\n",
            "Epoch 00023: val_accuracy improved from 0.48830 to 0.49030, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4903 - accuracy: 0.4651 - val_loss: 1.4461 - val_accuracy: 0.4903\n",
            "Epoch 24/100\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.4825 - accuracy: 0.4646\n",
            "Epoch 00024: val_accuracy improved from 0.49030 to 0.49670, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4820 - accuracy: 0.4653 - val_loss: 1.4363 - val_accuracy: 0.4967\n",
            "Epoch 25/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.4762 - accuracy: 0.4668\n",
            "Epoch 00025: val_accuracy improved from 0.49670 to 0.49740, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4758 - accuracy: 0.4669 - val_loss: 1.4287 - val_accuracy: 0.4974\n",
            "Epoch 26/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.4658 - accuracy: 0.4720\n",
            "Epoch 00026: val_accuracy improved from 0.49740 to 0.50190, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4662 - accuracy: 0.4719 - val_loss: 1.4160 - val_accuracy: 0.5019\n",
            "Epoch 27/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.4570 - accuracy: 0.4731\n",
            "Epoch 00027: val_accuracy did not improve from 0.50190\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4576 - accuracy: 0.4730 - val_loss: 1.4185 - val_accuracy: 0.5018\n",
            "Epoch 28/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.4515 - accuracy: 0.4781\n",
            "Epoch 00028: val_accuracy improved from 0.50190 to 0.50560, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4518 - accuracy: 0.4780 - val_loss: 1.4046 - val_accuracy: 0.5056\n",
            "Epoch 29/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.4414 - accuracy: 0.4828\n",
            "Epoch 00029: val_accuracy improved from 0.50560 to 0.50600, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.4413 - accuracy: 0.4828 - val_loss: 1.4069 - val_accuracy: 0.5060\n",
            "Epoch 30/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.4377 - accuracy: 0.4840\n",
            "Epoch 00030: val_accuracy improved from 0.50600 to 0.51090, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4376 - accuracy: 0.4840 - val_loss: 1.3978 - val_accuracy: 0.5109\n",
            "Epoch 31/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.4301 - accuracy: 0.4856\n",
            "Epoch 00031: val_accuracy improved from 0.51090 to 0.51180, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4300 - accuracy: 0.4856 - val_loss: 1.3907 - val_accuracy: 0.5118\n",
            "Epoch 32/100\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 1.4202 - accuracy: 0.4877\n",
            "Epoch 00032: val_accuracy improved from 0.51180 to 0.51750, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4197 - accuracy: 0.4879 - val_loss: 1.3763 - val_accuracy: 0.5175\n",
            "Epoch 33/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.4132 - accuracy: 0.4917\n",
            "Epoch 00033: val_accuracy improved from 0.51750 to 0.51900, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4134 - accuracy: 0.4916 - val_loss: 1.3724 - val_accuracy: 0.5190\n",
            "Epoch 34/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.4060 - accuracy: 0.4947\n",
            "Epoch 00034: val_accuracy improved from 0.51900 to 0.52180, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4050 - accuracy: 0.4951 - val_loss: 1.3636 - val_accuracy: 0.5218\n",
            "Epoch 35/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.4001 - accuracy: 0.4994\n",
            "Epoch 00035: val_accuracy improved from 0.52180 to 0.52430, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.4001 - accuracy: 0.4990 - val_loss: 1.3549 - val_accuracy: 0.5243\n",
            "Epoch 36/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.3879 - accuracy: 0.5032\n",
            "Epoch 00036: val_accuracy improved from 0.52430 to 0.52490, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3880 - accuracy: 0.5031 - val_loss: 1.3500 - val_accuracy: 0.5249\n",
            "Epoch 37/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.3845 - accuracy: 0.5038\n",
            "Epoch 00037: val_accuracy improved from 0.52490 to 0.52870, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3851 - accuracy: 0.5036 - val_loss: 1.3365 - val_accuracy: 0.5287\n",
            "Epoch 38/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.3758 - accuracy: 0.5064\n",
            "Epoch 00038: val_accuracy improved from 0.52870 to 0.53390, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3756 - accuracy: 0.5064 - val_loss: 1.3295 - val_accuracy: 0.5339\n",
            "Epoch 39/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.3675 - accuracy: 0.5078\n",
            "Epoch 00039: val_accuracy improved from 0.53390 to 0.53500, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3686 - accuracy: 0.5074 - val_loss: 1.3216 - val_accuracy: 0.5350\n",
            "Epoch 40/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.3576 - accuracy: 0.5125\n",
            "Epoch 00040: val_accuracy improved from 0.53500 to 0.53590, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3570 - accuracy: 0.5130 - val_loss: 1.3170 - val_accuracy: 0.5359\n",
            "Epoch 41/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.3522 - accuracy: 0.5137\n",
            "Epoch 00041: val_accuracy improved from 0.53590 to 0.54140, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3526 - accuracy: 0.5135 - val_loss: 1.3043 - val_accuracy: 0.5414\n",
            "Epoch 42/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.3506 - accuracy: 0.5157\n",
            "Epoch 00042: val_accuracy did not improve from 0.54140\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3508 - accuracy: 0.5156 - val_loss: 1.3037 - val_accuracy: 0.5400\n",
            "Epoch 43/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.3395 - accuracy: 0.5206\n",
            "Epoch 00043: val_accuracy improved from 0.54140 to 0.54600, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3393 - accuracy: 0.5207 - val_loss: 1.2935 - val_accuracy: 0.5460\n",
            "Epoch 44/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.3357 - accuracy: 0.5197\n",
            "Epoch 00044: val_accuracy improved from 0.54600 to 0.54710, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3357 - accuracy: 0.5199 - val_loss: 1.2897 - val_accuracy: 0.5471\n",
            "Epoch 45/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.3263 - accuracy: 0.5239\n",
            "Epoch 00045: val_accuracy improved from 0.54710 to 0.55290, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3271 - accuracy: 0.5235 - val_loss: 1.2763 - val_accuracy: 0.5529\n",
            "Epoch 46/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.3221 - accuracy: 0.5253\n",
            "Epoch 00046: val_accuracy did not improve from 0.55290\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3220 - accuracy: 0.5254 - val_loss: 1.2826 - val_accuracy: 0.5465\n",
            "Epoch 47/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.3149 - accuracy: 0.5300\n",
            "Epoch 00047: val_accuracy improved from 0.55290 to 0.55460, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3148 - accuracy: 0.5301 - val_loss: 1.2695 - val_accuracy: 0.5546\n",
            "Epoch 48/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.3068 - accuracy: 0.5349\n",
            "Epoch 00048: val_accuracy improved from 0.55460 to 0.55490, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3065 - accuracy: 0.5351 - val_loss: 1.2677 - val_accuracy: 0.5549\n",
            "Epoch 49/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.3076 - accuracy: 0.5313\n",
            "Epoch 00049: val_accuracy improved from 0.55490 to 0.55810, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.3078 - accuracy: 0.5311 - val_loss: 1.2591 - val_accuracy: 0.5581\n",
            "Epoch 50/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.2978 - accuracy: 0.5342\n",
            "Epoch 00050: val_accuracy improved from 0.55810 to 0.56180, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2978 - accuracy: 0.5342 - val_loss: 1.2546 - val_accuracy: 0.5618\n",
            "Epoch 51/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.2925 - accuracy: 0.5385\n",
            "Epoch 00051: val_accuracy did not improve from 0.56180\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2927 - accuracy: 0.5381 - val_loss: 1.2587 - val_accuracy: 0.5569\n",
            "Epoch 52/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.2873 - accuracy: 0.5402\n",
            "Epoch 00052: val_accuracy did not improve from 0.56180\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2874 - accuracy: 0.5402 - val_loss: 1.2492 - val_accuracy: 0.5614\n",
            "Epoch 53/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.2835 - accuracy: 0.5431\n",
            "Epoch 00053: val_accuracy improved from 0.56180 to 0.56780, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2835 - accuracy: 0.5430 - val_loss: 1.2416 - val_accuracy: 0.5678\n",
            "Epoch 54/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.2839 - accuracy: 0.5415\n",
            "Epoch 00054: val_accuracy did not improve from 0.56780\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2832 - accuracy: 0.5417 - val_loss: 1.2426 - val_accuracy: 0.5652\n",
            "Epoch 55/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.2748 - accuracy: 0.5435\n",
            "Epoch 00055: val_accuracy improved from 0.56780 to 0.57060, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2743 - accuracy: 0.5437 - val_loss: 1.2281 - val_accuracy: 0.5706\n",
            "Epoch 56/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.2691 - accuracy: 0.5462\n",
            "Epoch 00056: val_accuracy improved from 0.57060 to 0.57160, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2691 - accuracy: 0.5462 - val_loss: 1.2272 - val_accuracy: 0.5716\n",
            "Epoch 57/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.2660 - accuracy: 0.5491\n",
            "Epoch 00057: val_accuracy improved from 0.57160 to 0.57670, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2662 - accuracy: 0.5491 - val_loss: 1.2170 - val_accuracy: 0.5767\n",
            "Epoch 58/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.2617 - accuracy: 0.5491\n",
            "Epoch 00058: val_accuracy did not improve from 0.57670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2617 - accuracy: 0.5493 - val_loss: 1.2326 - val_accuracy: 0.5634\n",
            "Epoch 59/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.2587 - accuracy: 0.5518\n",
            "Epoch 00059: val_accuracy did not improve from 0.57670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2587 - accuracy: 0.5517 - val_loss: 1.2143 - val_accuracy: 0.5763\n",
            "Epoch 60/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.2549 - accuracy: 0.5516\n",
            "Epoch 00060: val_accuracy did not improve from 0.57670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2548 - accuracy: 0.5516 - val_loss: 1.2135 - val_accuracy: 0.5735\n",
            "Epoch 61/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.2459 - accuracy: 0.5552\n",
            "Epoch 00061: val_accuracy improved from 0.57670 to 0.58350, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2460 - accuracy: 0.5551 - val_loss: 1.1996 - val_accuracy: 0.5835\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.2421 - accuracy: 0.5576\n",
            "Epoch 00062: val_accuracy did not improve from 0.58350\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2421 - accuracy: 0.5576 - val_loss: 1.2024 - val_accuracy: 0.5799\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.2402 - accuracy: 0.5582\n",
            "Epoch 00063: val_accuracy did not improve from 0.58350\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2402 - accuracy: 0.5582 - val_loss: 1.2014 - val_accuracy: 0.5794\n",
            "Epoch 64/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.2367 - accuracy: 0.5598\n",
            "Epoch 00064: val_accuracy improved from 0.58350 to 0.58640, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2363 - accuracy: 0.5599 - val_loss: 1.1893 - val_accuracy: 0.5864\n",
            "Epoch 65/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.2329 - accuracy: 0.5638\n",
            "Epoch 00065: val_accuracy improved from 0.58640 to 0.58650, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2326 - accuracy: 0.5638 - val_loss: 1.1869 - val_accuracy: 0.5865\n",
            "Epoch 66/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.2287 - accuracy: 0.5617\n",
            "Epoch 00066: val_accuracy did not improve from 0.58650\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2284 - accuracy: 0.5619 - val_loss: 1.1873 - val_accuracy: 0.5831\n",
            "Epoch 67/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.2222 - accuracy: 0.5647\n",
            "Epoch 00067: val_accuracy improved from 0.58650 to 0.58830, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2224 - accuracy: 0.5648 - val_loss: 1.1785 - val_accuracy: 0.5883\n",
            "Epoch 68/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.2215 - accuracy: 0.5670\n",
            "Epoch 00068: val_accuracy improved from 0.58830 to 0.58900, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2214 - accuracy: 0.5672 - val_loss: 1.1779 - val_accuracy: 0.5890\n",
            "Epoch 69/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.2162 - accuracy: 0.5679\n",
            "Epoch 00069: val_accuracy did not improve from 0.58900\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2161 - accuracy: 0.5682 - val_loss: 1.1749 - val_accuracy: 0.5886\n",
            "Epoch 70/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.2183 - accuracy: 0.5654\n",
            "Epoch 00070: val_accuracy did not improve from 0.58900\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2180 - accuracy: 0.5656 - val_loss: 1.1793 - val_accuracy: 0.5837\n",
            "Epoch 71/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.2127 - accuracy: 0.5672\n",
            "Epoch 00071: val_accuracy improved from 0.58900 to 0.59630, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2126 - accuracy: 0.5673 - val_loss: 1.1616 - val_accuracy: 0.5963\n",
            "Epoch 72/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.2068 - accuracy: 0.5696\n",
            "Epoch 00072: val_accuracy did not improve from 0.59630\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2072 - accuracy: 0.5695 - val_loss: 1.1719 - val_accuracy: 0.5905\n",
            "Epoch 73/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.2017 - accuracy: 0.5721\n",
            "Epoch 00073: val_accuracy did not improve from 0.59630\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.2007 - accuracy: 0.5723 - val_loss: 1.1615 - val_accuracy: 0.5932\n",
            "Epoch 74/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 1.1990 - accuracy: 0.5738\n",
            "Epoch 00074: val_accuracy improved from 0.59630 to 0.59710, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1987 - accuracy: 0.5739 - val_loss: 1.1541 - val_accuracy: 0.5971\n",
            "Epoch 75/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.1980 - accuracy: 0.5736\n",
            "Epoch 00075: val_accuracy did not improve from 0.59710\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1982 - accuracy: 0.5736 - val_loss: 1.1590 - val_accuracy: 0.5959\n",
            "Epoch 76/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.1936 - accuracy: 0.5734\n",
            "Epoch 00076: val_accuracy improved from 0.59710 to 0.59990, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1932 - accuracy: 0.5735 - val_loss: 1.1458 - val_accuracy: 0.5999\n",
            "Epoch 77/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.1892 - accuracy: 0.5783\n",
            "Epoch 00077: val_accuracy did not improve from 0.59990\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1895 - accuracy: 0.5781 - val_loss: 1.1528 - val_accuracy: 0.5973\n",
            "Epoch 78/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.1878 - accuracy: 0.5767\n",
            "Epoch 00078: val_accuracy improved from 0.59990 to 0.60000, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1880 - accuracy: 0.5767 - val_loss: 1.1457 - val_accuracy: 0.6000\n",
            "Epoch 79/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.1862 - accuracy: 0.5786\n",
            "Epoch 00079: val_accuracy improved from 0.60000 to 0.60320, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1862 - accuracy: 0.5786 - val_loss: 1.1408 - val_accuracy: 0.6032\n",
            "Epoch 80/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.1833 - accuracy: 0.5808\n",
            "Epoch 00080: val_accuracy did not improve from 0.60320\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1829 - accuracy: 0.5809 - val_loss: 1.1455 - val_accuracy: 0.6008\n",
            "Epoch 81/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 1.1774 - accuracy: 0.5802\n",
            "Epoch 00081: val_accuracy did not improve from 0.60320\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1770 - accuracy: 0.5804 - val_loss: 1.1394 - val_accuracy: 0.6003\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.1732 - accuracy: 0.5824\n",
            "Epoch 00082: val_accuracy improved from 0.60320 to 0.60760, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1732 - accuracy: 0.5824 - val_loss: 1.1298 - val_accuracy: 0.6076\n",
            "Epoch 83/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.1719 - accuracy: 0.5872\n",
            "Epoch 00083: val_accuracy improved from 0.60760 to 0.60770, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1724 - accuracy: 0.5871 - val_loss: 1.1318 - val_accuracy: 0.6077\n",
            "Epoch 84/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.1671 - accuracy: 0.5884\n",
            "Epoch 00084: val_accuracy did not improve from 0.60770\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1670 - accuracy: 0.5881 - val_loss: 1.1333 - val_accuracy: 0.6063\n",
            "Epoch 85/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.1689 - accuracy: 0.5840\n",
            "Epoch 00085: val_accuracy did not improve from 0.60770\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1689 - accuracy: 0.5840 - val_loss: 1.1289 - val_accuracy: 0.6060\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.1595 - accuracy: 0.5891\n",
            "Epoch 00086: val_accuracy did not improve from 0.60770\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1595 - accuracy: 0.5891 - val_loss: 1.1313 - val_accuracy: 0.6045\n",
            "Epoch 87/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.1574 - accuracy: 0.5893\n",
            "Epoch 00087: val_accuracy improved from 0.60770 to 0.61240, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1575 - accuracy: 0.5891 - val_loss: 1.1154 - val_accuracy: 0.6124\n",
            "Epoch 88/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.1569 - accuracy: 0.5877\n",
            "Epoch 00088: val_accuracy improved from 0.61240 to 0.61450, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1568 - accuracy: 0.5876 - val_loss: 1.1125 - val_accuracy: 0.6145\n",
            "Epoch 89/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.1508 - accuracy: 0.5911\n",
            "Epoch 00089: val_accuracy did not improve from 0.61450\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1509 - accuracy: 0.5912 - val_loss: 1.1176 - val_accuracy: 0.6131\n",
            "Epoch 90/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.1538 - accuracy: 0.5913\n",
            "Epoch 00090: val_accuracy did not improve from 0.61450\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1533 - accuracy: 0.5915 - val_loss: 1.1298 - val_accuracy: 0.6076\n",
            "Epoch 91/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 1.1483 - accuracy: 0.5938\n",
            "Epoch 00091: val_accuracy did not improve from 0.61450\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1486 - accuracy: 0.5937 - val_loss: 1.1152 - val_accuracy: 0.6136\n",
            "Epoch 92/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.1488 - accuracy: 0.5933\n",
            "Epoch 00092: val_accuracy improved from 0.61450 to 0.61490, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1482 - accuracy: 0.5936 - val_loss: 1.1058 - val_accuracy: 0.6149\n",
            "Epoch 93/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.1460 - accuracy: 0.5928\n",
            "Epoch 00093: val_accuracy improved from 0.61490 to 0.61770, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1458 - accuracy: 0.5929 - val_loss: 1.1044 - val_accuracy: 0.6177\n",
            "Epoch 94/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.1403 - accuracy: 0.5946\n",
            "Epoch 00094: val_accuracy improved from 0.61770 to 0.61860, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1402 - accuracy: 0.5944 - val_loss: 1.0975 - val_accuracy: 0.6186\n",
            "Epoch 95/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.1387 - accuracy: 0.5948\n",
            "Epoch 00095: val_accuracy did not improve from 0.61860\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1386 - accuracy: 0.5948 - val_loss: 1.0951 - val_accuracy: 0.6183\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.1354 - accuracy: 0.5988\n",
            "Epoch 00096: val_accuracy improved from 0.61860 to 0.61900, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1354 - accuracy: 0.5988 - val_loss: 1.0994 - val_accuracy: 0.6190\n",
            "Epoch 97/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.1309 - accuracy: 0.5979\n",
            "Epoch 00097: val_accuracy improved from 0.61900 to 0.61950, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1307 - accuracy: 0.5981 - val_loss: 1.0944 - val_accuracy: 0.6195\n",
            "Epoch 98/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.1337 - accuracy: 0.5981\n",
            "Epoch 00098: val_accuracy improved from 0.61950 to 0.62250, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1339 - accuracy: 0.5980 - val_loss: 1.0916 - val_accuracy: 0.6225\n",
            "Epoch 99/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.1243 - accuracy: 0.6005\n",
            "Epoch 00099: val_accuracy improved from 0.62250 to 0.62430, saving model to best_Adagrad_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1243 - accuracy: 0.6005 - val_loss: 1.0861 - val_accuracy: 0.6243\n",
            "Epoch 100/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.1261 - accuracy: 0.6018\n",
            "Epoch 00100: val_accuracy did not improve from 0.62430\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.1264 - accuracy: 0.6015 - val_loss: 1.0977 - val_accuracy: 0.6189\n",
            "Epoch 1/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.5556 - accuracy: 0.4273\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.57150, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5547 - accuracy: 0.4277 - val_loss: 1.1894 - val_accuracy: 0.5715\n",
            "Epoch 2/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.1233 - accuracy: 0.5972\n",
            "Epoch 00002: val_accuracy improved from 0.57150 to 0.64640, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1232 - accuracy: 0.5972 - val_loss: 1.0051 - val_accuracy: 0.6464\n",
            "Epoch 3/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.9545 - accuracy: 0.6649\n",
            "Epoch 00003: val_accuracy improved from 0.64640 to 0.69850, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9544 - accuracy: 0.6647 - val_loss: 0.8646 - val_accuracy: 0.6985\n",
            "Epoch 4/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8495 - accuracy: 0.7013\n",
            "Epoch 00004: val_accuracy improved from 0.69850 to 0.72730, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8493 - accuracy: 0.7015 - val_loss: 0.7712 - val_accuracy: 0.7273\n",
            "Epoch 5/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7765 - accuracy: 0.7251\n",
            "Epoch 00005: val_accuracy improved from 0.72730 to 0.74340, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7764 - accuracy: 0.7251 - val_loss: 0.7376 - val_accuracy: 0.7434\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7212 - accuracy: 0.7461\n",
            "Epoch 00006: val_accuracy improved from 0.74340 to 0.76110, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7212 - accuracy: 0.7461 - val_loss: 0.7034 - val_accuracy: 0.7611\n",
            "Epoch 7/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.7603\n",
            "Epoch 00007: val_accuracy did not improve from 0.76110\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6783 - accuracy: 0.7602 - val_loss: 0.7067 - val_accuracy: 0.7599\n",
            "Epoch 8/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.6351 - accuracy: 0.7772\n",
            "Epoch 00008: val_accuracy did not improve from 0.76110\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6348 - accuracy: 0.7771 - val_loss: 0.6995 - val_accuracy: 0.7605\n",
            "Epoch 9/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6004 - accuracy: 0.7855\n",
            "Epoch 00009: val_accuracy improved from 0.76110 to 0.77170, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.6006 - accuracy: 0.7854 - val_loss: 0.6777 - val_accuracy: 0.7717\n",
            "Epoch 10/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.5702 - accuracy: 0.7988\n",
            "Epoch 00010: val_accuracy did not improve from 0.77170\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.5701 - accuracy: 0.7988 - val_loss: 0.6772 - val_accuracy: 0.7669\n",
            "Epoch 11/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.5448 - accuracy: 0.8080\n",
            "Epoch 00011: val_accuracy improved from 0.77170 to 0.78050, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.5451 - accuracy: 0.8080 - val_loss: 0.6681 - val_accuracy: 0.7805\n",
            "Epoch 12/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5209 - accuracy: 0.8166\n",
            "Epoch 00012: val_accuracy did not improve from 0.78050\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.5208 - accuracy: 0.8165 - val_loss: 0.6925 - val_accuracy: 0.7641\n",
            "Epoch 13/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.4952 - accuracy: 0.8245\n",
            "Epoch 00013: val_accuracy did not improve from 0.78050\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4954 - accuracy: 0.8245 - val_loss: 0.7209 - val_accuracy: 0.7689\n",
            "Epoch 14/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.4848 - accuracy: 0.8265\n",
            "Epoch 00014: val_accuracy did not improve from 0.78050\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4845 - accuracy: 0.8266 - val_loss: 0.6992 - val_accuracy: 0.7703\n",
            "Epoch 15/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.4656 - accuracy: 0.8375\n",
            "Epoch 00015: val_accuracy did not improve from 0.78050\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4656 - accuracy: 0.8375 - val_loss: 0.7001 - val_accuracy: 0.7754\n",
            "Epoch 16/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.4462 - accuracy: 0.8428\n",
            "Epoch 00016: val_accuracy did not improve from 0.78050\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4460 - accuracy: 0.8429 - val_loss: 0.7063 - val_accuracy: 0.7750\n",
            "Epoch 17/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.8481\n",
            "Epoch 00017: val_accuracy improved from 0.78050 to 0.78330, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4303 - accuracy: 0.8482 - val_loss: 0.6885 - val_accuracy: 0.7833\n",
            "Epoch 18/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8520\n",
            "Epoch 00018: val_accuracy did not improve from 0.78330\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4220 - accuracy: 0.8520 - val_loss: 0.6779 - val_accuracy: 0.7803\n",
            "Epoch 19/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8537\n",
            "Epoch 00019: val_accuracy improved from 0.78330 to 0.78670, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.4197 - accuracy: 0.8535 - val_loss: 0.6926 - val_accuracy: 0.7867\n",
            "Epoch 20/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.3974 - accuracy: 0.8624\n",
            "Epoch 00020: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3971 - accuracy: 0.8625 - val_loss: 0.6930 - val_accuracy: 0.7834\n",
            "Epoch 21/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.3925 - accuracy: 0.8617\n",
            "Epoch 00021: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3927 - accuracy: 0.8616 - val_loss: 0.7478 - val_accuracy: 0.7770\n",
            "Epoch 22/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.3747 - accuracy: 0.8691\n",
            "Epoch 00022: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3747 - accuracy: 0.8691 - val_loss: 0.7084 - val_accuracy: 0.7841\n",
            "Epoch 23/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.3784 - accuracy: 0.8677\n",
            "Epoch 00023: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3783 - accuracy: 0.8678 - val_loss: 0.7000 - val_accuracy: 0.7835\n",
            "Epoch 24/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8735\n",
            "Epoch 00024: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3605 - accuracy: 0.8734 - val_loss: 0.7732 - val_accuracy: 0.7789\n",
            "Epoch 25/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.3653 - accuracy: 0.8721\n",
            "Epoch 00025: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3649 - accuracy: 0.8723 - val_loss: 0.7332 - val_accuracy: 0.7867\n",
            "Epoch 26/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.8795\n",
            "Epoch 00026: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3497 - accuracy: 0.8795 - val_loss: 0.7172 - val_accuracy: 0.7796\n",
            "Epoch 27/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.3452 - accuracy: 0.8809\n",
            "Epoch 00027: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3451 - accuracy: 0.8810 - val_loss: 0.7090 - val_accuracy: 0.7828\n",
            "Epoch 28/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.3495 - accuracy: 0.8803\n",
            "Epoch 00028: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3497 - accuracy: 0.8802 - val_loss: 0.7432 - val_accuracy: 0.7838\n",
            "Epoch 29/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.3362 - accuracy: 0.8830\n",
            "Epoch 00029: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3362 - accuracy: 0.8830 - val_loss: 0.7669 - val_accuracy: 0.7742\n",
            "Epoch 30/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.8879\n",
            "Epoch 00030: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3294 - accuracy: 0.8878 - val_loss: 0.7698 - val_accuracy: 0.7824\n",
            "Epoch 31/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.8865\n",
            "Epoch 00031: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3302 - accuracy: 0.8865 - val_loss: 0.7383 - val_accuracy: 0.7826\n",
            "Epoch 32/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8886\n",
            "Epoch 00032: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3201 - accuracy: 0.8887 - val_loss: 0.7339 - val_accuracy: 0.7836\n",
            "Epoch 33/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.3282 - accuracy: 0.8870\n",
            "Epoch 00033: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3283 - accuracy: 0.8869 - val_loss: 0.7702 - val_accuracy: 0.7798\n",
            "Epoch 34/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.8914\n",
            "Epoch 00034: val_accuracy did not improve from 0.78670\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3106 - accuracy: 0.8912 - val_loss: 0.7988 - val_accuracy: 0.7734\n",
            "Epoch 35/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.3106 - accuracy: 0.8931\n",
            "Epoch 00035: val_accuracy improved from 0.78670 to 0.79000, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3108 - accuracy: 0.8930 - val_loss: 0.7510 - val_accuracy: 0.7900\n",
            "Epoch 36/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.3060 - accuracy: 0.8942\n",
            "Epoch 00036: val_accuracy did not improve from 0.79000\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.3061 - accuracy: 0.8942 - val_loss: 0.7790 - val_accuracy: 0.7775\n",
            "Epoch 37/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.3104 - accuracy: 0.8944\n",
            "Epoch 00037: val_accuracy did not improve from 0.79000\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3098 - accuracy: 0.8947 - val_loss: 0.8014 - val_accuracy: 0.7790\n",
            "Epoch 38/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.8947\n",
            "Epoch 00038: val_accuracy did not improve from 0.79000\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3045 - accuracy: 0.8946 - val_loss: 0.7778 - val_accuracy: 0.7851\n",
            "Epoch 39/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.2949 - accuracy: 0.8993\n",
            "Epoch 00039: val_accuracy did not improve from 0.79000\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2951 - accuracy: 0.8992 - val_loss: 0.7701 - val_accuracy: 0.7874\n",
            "Epoch 40/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.8980\n",
            "Epoch 00040: val_accuracy did not improve from 0.79000\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.3032 - accuracy: 0.8979 - val_loss: 0.7824 - val_accuracy: 0.7863\n",
            "Epoch 41/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.9017\n",
            "Epoch 00041: val_accuracy did not improve from 0.79000\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2903 - accuracy: 0.9017 - val_loss: 0.7924 - val_accuracy: 0.7878\n",
            "Epoch 42/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.8962\n",
            "Epoch 00042: val_accuracy did not improve from 0.79000\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2987 - accuracy: 0.8963 - val_loss: 0.8146 - val_accuracy: 0.7857\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.2918 - accuracy: 0.9028\n",
            "Epoch 00043: val_accuracy improved from 0.79000 to 0.79140, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2918 - accuracy: 0.9028 - val_loss: 0.7938 - val_accuracy: 0.7914\n",
            "Epoch 44/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2889 - accuracy: 0.9018\n",
            "Epoch 00044: val_accuracy improved from 0.79140 to 0.79280, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2892 - accuracy: 0.9018 - val_loss: 0.7639 - val_accuracy: 0.7928\n",
            "Epoch 45/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9020\n",
            "Epoch 00045: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2859 - accuracy: 0.9019 - val_loss: 0.8102 - val_accuracy: 0.7812\n",
            "Epoch 46/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.9040\n",
            "Epoch 00046: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2866 - accuracy: 0.9039 - val_loss: 0.8125 - val_accuracy: 0.7902\n",
            "Epoch 47/100\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 0.2745 - accuracy: 0.9060\n",
            "Epoch 00047: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2740 - accuracy: 0.9062 - val_loss: 0.7969 - val_accuracy: 0.7874\n",
            "Epoch 48/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.9077\n",
            "Epoch 00048: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2719 - accuracy: 0.9077 - val_loss: 0.7885 - val_accuracy: 0.7870\n",
            "Epoch 49/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.2742 - accuracy: 0.9072\n",
            "Epoch 00049: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2744 - accuracy: 0.9073 - val_loss: 0.8350 - val_accuracy: 0.7811\n",
            "Epoch 50/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.2641 - accuracy: 0.9097\n",
            "Epoch 00050: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2646 - accuracy: 0.9095 - val_loss: 0.7973 - val_accuracy: 0.7823\n",
            "Epoch 51/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.2751 - accuracy: 0.9077\n",
            "Epoch 00051: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2754 - accuracy: 0.9076 - val_loss: 0.8283 - val_accuracy: 0.7892\n",
            "Epoch 52/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.9066\n",
            "Epoch 00052: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2757 - accuracy: 0.9067 - val_loss: 0.7844 - val_accuracy: 0.7865\n",
            "Epoch 53/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.2675 - accuracy: 0.9094\n",
            "Epoch 00053: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2683 - accuracy: 0.9093 - val_loss: 0.8755 - val_accuracy: 0.7844\n",
            "Epoch 54/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.2626 - accuracy: 0.9119\n",
            "Epoch 00054: val_accuracy did not improve from 0.79280\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2632 - accuracy: 0.9116 - val_loss: 0.7961 - val_accuracy: 0.7838\n",
            "Epoch 55/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.2588 - accuracy: 0.9125\n",
            "Epoch 00055: val_accuracy improved from 0.79280 to 0.79320, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2586 - accuracy: 0.9125 - val_loss: 0.8127 - val_accuracy: 0.7932\n",
            "Epoch 56/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2540 - accuracy: 0.9164\n",
            "Epoch 00056: val_accuracy did not improve from 0.79320\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2539 - accuracy: 0.9164 - val_loss: 0.8254 - val_accuracy: 0.7885\n",
            "Epoch 57/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.2658 - accuracy: 0.9125\n",
            "Epoch 00057: val_accuracy improved from 0.79320 to 0.79540, saving model to best_Adam_model.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2660 - accuracy: 0.9125 - val_loss: 0.8308 - val_accuracy: 0.7954\n",
            "Epoch 58/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2641 - accuracy: 0.9128\n",
            "Epoch 00058: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2643 - accuracy: 0.9127 - val_loss: 0.8462 - val_accuracy: 0.7827\n",
            "Epoch 59/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9122\n",
            "Epoch 00059: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2569 - accuracy: 0.9122 - val_loss: 0.8521 - val_accuracy: 0.7833\n",
            "Epoch 60/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.9139\n",
            "Epoch 00060: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2509 - accuracy: 0.9140 - val_loss: 0.8254 - val_accuracy: 0.7888\n",
            "Epoch 61/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.2509 - accuracy: 0.9165\n",
            "Epoch 00061: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2513 - accuracy: 0.9164 - val_loss: 0.7741 - val_accuracy: 0.7890\n",
            "Epoch 62/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.2432 - accuracy: 0.9183\n",
            "Epoch 00062: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2431 - accuracy: 0.9184 - val_loss: 0.8217 - val_accuracy: 0.7888\n",
            "Epoch 63/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.2427 - accuracy: 0.9195\n",
            "Epoch 00063: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2430 - accuracy: 0.9193 - val_loss: 0.8544 - val_accuracy: 0.7816\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.2567 - accuracy: 0.9137\n",
            "Epoch 00064: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2567 - accuracy: 0.9137 - val_loss: 0.8365 - val_accuracy: 0.7816\n",
            "Epoch 65/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9185\n",
            "Epoch 00065: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2381 - accuracy: 0.9186 - val_loss: 0.8322 - val_accuracy: 0.7902\n",
            "Epoch 66/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.9177\n",
            "Epoch 00066: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2480 - accuracy: 0.9178 - val_loss: 0.8272 - val_accuracy: 0.7846\n",
            "Epoch 67/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.9199\n",
            "Epoch 00067: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2426 - accuracy: 0.9199 - val_loss: 0.8069 - val_accuracy: 0.7795\n",
            "Epoch 68/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2393 - accuracy: 0.9204\n",
            "Epoch 00068: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2392 - accuracy: 0.9204 - val_loss: 0.8472 - val_accuracy: 0.7885\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9175\n",
            "Epoch 00069: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2444 - accuracy: 0.9175 - val_loss: 0.9014 - val_accuracy: 0.7837\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9208\n",
            "Epoch 00070: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2451 - accuracy: 0.9208 - val_loss: 0.8793 - val_accuracy: 0.7874\n",
            "Epoch 71/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.9210\n",
            "Epoch 00071: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2347 - accuracy: 0.9208 - val_loss: 0.8848 - val_accuracy: 0.7892\n",
            "Epoch 72/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.2314 - accuracy: 0.9236\n",
            "Epoch 00072: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2315 - accuracy: 0.9236 - val_loss: 0.9091 - val_accuracy: 0.7794\n",
            "Epoch 73/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2362 - accuracy: 0.9221\n",
            "Epoch 00073: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2364 - accuracy: 0.9220 - val_loss: 0.8144 - val_accuracy: 0.7853\n",
            "Epoch 74/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.9220\n",
            "Epoch 00074: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2371 - accuracy: 0.9218 - val_loss: 0.9637 - val_accuracy: 0.7818\n",
            "Epoch 75/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.9212\n",
            "Epoch 00075: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2385 - accuracy: 0.9212 - val_loss: 0.8681 - val_accuracy: 0.7852\n",
            "Epoch 76/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.9227\n",
            "Epoch 00076: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2366 - accuracy: 0.9226 - val_loss: 0.8942 - val_accuracy: 0.7846\n",
            "Epoch 77/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9226\n",
            "Epoch 00077: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2370 - accuracy: 0.9227 - val_loss: 0.8760 - val_accuracy: 0.7887\n",
            "Epoch 78/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.2327 - accuracy: 0.9241\n",
            "Epoch 00078: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2326 - accuracy: 0.9241 - val_loss: 0.8992 - val_accuracy: 0.7856\n",
            "Epoch 79/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.9255\n",
            "Epoch 00079: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2253 - accuracy: 0.9255 - val_loss: 0.8994 - val_accuracy: 0.7884\n",
            "Epoch 80/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9232\n",
            "Epoch 00080: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2342 - accuracy: 0.9232 - val_loss: 0.8354 - val_accuracy: 0.7858\n",
            "Epoch 81/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.2281 - accuracy: 0.9247\n",
            "Epoch 00081: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2281 - accuracy: 0.9247 - val_loss: 0.9472 - val_accuracy: 0.7866\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9237\n",
            "Epoch 00082: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2332 - accuracy: 0.9237 - val_loss: 0.8713 - val_accuracy: 0.7882\n",
            "Epoch 83/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.2249 - accuracy: 0.9269\n",
            "Epoch 00083: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2247 - accuracy: 0.9270 - val_loss: 0.9138 - val_accuracy: 0.7923\n",
            "Epoch 84/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9223\n",
            "Epoch 00084: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2361 - accuracy: 0.9223 - val_loss: 0.9005 - val_accuracy: 0.7824\n",
            "Epoch 85/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.2229 - accuracy: 0.9255\n",
            "Epoch 00085: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2229 - accuracy: 0.9255 - val_loss: 0.8259 - val_accuracy: 0.7918\n",
            "Epoch 86/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.2362 - accuracy: 0.9250\n",
            "Epoch 00086: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2365 - accuracy: 0.9249 - val_loss: 0.8265 - val_accuracy: 0.7875\n",
            "Epoch 87/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2199 - accuracy: 0.9272\n",
            "Epoch 00087: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.2199 - accuracy: 0.9272 - val_loss: 0.8691 - val_accuracy: 0.7835\n",
            "Epoch 88/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9254\n",
            "Epoch 00088: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2279 - accuracy: 0.9251 - val_loss: 0.9233 - val_accuracy: 0.7906\n",
            "Epoch 89/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.2199 - accuracy: 0.9278\n",
            "Epoch 00089: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2200 - accuracy: 0.9278 - val_loss: 0.8863 - val_accuracy: 0.7887\n",
            "Epoch 90/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.2236 - accuracy: 0.9258\n",
            "Epoch 00090: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2235 - accuracy: 0.9258 - val_loss: 0.9824 - val_accuracy: 0.7933\n",
            "Epoch 91/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.2188 - accuracy: 0.9285\n",
            "Epoch 00091: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2190 - accuracy: 0.9283 - val_loss: 0.8630 - val_accuracy: 0.7914\n",
            "Epoch 92/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.2299 - accuracy: 0.9262\n",
            "Epoch 00092: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2301 - accuracy: 0.9261 - val_loss: 0.9940 - val_accuracy: 0.7843\n",
            "Epoch 93/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9286\n",
            "Epoch 00093: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2181 - accuracy: 0.9288 - val_loss: 0.9909 - val_accuracy: 0.7871\n",
            "Epoch 94/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9265\n",
            "Epoch 00094: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2264 - accuracy: 0.9267 - val_loss: 0.8852 - val_accuracy: 0.7859\n",
            "Epoch 95/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.2146 - accuracy: 0.9297\n",
            "Epoch 00095: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2149 - accuracy: 0.9296 - val_loss: 0.8972 - val_accuracy: 0.7834\n",
            "Epoch 96/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.2117 - accuracy: 0.9318\n",
            "Epoch 00096: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2121 - accuracy: 0.9317 - val_loss: 0.8956 - val_accuracy: 0.7891\n",
            "Epoch 97/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.2164 - accuracy: 0.9287\n",
            "Epoch 00097: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2165 - accuracy: 0.9285 - val_loss: 0.9210 - val_accuracy: 0.7894\n",
            "Epoch 98/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.2214 - accuracy: 0.9277\n",
            "Epoch 00098: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2213 - accuracy: 0.9277 - val_loss: 0.8851 - val_accuracy: 0.7900\n",
            "Epoch 99/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.2119 - accuracy: 0.9300\n",
            "Epoch 00099: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2119 - accuracy: 0.9300 - val_loss: 0.8705 - val_accuracy: 0.7885\n",
            "Epoch 100/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.2145 - accuracy: 0.9305\n",
            "Epoch 00100: val_accuracy did not improve from 0.79540\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.2147 - accuracy: 0.9304 - val_loss: 0.8780 - val_accuracy: 0.7920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.7832\n",
            "Test accuracy: 0.7832000255584717\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.0776 - accuracy: 0.6221\n",
            "Test accuracy: 0.6220999956130981\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8619 - accuracy: 0.7776\n",
            "Test accuracy: 0.7775999903678894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Rngc4Jn15P"
      },
      "source": [
        "Adam performs the best among three models, and Adagrad performs the worst. The Adagrad has the cons of radical diminishing learning rate while the other two models could resolve this problem. In other words, RMSprop optimization method and Adam optimization are improved version of Adagrad optimization method with radiacl diminishing learning rate problem resolved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEyLLR6kn2BU"
      },
      "source": [
        "### **Part 5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o48i6iXFKuEt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad7e7374-e569-4fb9-cd86-e9611b6004d5"
      },
      "source": [
        "e()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.8514 - accuracy: 0.3255\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.43860, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.8506 - accuracy: 0.3259 - val_loss: 1.5778 - val_accuracy: 0.4386\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 1.5171 - accuracy: 0.4552\n",
            "Epoch 00002: val_accuracy improved from 0.43860 to 0.50880, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5171 - accuracy: 0.4552 - val_loss: 1.3748 - val_accuracy: 0.5088\n",
            "Epoch 3/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.3739 - accuracy: 0.5029\n",
            "Epoch 00003: val_accuracy improved from 0.50880 to 0.52690, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.3739 - accuracy: 0.5030 - val_loss: 1.3462 - val_accuracy: 0.5269\n",
            "Epoch 4/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.2608 - accuracy: 0.5482\n",
            "Epoch 00004: val_accuracy improved from 0.52690 to 0.57950, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.2606 - accuracy: 0.5483 - val_loss: 1.1933 - val_accuracy: 0.5795\n",
            "Epoch 5/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.1757 - accuracy: 0.5809\n",
            "Epoch 00005: val_accuracy improved from 0.57950 to 0.61140, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1757 - accuracy: 0.5808 - val_loss: 1.1072 - val_accuracy: 0.6114\n",
            "Epoch 6/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 1.1093 - accuracy: 0.6068\n",
            "Epoch 00006: val_accuracy improved from 0.61140 to 0.63040, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.1093 - accuracy: 0.6069 - val_loss: 1.0396 - val_accuracy: 0.6304\n",
            "Epoch 7/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.0498 - accuracy: 0.6323\n",
            "Epoch 00007: val_accuracy improved from 0.63040 to 0.65310, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0500 - accuracy: 0.6321 - val_loss: 0.9877 - val_accuracy: 0.6531\n",
            "Epoch 8/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.0000 - accuracy: 0.6455\n",
            "Epoch 00008: val_accuracy improved from 0.65310 to 0.66840, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.0004 - accuracy: 0.6455 - val_loss: 0.9529 - val_accuracy: 0.6684\n",
            "Epoch 9/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.9640 - accuracy: 0.6618\n",
            "Epoch 00009: val_accuracy improved from 0.66840 to 0.68300, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9644 - accuracy: 0.6617 - val_loss: 0.8979 - val_accuracy: 0.6830\n",
            "Epoch 10/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.9224 - accuracy: 0.6768\n",
            "Epoch 00010: val_accuracy improved from 0.68300 to 0.69560, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.9228 - accuracy: 0.6766 - val_loss: 0.8930 - val_accuracy: 0.6956\n",
            "Epoch 11/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.8947 - accuracy: 0.6889\n",
            "Epoch 00011: val_accuracy improved from 0.69560 to 0.70570, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8955 - accuracy: 0.6890 - val_loss: 0.8479 - val_accuracy: 0.7057\n",
            "Epoch 12/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.8662 - accuracy: 0.6976\n",
            "Epoch 00012: val_accuracy improved from 0.70570 to 0.71460, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8667 - accuracy: 0.6972 - val_loss: 0.8344 - val_accuracy: 0.7146\n",
            "Epoch 13/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.8446 - accuracy: 0.7067\n",
            "Epoch 00013: val_accuracy improved from 0.71460 to 0.72190, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8444 - accuracy: 0.7068 - val_loss: 0.8092 - val_accuracy: 0.7219\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8175 - accuracy: 0.7145\n",
            "Epoch 00014: val_accuracy improved from 0.72190 to 0.72210, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8175 - accuracy: 0.7145 - val_loss: 0.7988 - val_accuracy: 0.7221\n",
            "Epoch 15/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8020 - accuracy: 0.7219\n",
            "Epoch 00015: val_accuracy improved from 0.72210 to 0.72510, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.8022 - accuracy: 0.7218 - val_loss: 0.8007 - val_accuracy: 0.7251\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7884 - accuracy: 0.7304\n",
            "Epoch 00016: val_accuracy improved from 0.72510 to 0.73670, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7884 - accuracy: 0.7304 - val_loss: 0.7672 - val_accuracy: 0.7367\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7676 - accuracy: 0.7343\n",
            "Epoch 00017: val_accuracy improved from 0.73670 to 0.73870, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7676 - accuracy: 0.7343 - val_loss: 0.7696 - val_accuracy: 0.7387\n",
            "Epoch 18/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7564 - accuracy: 0.7398\n",
            "Epoch 00018: val_accuracy did not improve from 0.73870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7565 - accuracy: 0.7400 - val_loss: 0.7645 - val_accuracy: 0.7385\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7406 - accuracy: 0.7477\n",
            "Epoch 00019: val_accuracy did not improve from 0.73870\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7406 - accuracy: 0.7477 - val_loss: 0.7765 - val_accuracy: 0.7336\n",
            "Epoch 20/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7304 - accuracy: 0.7486\n",
            "Epoch 00020: val_accuracy improved from 0.73870 to 0.75200, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7305 - accuracy: 0.7485 - val_loss: 0.7366 - val_accuracy: 0.7520\n",
            "Epoch 21/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7192 - accuracy: 0.7544\n",
            "Epoch 00021: val_accuracy did not improve from 0.75200\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7190 - accuracy: 0.7544 - val_loss: 0.7334 - val_accuracy: 0.7489\n",
            "Epoch 22/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.7142 - accuracy: 0.7560\n",
            "Epoch 00022: val_accuracy improved from 0.75200 to 0.75800, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7143 - accuracy: 0.7562 - val_loss: 0.7186 - val_accuracy: 0.7580\n",
            "Epoch 23/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.7036 - accuracy: 0.7621\n",
            "Epoch 00023: val_accuracy did not improve from 0.75800\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.7038 - accuracy: 0.7619 - val_loss: 0.7670 - val_accuracy: 0.7505\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6986 - accuracy: 0.7613\n",
            "Epoch 00024: val_accuracy improved from 0.75800 to 0.75810, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6986 - accuracy: 0.7613 - val_loss: 0.7252 - val_accuracy: 0.7581\n",
            "Epoch 25/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6906 - accuracy: 0.7668\n",
            "Epoch 00025: val_accuracy improved from 0.75810 to 0.75950, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6905 - accuracy: 0.7668 - val_loss: 0.7135 - val_accuracy: 0.7595\n",
            "Epoch 26/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6838 - accuracy: 0.7691\n",
            "Epoch 00026: val_accuracy improved from 0.75950 to 0.75970, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6834 - accuracy: 0.7691 - val_loss: 0.7113 - val_accuracy: 0.7597\n",
            "Epoch 27/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6766 - accuracy: 0.7725\n",
            "Epoch 00027: val_accuracy did not improve from 0.75970\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6770 - accuracy: 0.7724 - val_loss: 0.7116 - val_accuracy: 0.7591\n",
            "Epoch 28/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6753 - accuracy: 0.7717\n",
            "Epoch 00028: val_accuracy improved from 0.75970 to 0.76780, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6745 - accuracy: 0.7721 - val_loss: 0.6926 - val_accuracy: 0.7678\n",
            "Epoch 29/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.6678 - accuracy: 0.7766\n",
            "Epoch 00029: val_accuracy improved from 0.76780 to 0.77270, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6677 - accuracy: 0.7767 - val_loss: 0.6940 - val_accuracy: 0.7727\n",
            "Epoch 30/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6675 - accuracy: 0.7752\n",
            "Epoch 00030: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6678 - accuracy: 0.7752 - val_loss: 0.6990 - val_accuracy: 0.7653\n",
            "Epoch 31/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6620 - accuracy: 0.7783\n",
            "Epoch 00031: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6619 - accuracy: 0.7783 - val_loss: 0.6844 - val_accuracy: 0.7715\n",
            "Epoch 32/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6598 - accuracy: 0.7795\n",
            "Epoch 00032: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6594 - accuracy: 0.7797 - val_loss: 0.6939 - val_accuracy: 0.7674\n",
            "Epoch 33/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.6546 - accuracy: 0.7805\n",
            "Epoch 00033: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6556 - accuracy: 0.7803 - val_loss: 0.7326 - val_accuracy: 0.7718\n",
            "Epoch 34/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6468 - accuracy: 0.7815\n",
            "Epoch 00034: val_accuracy did not improve from 0.77270\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6470 - accuracy: 0.7814 - val_loss: 0.7333 - val_accuracy: 0.7701\n",
            "Epoch 35/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6456 - accuracy: 0.7842\n",
            "Epoch 00035: val_accuracy improved from 0.77270 to 0.77480, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6456 - accuracy: 0.7843 - val_loss: 0.6759 - val_accuracy: 0.7748\n",
            "Epoch 36/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6459 - accuracy: 0.7853\n",
            "Epoch 00036: val_accuracy did not improve from 0.77480\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6453 - accuracy: 0.7853 - val_loss: 0.7039 - val_accuracy: 0.7738\n",
            "Epoch 37/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6417 - accuracy: 0.7869\n",
            "Epoch 00037: val_accuracy did not improve from 0.77480\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6412 - accuracy: 0.7872 - val_loss: 0.6906 - val_accuracy: 0.7738\n",
            "Epoch 38/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6329 - accuracy: 0.7880\n",
            "Epoch 00038: val_accuracy did not improve from 0.77480\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6326 - accuracy: 0.7881 - val_loss: 0.7041 - val_accuracy: 0.7702\n",
            "Epoch 39/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6325 - accuracy: 0.7892\n",
            "Epoch 00039: val_accuracy improved from 0.77480 to 0.77930, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.6326 - accuracy: 0.7890 - val_loss: 0.6746 - val_accuracy: 0.7793\n",
            "Epoch 40/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.6351 - accuracy: 0.7885\n",
            "Epoch 00040: val_accuracy did not improve from 0.77930\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6345 - accuracy: 0.7886 - val_loss: 0.6809 - val_accuracy: 0.7773\n",
            "Epoch 41/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6232 - accuracy: 0.7926\n",
            "Epoch 00041: val_accuracy did not improve from 0.77930\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6238 - accuracy: 0.7922 - val_loss: 0.7128 - val_accuracy: 0.7791\n",
            "Epoch 42/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6279 - accuracy: 0.7920\n",
            "Epoch 00042: val_accuracy improved from 0.77930 to 0.78010, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6284 - accuracy: 0.7919 - val_loss: 0.6702 - val_accuracy: 0.7801\n",
            "Epoch 43/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6214 - accuracy: 0.7914\n",
            "Epoch 00043: val_accuracy did not improve from 0.78010\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6221 - accuracy: 0.7912 - val_loss: 0.7544 - val_accuracy: 0.7694\n",
            "Epoch 44/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6249 - accuracy: 0.7944\n",
            "Epoch 00044: val_accuracy did not improve from 0.78010\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6248 - accuracy: 0.7944 - val_loss: 0.6823 - val_accuracy: 0.7748\n",
            "Epoch 45/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6214 - accuracy: 0.7933\n",
            "Epoch 00045: val_accuracy improved from 0.78010 to 0.78120, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6213 - accuracy: 0.7934 - val_loss: 0.6654 - val_accuracy: 0.7812\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6201 - accuracy: 0.7926\n",
            "Epoch 00046: val_accuracy did not improve from 0.78120\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6201 - accuracy: 0.7926 - val_loss: 0.7021 - val_accuracy: 0.7810\n",
            "Epoch 47/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6212 - accuracy: 0.7903\n",
            "Epoch 00047: val_accuracy improved from 0.78120 to 0.78410, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6212 - accuracy: 0.7903 - val_loss: 0.7032 - val_accuracy: 0.7841\n",
            "Epoch 48/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6182 - accuracy: 0.7944\n",
            "Epoch 00048: val_accuracy did not improve from 0.78410\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6185 - accuracy: 0.7942 - val_loss: 0.7313 - val_accuracy: 0.7773\n",
            "Epoch 49/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6216 - accuracy: 0.7960\n",
            "Epoch 00049: val_accuracy did not improve from 0.78410\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6210 - accuracy: 0.7962 - val_loss: 0.6971 - val_accuracy: 0.7804\n",
            "Epoch 50/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6138 - accuracy: 0.7964\n",
            "Epoch 00050: val_accuracy did not improve from 0.78410\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6139 - accuracy: 0.7963 - val_loss: 0.7088 - val_accuracy: 0.7794\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.8001\n",
            "Epoch 00051: val_accuracy did not improve from 0.78410\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6110 - accuracy: 0.8001 - val_loss: 0.6815 - val_accuracy: 0.7790\n",
            "Epoch 52/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.6085 - accuracy: 0.8003\n",
            "Epoch 00052: val_accuracy did not improve from 0.78410\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6084 - accuracy: 0.8003 - val_loss: 0.7354 - val_accuracy: 0.7762\n",
            "Epoch 53/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6097 - accuracy: 0.7983\n",
            "Epoch 00053: val_accuracy did not improve from 0.78410\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6095 - accuracy: 0.7982 - val_loss: 0.6927 - val_accuracy: 0.7800\n",
            "Epoch 54/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.6065 - accuracy: 0.7999\n",
            "Epoch 00054: val_accuracy did not improve from 0.78410\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6074 - accuracy: 0.7997 - val_loss: 0.7081 - val_accuracy: 0.7802\n",
            "Epoch 55/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6066 - accuracy: 0.8025\n",
            "Epoch 00055: val_accuracy did not improve from 0.78410\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6064 - accuracy: 0.8025 - val_loss: 0.6863 - val_accuracy: 0.7780\n",
            "Epoch 56/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6042 - accuracy: 0.8015\n",
            "Epoch 00056: val_accuracy improved from 0.78410 to 0.78520, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6035 - accuracy: 0.8018 - val_loss: 0.6652 - val_accuracy: 0.7852\n",
            "Epoch 57/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6047 - accuracy: 0.8038\n",
            "Epoch 00057: val_accuracy did not improve from 0.78520\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6045 - accuracy: 0.8039 - val_loss: 0.7175 - val_accuracy: 0.7792\n",
            "Epoch 58/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.6030 - accuracy: 0.7995\n",
            "Epoch 00058: val_accuracy improved from 0.78520 to 0.78890, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6031 - accuracy: 0.7994 - val_loss: 0.6621 - val_accuracy: 0.7889\n",
            "Epoch 59/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.5986 - accuracy: 0.8008\n",
            "Epoch 00059: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5983 - accuracy: 0.8010 - val_loss: 0.7013 - val_accuracy: 0.7740\n",
            "Epoch 60/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.6055 - accuracy: 0.8030\n",
            "Epoch 00060: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6050 - accuracy: 0.8031 - val_loss: 0.6957 - val_accuracy: 0.7691\n",
            "Epoch 61/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6088 - accuracy: 0.7997\n",
            "Epoch 00061: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6090 - accuracy: 0.7996 - val_loss: 0.7039 - val_accuracy: 0.7758\n",
            "Epoch 62/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.6065 - accuracy: 0.8028\n",
            "Epoch 00062: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6069 - accuracy: 0.8026 - val_loss: 0.8076 - val_accuracy: 0.7775\n",
            "Epoch 63/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.5975 - accuracy: 0.8049\n",
            "Epoch 00063: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5975 - accuracy: 0.8049 - val_loss: 0.6718 - val_accuracy: 0.7858\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.5968 - accuracy: 0.8040\n",
            "Epoch 00064: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5968 - accuracy: 0.8040 - val_loss: 0.6890 - val_accuracy: 0.7844\n",
            "Epoch 65/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5955 - accuracy: 0.8049\n",
            "Epoch 00065: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5953 - accuracy: 0.8050 - val_loss: 0.7222 - val_accuracy: 0.7822\n",
            "Epoch 66/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.5988 - accuracy: 0.8040\n",
            "Epoch 00066: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6000 - accuracy: 0.8037 - val_loss: 0.7265 - val_accuracy: 0.7820\n",
            "Epoch 67/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.6012 - accuracy: 0.8045\n",
            "Epoch 00067: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6010 - accuracy: 0.8045 - val_loss: 0.6938 - val_accuracy: 0.7871\n",
            "Epoch 68/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5998 - accuracy: 0.8054\n",
            "Epoch 00068: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5997 - accuracy: 0.8053 - val_loss: 0.6807 - val_accuracy: 0.7796\n",
            "Epoch 69/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.6002 - accuracy: 0.8023\n",
            "Epoch 00069: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.6004 - accuracy: 0.8022 - val_loss: 0.8198 - val_accuracy: 0.7672\n",
            "Epoch 70/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.5938 - accuracy: 0.8060\n",
            "Epoch 00070: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5939 - accuracy: 0.8059 - val_loss: 0.7818 - val_accuracy: 0.7753\n",
            "Epoch 71/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.5951 - accuracy: 0.8054\n",
            "Epoch 00071: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5955 - accuracy: 0.8051 - val_loss: 0.7201 - val_accuracy: 0.7839\n",
            "Epoch 72/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.5954 - accuracy: 0.8061\n",
            "Epoch 00072: val_accuracy did not improve from 0.78890\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5953 - accuracy: 0.8061 - val_loss: 0.7727 - val_accuracy: 0.7720\n",
            "Epoch 73/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5973 - accuracy: 0.8041\n",
            "Epoch 00073: val_accuracy improved from 0.78890 to 0.79140, saving model to best_model_3x3.h5\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5974 - accuracy: 0.8041 - val_loss: 0.6536 - val_accuracy: 0.7914\n",
            "Epoch 74/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5939 - accuracy: 0.8047\n",
            "Epoch 00074: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5933 - accuracy: 0.8048 - val_loss: 0.6623 - val_accuracy: 0.7857\n",
            "Epoch 75/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.5967 - accuracy: 0.8019\n",
            "Epoch 00075: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5968 - accuracy: 0.8019 - val_loss: 0.7235 - val_accuracy: 0.7836\n",
            "Epoch 76/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.5940 - accuracy: 0.8062\n",
            "Epoch 00076: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5936 - accuracy: 0.8061 - val_loss: 0.7097 - val_accuracy: 0.7835\n",
            "Epoch 77/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5885 - accuracy: 0.8093\n",
            "Epoch 00077: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5887 - accuracy: 0.8091 - val_loss: 0.7575 - val_accuracy: 0.7771\n",
            "Epoch 78/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.5945 - accuracy: 0.8064\n",
            "Epoch 00078: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5945 - accuracy: 0.8063 - val_loss: 0.7337 - val_accuracy: 0.7831\n",
            "Epoch 79/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.5910 - accuracy: 0.8076\n",
            "Epoch 00079: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5907 - accuracy: 0.8077 - val_loss: 0.7568 - val_accuracy: 0.7741\n",
            "Epoch 80/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5936 - accuracy: 0.8068\n",
            "Epoch 00080: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5935 - accuracy: 0.8068 - val_loss: 0.8684 - val_accuracy: 0.7637\n",
            "Epoch 81/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5922 - accuracy: 0.8088\n",
            "Epoch 00081: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5920 - accuracy: 0.8088 - val_loss: 0.6736 - val_accuracy: 0.7865\n",
            "Epoch 82/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.5944 - accuracy: 0.8047\n",
            "Epoch 00082: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 0.5946 - accuracy: 0.8045 - val_loss: 0.7919 - val_accuracy: 0.7686\n",
            "Epoch 83/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.5935 - accuracy: 0.8059\n",
            "Epoch 00083: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5938 - accuracy: 0.8060 - val_loss: 0.8497 - val_accuracy: 0.7707\n",
            "Epoch 84/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5921 - accuracy: 0.8087\n",
            "Epoch 00084: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5922 - accuracy: 0.8087 - val_loss: 0.7338 - val_accuracy: 0.7721\n",
            "Epoch 85/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5871 - accuracy: 0.8073\n",
            "Epoch 00085: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5870 - accuracy: 0.8073 - val_loss: 0.7529 - val_accuracy: 0.7755\n",
            "Epoch 86/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5869 - accuracy: 0.8081\n",
            "Epoch 00086: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5873 - accuracy: 0.8080 - val_loss: 0.7491 - val_accuracy: 0.7690\n",
            "Epoch 87/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5944 - accuracy: 0.8041\n",
            "Epoch 00087: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5943 - accuracy: 0.8043 - val_loss: 0.8020 - val_accuracy: 0.7817\n",
            "Epoch 88/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.5838 - accuracy: 0.8082\n",
            "Epoch 00088: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5837 - accuracy: 0.8083 - val_loss: 0.7106 - val_accuracy: 0.7806\n",
            "Epoch 89/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.5868 - accuracy: 0.8101\n",
            "Epoch 00089: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5873 - accuracy: 0.8101 - val_loss: 0.7106 - val_accuracy: 0.7760\n",
            "Epoch 90/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5914 - accuracy: 0.8079\n",
            "Epoch 00090: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5914 - accuracy: 0.8079 - val_loss: 0.7824 - val_accuracy: 0.7755\n",
            "Epoch 91/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5948 - accuracy: 0.8071\n",
            "Epoch 00091: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5946 - accuracy: 0.8072 - val_loss: 0.7357 - val_accuracy: 0.7767\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.5932 - accuracy: 0.8102\n",
            "Epoch 00092: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5932 - accuracy: 0.8102 - val_loss: 0.7542 - val_accuracy: 0.7762\n",
            "Epoch 93/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.5952 - accuracy: 0.8075\n",
            "Epoch 00093: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5952 - accuracy: 0.8076 - val_loss: 0.7374 - val_accuracy: 0.7809\n",
            "Epoch 94/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5919 - accuracy: 0.8084\n",
            "Epoch 00094: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5921 - accuracy: 0.8084 - val_loss: 0.7729 - val_accuracy: 0.7738\n",
            "Epoch 95/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.5908 - accuracy: 0.8066\n",
            "Epoch 00095: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5902 - accuracy: 0.8067 - val_loss: 0.7385 - val_accuracy: 0.7711\n",
            "Epoch 96/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.5895 - accuracy: 0.8099\n",
            "Epoch 00096: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5899 - accuracy: 0.8099 - val_loss: 0.6991 - val_accuracy: 0.7743\n",
            "Epoch 97/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.5934 - accuracy: 0.8080\n",
            "Epoch 00097: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5936 - accuracy: 0.8079 - val_loss: 0.7193 - val_accuracy: 0.7776\n",
            "Epoch 98/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.5895 - accuracy: 0.8094\n",
            "Epoch 00098: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5896 - accuracy: 0.8095 - val_loss: 0.8520 - val_accuracy: 0.7597\n",
            "Epoch 99/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.5948 - accuracy: 0.8062\n",
            "Epoch 00099: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5947 - accuracy: 0.8063 - val_loss: 0.7450 - val_accuracy: 0.7740\n",
            "Epoch 100/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.5906 - accuracy: 0.8092\n",
            "Epoch 00100: val_accuracy did not improve from 0.79140\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 0.5910 - accuracy: 0.8092 - val_loss: 0.7260 - val_accuracy: 0.7779\n",
            "Epoch 1/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.8425 - accuracy: 0.3307\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.44210, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8421 - accuracy: 0.3308 - val_loss: 1.6203 - val_accuracy: 0.4421\n",
            "Epoch 2/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 1.5402 - accuracy: 0.4433\n",
            "Epoch 00002: val_accuracy improved from 0.44210 to 0.50130, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5393 - accuracy: 0.4436 - val_loss: 1.4342 - val_accuracy: 0.5013\n",
            "Epoch 3/100\n",
            "1238/1250 [============================>.] - ETA: 0s - loss: 1.4151 - accuracy: 0.4928\n",
            "Epoch 00003: val_accuracy improved from 0.50130 to 0.53670, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.4145 - accuracy: 0.4929 - val_loss: 1.3344 - val_accuracy: 0.5367\n",
            "Epoch 4/100\n",
            "1238/1250 [============================>.] - ETA: 0s - loss: 1.3294 - accuracy: 0.5259\n",
            "Epoch 00004: val_accuracy improved from 0.53670 to 0.56260, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.3300 - accuracy: 0.5255 - val_loss: 1.2615 - val_accuracy: 0.5626\n",
            "Epoch 5/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.2597 - accuracy: 0.5506\n",
            "Epoch 00005: val_accuracy improved from 0.56260 to 0.58260, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2602 - accuracy: 0.5506 - val_loss: 1.2021 - val_accuracy: 0.5826\n",
            "Epoch 6/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 1.2010 - accuracy: 0.5739\n",
            "Epoch 00006: val_accuracy improved from 0.58260 to 0.61340, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.2014 - accuracy: 0.5738 - val_loss: 1.1420 - val_accuracy: 0.6134\n",
            "Epoch 7/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.1517 - accuracy: 0.5912\n",
            "Epoch 00007: val_accuracy improved from 0.61340 to 0.61970, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.1514 - accuracy: 0.5913 - val_loss: 1.0967 - val_accuracy: 0.6197\n",
            "Epoch 8/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 1.1110 - accuracy: 0.6097\n",
            "Epoch 00008: val_accuracy improved from 0.61970 to 0.63560, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.1114 - accuracy: 0.6096 - val_loss: 1.0681 - val_accuracy: 0.6356\n",
            "Epoch 9/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.0745 - accuracy: 0.6231\n",
            "Epoch 00009: val_accuracy improved from 0.63560 to 0.64950, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0749 - accuracy: 0.6231 - val_loss: 1.0263 - val_accuracy: 0.6495\n",
            "Epoch 10/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 1.0448 - accuracy: 0.6352\n",
            "Epoch 00010: val_accuracy improved from 0.64950 to 0.65280, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0452 - accuracy: 0.6352 - val_loss: 1.0124 - val_accuracy: 0.6528\n",
            "Epoch 11/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 1.0163 - accuracy: 0.6461\n",
            "Epoch 00011: val_accuracy did not improve from 0.65280\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.0166 - accuracy: 0.6460 - val_loss: 1.0135 - val_accuracy: 0.6520\n",
            "Epoch 12/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.9930 - accuracy: 0.6532\n",
            "Epoch 00012: val_accuracy improved from 0.65280 to 0.67270, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9929 - accuracy: 0.6533 - val_loss: 0.9622 - val_accuracy: 0.6727\n",
            "Epoch 13/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.9725 - accuracy: 0.6609\n",
            "Epoch 00013: val_accuracy improved from 0.67270 to 0.67870, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9724 - accuracy: 0.6610 - val_loss: 0.9466 - val_accuracy: 0.6787\n",
            "Epoch 14/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.9484 - accuracy: 0.6697\n",
            "Epoch 00014: val_accuracy did not improve from 0.67870\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9483 - accuracy: 0.6697 - val_loss: 0.9399 - val_accuracy: 0.6771\n",
            "Epoch 15/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.9376 - accuracy: 0.6752\n",
            "Epoch 00015: val_accuracy did not improve from 0.67870\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9381 - accuracy: 0.6750 - val_loss: 0.9657 - val_accuracy: 0.6686\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.9207 - accuracy: 0.6818\n",
            "Epoch 00016: val_accuracy improved from 0.67870 to 0.68240, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9207 - accuracy: 0.6818 - val_loss: 0.9303 - val_accuracy: 0.6824\n",
            "Epoch 17/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.9031 - accuracy: 0.6914\n",
            "Epoch 00017: val_accuracy did not improve from 0.68240\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.9028 - accuracy: 0.6916 - val_loss: 0.9700 - val_accuracy: 0.6677\n",
            "Epoch 18/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.8945 - accuracy: 0.6913\n",
            "Epoch 00018: val_accuracy improved from 0.68240 to 0.69840, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8945 - accuracy: 0.6912 - val_loss: 0.9168 - val_accuracy: 0.6984\n",
            "Epoch 19/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.8826 - accuracy: 0.6951\n",
            "Epoch 00019: val_accuracy improved from 0.69840 to 0.70140, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8825 - accuracy: 0.6953 - val_loss: 0.8911 - val_accuracy: 0.7014\n",
            "Epoch 20/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.8714 - accuracy: 0.6989\n",
            "Epoch 00020: val_accuracy improved from 0.70140 to 0.70290, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8715 - accuracy: 0.6988 - val_loss: 0.9448 - val_accuracy: 0.7029\n",
            "Epoch 21/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.8631 - accuracy: 0.7029\n",
            "Epoch 00021: val_accuracy improved from 0.70290 to 0.70840, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8630 - accuracy: 0.7030 - val_loss: 0.8896 - val_accuracy: 0.7084\n",
            "Epoch 22/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.8556 - accuracy: 0.7059\n",
            "Epoch 00022: val_accuracy improved from 0.70840 to 0.71140, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8553 - accuracy: 0.7060 - val_loss: 0.8588 - val_accuracy: 0.7114\n",
            "Epoch 23/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.8467 - accuracy: 0.7084\n",
            "Epoch 00023: val_accuracy did not improve from 0.71140\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8470 - accuracy: 0.7083 - val_loss: 0.9490 - val_accuracy: 0.6976\n",
            "Epoch 24/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.8426 - accuracy: 0.7139\n",
            "Epoch 00024: val_accuracy did not improve from 0.71140\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8423 - accuracy: 0.7139 - val_loss: 0.9587 - val_accuracy: 0.6918\n",
            "Epoch 25/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.8378 - accuracy: 0.7170\n",
            "Epoch 00025: val_accuracy did not improve from 0.71140\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8383 - accuracy: 0.7171 - val_loss: 0.9477 - val_accuracy: 0.7079\n",
            "Epoch 26/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8284 - accuracy: 0.7196\n",
            "Epoch 00026: val_accuracy did not improve from 0.71140\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8285 - accuracy: 0.7196 - val_loss: 0.8967 - val_accuracy: 0.7112\n",
            "Epoch 27/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.8265 - accuracy: 0.7205\n",
            "Epoch 00027: val_accuracy improved from 0.71140 to 0.71460, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8264 - accuracy: 0.7207 - val_loss: 0.8936 - val_accuracy: 0.7146\n",
            "Epoch 28/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.8180 - accuracy: 0.7257\n",
            "Epoch 00028: val_accuracy did not improve from 0.71460\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8184 - accuracy: 0.7256 - val_loss: 0.9107 - val_accuracy: 0.7105\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.8197 - accuracy: 0.7234\n",
            "Epoch 00029: val_accuracy did not improve from 0.71460\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.8197 - accuracy: 0.7234 - val_loss: 0.8942 - val_accuracy: 0.7033\n",
            "Epoch 30/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.8122 - accuracy: 0.7266\n",
            "Epoch 00030: val_accuracy did not improve from 0.71460\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8119 - accuracy: 0.7268 - val_loss: 0.9442 - val_accuracy: 0.7146\n",
            "Epoch 31/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.8079 - accuracy: 0.7277\n",
            "Epoch 00031: val_accuracy did not improve from 0.71460\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8082 - accuracy: 0.7279 - val_loss: 0.8998 - val_accuracy: 0.7077\n",
            "Epoch 32/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.8052 - accuracy: 0.7298\n",
            "Epoch 00032: val_accuracy did not improve from 0.71460\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8050 - accuracy: 0.7297 - val_loss: 0.9225 - val_accuracy: 0.7074\n",
            "Epoch 33/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.8049 - accuracy: 0.7301\n",
            "Epoch 00033: val_accuracy improved from 0.71460 to 0.72170, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8048 - accuracy: 0.7301 - val_loss: 0.8681 - val_accuracy: 0.7217\n",
            "Epoch 34/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.8051 - accuracy: 0.7300\n",
            "Epoch 00034: val_accuracy improved from 0.72170 to 0.72270, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.8045 - accuracy: 0.7303 - val_loss: 0.9082 - val_accuracy: 0.7227\n",
            "Epoch 35/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7922 - accuracy: 0.7332\n",
            "Epoch 00035: val_accuracy did not improve from 0.72270\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7926 - accuracy: 0.7330 - val_loss: 0.9383 - val_accuracy: 0.6926\n",
            "Epoch 36/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7879 - accuracy: 0.7342\n",
            "Epoch 00036: val_accuracy did not improve from 0.72270\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7879 - accuracy: 0.7343 - val_loss: 0.9457 - val_accuracy: 0.7001\n",
            "Epoch 37/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7916 - accuracy: 0.7346\n",
            "Epoch 00037: val_accuracy did not improve from 0.72270\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7916 - accuracy: 0.7346 - val_loss: 0.9981 - val_accuracy: 0.7199\n",
            "Epoch 38/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7879 - accuracy: 0.7380\n",
            "Epoch 00038: val_accuracy did not improve from 0.72270\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7883 - accuracy: 0.7379 - val_loss: 0.9736 - val_accuracy: 0.7153\n",
            "Epoch 39/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.7893 - accuracy: 0.7380\n",
            "Epoch 00039: val_accuracy did not improve from 0.72270\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7891 - accuracy: 0.7380 - val_loss: 0.8632 - val_accuracy: 0.7225\n",
            "Epoch 40/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7854 - accuracy: 0.7409\n",
            "Epoch 00040: val_accuracy improved from 0.72270 to 0.73180, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7850 - accuracy: 0.7411 - val_loss: 0.8445 - val_accuracy: 0.7318\n",
            "Epoch 41/100\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 0.7837 - accuracy: 0.7400\n",
            "Epoch 00041: val_accuracy improved from 0.73180 to 0.73500, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7842 - accuracy: 0.7400 - val_loss: 0.9468 - val_accuracy: 0.7350\n",
            "Epoch 42/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.7847 - accuracy: 0.7367\n",
            "Epoch 00042: val_accuracy did not improve from 0.73500\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7843 - accuracy: 0.7368 - val_loss: 0.8868 - val_accuracy: 0.7230\n",
            "Epoch 43/100\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 0.7778 - accuracy: 0.7417\n",
            "Epoch 00043: val_accuracy did not improve from 0.73500\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7780 - accuracy: 0.7418 - val_loss: 0.9700 - val_accuracy: 0.7189\n",
            "Epoch 44/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7711 - accuracy: 0.7434\n",
            "Epoch 00044: val_accuracy did not improve from 0.73500\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7714 - accuracy: 0.7434 - val_loss: 1.0418 - val_accuracy: 0.7173\n",
            "Epoch 45/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.7749 - accuracy: 0.7419\n",
            "Epoch 00045: val_accuracy did not improve from 0.73500\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7752 - accuracy: 0.7419 - val_loss: 0.9904 - val_accuracy: 0.7074\n",
            "Epoch 46/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.7717 - accuracy: 0.7442\n",
            "Epoch 00046: val_accuracy improved from 0.73500 to 0.73680, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7714 - accuracy: 0.7443 - val_loss: 0.8689 - val_accuracy: 0.7368\n",
            "Epoch 47/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.7727 - accuracy: 0.7434\n",
            "Epoch 00047: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7727 - accuracy: 0.7433 - val_loss: 0.9147 - val_accuracy: 0.7068\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7732 - accuracy: 0.7422\n",
            "Epoch 00048: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7732 - accuracy: 0.7422 - val_loss: 0.9823 - val_accuracy: 0.7040\n",
            "Epoch 49/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.7649 - accuracy: 0.7497\n",
            "Epoch 00049: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7651 - accuracy: 0.7496 - val_loss: 0.9221 - val_accuracy: 0.7227\n",
            "Epoch 50/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7677 - accuracy: 0.7458\n",
            "Epoch 00050: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7677 - accuracy: 0.7458 - val_loss: 1.0200 - val_accuracy: 0.7128\n",
            "Epoch 51/100\n",
            "1238/1250 [============================>.] - ETA: 0s - loss: 0.7582 - accuracy: 0.7475\n",
            "Epoch 00051: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7583 - accuracy: 0.7474 - val_loss: 1.0256 - val_accuracy: 0.7225\n",
            "Epoch 52/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.7711 - accuracy: 0.7463\n",
            "Epoch 00052: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7712 - accuracy: 0.7464 - val_loss: 1.0780 - val_accuracy: 0.7111\n",
            "Epoch 53/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7642 - accuracy: 0.7464\n",
            "Epoch 00053: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7642 - accuracy: 0.7463 - val_loss: 1.2078 - val_accuracy: 0.6844\n",
            "Epoch 54/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7636 - accuracy: 0.7494\n",
            "Epoch 00054: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7635 - accuracy: 0.7496 - val_loss: 1.0058 - val_accuracy: 0.7110\n",
            "Epoch 55/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7564 - accuracy: 0.7470\n",
            "Epoch 00055: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7566 - accuracy: 0.7470 - val_loss: 0.9455 - val_accuracy: 0.7307\n",
            "Epoch 56/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7617 - accuracy: 0.7511\n",
            "Epoch 00056: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7617 - accuracy: 0.7509 - val_loss: 1.0227 - val_accuracy: 0.7110\n",
            "Epoch 57/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7626 - accuracy: 0.7491\n",
            "Epoch 00057: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7625 - accuracy: 0.7491 - val_loss: 0.9869 - val_accuracy: 0.7156\n",
            "Epoch 58/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7559 - accuracy: 0.7536\n",
            "Epoch 00058: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7559 - accuracy: 0.7535 - val_loss: 0.9684 - val_accuracy: 0.6927\n",
            "Epoch 59/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.7574 - accuracy: 0.7489\n",
            "Epoch 00059: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7578 - accuracy: 0.7487 - val_loss: 1.0824 - val_accuracy: 0.7150\n",
            "Epoch 60/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7578 - accuracy: 0.7512\n",
            "Epoch 00060: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7586 - accuracy: 0.7509 - val_loss: 1.0348 - val_accuracy: 0.7270\n",
            "Epoch 61/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7543 - accuracy: 0.7523\n",
            "Epoch 00061: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7547 - accuracy: 0.7522 - val_loss: 1.1072 - val_accuracy: 0.7238\n",
            "Epoch 62/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.7478 - accuracy: 0.7533\n",
            "Epoch 00062: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7482 - accuracy: 0.7532 - val_loss: 1.0187 - val_accuracy: 0.7250\n",
            "Epoch 63/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7518 - accuracy: 0.7549\n",
            "Epoch 00063: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7514 - accuracy: 0.7552 - val_loss: 0.9207 - val_accuracy: 0.7267\n",
            "Epoch 64/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.7490 - accuracy: 0.7544\n",
            "Epoch 00064: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7493 - accuracy: 0.7543 - val_loss: 1.0543 - val_accuracy: 0.7103\n",
            "Epoch 65/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.7525 - accuracy: 0.7542\n",
            "Epoch 00065: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7526 - accuracy: 0.7541 - val_loss: 1.0085 - val_accuracy: 0.7210\n",
            "Epoch 66/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.7509 - accuracy: 0.7539\n",
            "Epoch 00066: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7508 - accuracy: 0.7538 - val_loss: 1.0447 - val_accuracy: 0.7122\n",
            "Epoch 67/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7486 - accuracy: 0.7535\n",
            "Epoch 00067: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7483 - accuracy: 0.7538 - val_loss: 1.0387 - val_accuracy: 0.7062\n",
            "Epoch 68/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7468 - accuracy: 0.7540\n",
            "Epoch 00068: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7473 - accuracy: 0.7540 - val_loss: 0.9432 - val_accuracy: 0.7314\n",
            "Epoch 69/100\n",
            "1242/1250 [============================>.] - ETA: 0s - loss: 0.7537 - accuracy: 0.7540\n",
            "Epoch 00069: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7533 - accuracy: 0.7540 - val_loss: 0.9197 - val_accuracy: 0.7357\n",
            "Epoch 70/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.7474 - accuracy: 0.7534\n",
            "Epoch 00070: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7474 - accuracy: 0.7532 - val_loss: 1.0102 - val_accuracy: 0.7241\n",
            "Epoch 71/100\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 0.7517 - accuracy: 0.7552\n",
            "Epoch 00071: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7516 - accuracy: 0.7552 - val_loss: 0.9020 - val_accuracy: 0.7111\n",
            "Epoch 72/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7508 - accuracy: 0.7543\n",
            "Epoch 00072: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7517 - accuracy: 0.7540 - val_loss: 1.1867 - val_accuracy: 0.7047\n",
            "Epoch 73/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7429 - accuracy: 0.7573\n",
            "Epoch 00073: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7429 - accuracy: 0.7573 - val_loss: 1.0599 - val_accuracy: 0.7274\n",
            "Epoch 74/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7456 - accuracy: 0.7577\n",
            "Epoch 00074: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7456 - accuracy: 0.7577 - val_loss: 1.0341 - val_accuracy: 0.7215\n",
            "Epoch 75/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7448 - accuracy: 0.7560\n",
            "Epoch 00075: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7448 - accuracy: 0.7561 - val_loss: 0.9848 - val_accuracy: 0.7232\n",
            "Epoch 76/100\n",
            "1246/1250 [============================>.] - ETA: 0s - loss: 0.7491 - accuracy: 0.7551\n",
            "Epoch 00076: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7494 - accuracy: 0.7549 - val_loss: 1.1272 - val_accuracy: 0.7032\n",
            "Epoch 77/100\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 0.7465 - accuracy: 0.7560\n",
            "Epoch 00077: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7461 - accuracy: 0.7561 - val_loss: 1.1034 - val_accuracy: 0.6867\n",
            "Epoch 78/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.7463 - accuracy: 0.7572\n",
            "Epoch 00078: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7468 - accuracy: 0.7571 - val_loss: 1.0446 - val_accuracy: 0.7118\n",
            "Epoch 79/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7432 - accuracy: 0.7571\n",
            "Epoch 00079: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7432 - accuracy: 0.7571 - val_loss: 1.0920 - val_accuracy: 0.7076\n",
            "Epoch 80/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7445 - accuracy: 0.7589\n",
            "Epoch 00080: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 0.7440 - accuracy: 0.7591 - val_loss: 0.9408 - val_accuracy: 0.7227\n",
            "Epoch 81/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7365 - accuracy: 0.7591\n",
            "Epoch 00081: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7364 - accuracy: 0.7591 - val_loss: 0.9898 - val_accuracy: 0.7185\n",
            "Epoch 82/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7405 - accuracy: 0.7587\n",
            "Epoch 00082: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7409 - accuracy: 0.7587 - val_loss: 0.9880 - val_accuracy: 0.7086\n",
            "Epoch 83/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7414 - accuracy: 0.7610\n",
            "Epoch 00083: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7411 - accuracy: 0.7611 - val_loss: 0.9509 - val_accuracy: 0.7322\n",
            "Epoch 84/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7469 - accuracy: 0.7561\n",
            "Epoch 00084: val_accuracy did not improve from 0.73680\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7470 - accuracy: 0.7562 - val_loss: 1.0059 - val_accuracy: 0.7155\n",
            "Epoch 85/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.7403 - accuracy: 0.7597\n",
            "Epoch 00085: val_accuracy improved from 0.73680 to 0.73770, saving model to best_model_5x5.h5\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7399 - accuracy: 0.7595 - val_loss: 0.9191 - val_accuracy: 0.7377\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7380 - accuracy: 0.7605\n",
            "Epoch 00086: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7380 - accuracy: 0.7605 - val_loss: 1.0644 - val_accuracy: 0.7133\n",
            "Epoch 87/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7407 - accuracy: 0.7571\n",
            "Epoch 00087: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7406 - accuracy: 0.7569 - val_loss: 1.1517 - val_accuracy: 0.6863\n",
            "Epoch 88/100\n",
            "1240/1250 [============================>.] - ETA: 0s - loss: 0.7414 - accuracy: 0.7593\n",
            "Epoch 00088: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7419 - accuracy: 0.7591 - val_loss: 1.0079 - val_accuracy: 0.6957\n",
            "Epoch 89/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 0.7456 - accuracy: 0.7597\n",
            "Epoch 00089: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7461 - accuracy: 0.7595 - val_loss: 1.0715 - val_accuracy: 0.7183\n",
            "Epoch 90/100\n",
            "1249/1250 [============================>.] - ETA: 0s - loss: 0.7426 - accuracy: 0.7561\n",
            "Epoch 00090: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7425 - accuracy: 0.7561 - val_loss: 1.0321 - val_accuracy: 0.7263\n",
            "Epoch 91/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7375 - accuracy: 0.7584\n",
            "Epoch 00091: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7381 - accuracy: 0.7582 - val_loss: 1.0477 - val_accuracy: 0.6926\n",
            "Epoch 92/100\n",
            "1239/1250 [============================>.] - ETA: 0s - loss: 0.7411 - accuracy: 0.7573\n",
            "Epoch 00092: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7410 - accuracy: 0.7574 - val_loss: 1.0763 - val_accuracy: 0.7179\n",
            "Epoch 93/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7359 - accuracy: 0.7603\n",
            "Epoch 00093: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7361 - accuracy: 0.7603 - val_loss: 1.0361 - val_accuracy: 0.7136\n",
            "Epoch 94/100\n",
            "1248/1250 [============================>.] - ETA: 0s - loss: 0.7424 - accuracy: 0.7601\n",
            "Epoch 00094: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7425 - accuracy: 0.7600 - val_loss: 1.0078 - val_accuracy: 0.6891\n",
            "Epoch 95/100\n",
            "1244/1250 [============================>.] - ETA: 0s - loss: 0.7367 - accuracy: 0.7571\n",
            "Epoch 00095: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7368 - accuracy: 0.7573 - val_loss: 1.0270 - val_accuracy: 0.7258\n",
            "Epoch 96/100\n",
            "1241/1250 [============================>.] - ETA: 0s - loss: 0.7420 - accuracy: 0.7566\n",
            "Epoch 00096: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7420 - accuracy: 0.7566 - val_loss: 1.1505 - val_accuracy: 0.6889\n",
            "Epoch 97/100\n",
            "1243/1250 [============================>.] - ETA: 0s - loss: 0.7470 - accuracy: 0.7567\n",
            "Epoch 00097: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7460 - accuracy: 0.7570 - val_loss: 0.9885 - val_accuracy: 0.7040\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7440 - accuracy: 0.7588\n",
            "Epoch 00098: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7440 - accuracy: 0.7588 - val_loss: 1.1715 - val_accuracy: 0.7015\n",
            "Epoch 99/100\n",
            "1250/1250 [==============================] - ETA: 0s - loss: 0.7417 - accuracy: 0.7574\n",
            "Epoch 00099: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7417 - accuracy: 0.7574 - val_loss: 1.2062 - val_accuracy: 0.7202\n",
            "Epoch 100/100\n",
            "1245/1250 [============================>.] - ETA: 0s - loss: 0.7408 - accuracy: 0.7576\n",
            "Epoch 00100: val_accuracy did not improve from 0.73770\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 0.7407 - accuracy: 0.7577 - val_loss: 1.0051 - val_accuracy: 0.7312\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.7825\n",
            "Test accuracy: 0.7825000286102295\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.9266 - accuracy: 0.7336\n",
            "Test accuracy: 0.7336000204086304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AJ9wPsCn5tr"
      },
      "source": [
        "The 3x3 model performs better than the 5x5 model on both training accuracy and validation accuracy. In conclusion, smaller layer with deeper neural network perform better than larger layer with shallower nerual network on image classification. Smaller layer with deeper neural network is more expressive on image classification. Larger layers need more computational power. With the same computational power, smaller layer which is smaller filter will result in less parameters which need less computationality to construct a deeper neural network, which could lead to a better performance on accuracy of image classification."
      ]
    }
  ]
}